{"title":"Email analysis","markdown":{"yaml":{"title":"Email analysis","date":"2019-04-06","categories":["R","learning"]},"headingText":"Emails from Gmail","containsRefs":false,"markdown":"\n\n```{r}\n#| echo: false\n#| warning: false\n#| message: false\n#| results: hide\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(scales) # for time scaling\n```\n\n\nI am keen on tracking things. Some people call this *personal analytics*, I call it fun. In this post, I will explore how to extract your email information using python and R. My goal is to do some analysis on the data and, hopefully, explore different visualizations that can inform future behavior. \n\n\n\nReading emails in python is quite simple, we need to import the mailbox library. My file is called `correo` and comes from downloading my Gmail stuff out of https://takeout.google.com/settings/takeout. It can take a while so be patient.\n\n\n```{python}\n#| eval: false\nimport mailbox\n\n# read in mbox\nmbox = mailbox.mbox('correo')\n\n# Show what we have here\nfor key in mbox[0].keys():\n    print(key)\n\n```\n\n\nThis should return something that looks like:\n\n```\nX-GM-THRID\nX-Gmail-Labels\nReceived\nMIME-Version\nDate\nTo\nFrom\nSubject\nContent-Type\nContent-Disposition\nMessage-Id\n```\n\nLet's save the info we care about into a new file. We select `subject`, `from`, `date`, `to`, and some variables that allow us to keep track of things (i.e, labels and threads).\n\n\n```{python}\n#| eval: false\nimport csv\nwith open(\"gmail_mbox.csv\", \"w\") as outfile:\n    writer = csv.writer(outfile)\n    for message in mbox:\n        writer.writerow([message['subject'], message['from'],\n        message['date'], message['to'],\n        message['X-Gmail-Labels'], message['X-GM-THRID']])\n\n\n```\n\n\n\n## Emails from Thunderbird\n\nI keep 3 accounts in Thunderbird. Using the [ImportExportTools Add-on](https://addons.thunderbird.net/en-US/thunderbird/addon/importexporttools/), I exported things into `.mbox` format. Following a similar procedure to the one depicted above, I got the other three accounts exported to `.csv` files. Just be sure you select the correct keys (see example below, this might change for other email clients). \n\n\n```{python}\n#| eval: false\n\nwith open(\"MIT_mbox.csv\", \"w\") as outfile:\n    writer = csv.writer(outfile)\n    for message in mbox:\n        writer.writerow([message['Subject'],\n        message['From'], message['Date'],\n        message['To'],message['Status']])\n\n```\n\n\n\n## Data cleaning\n\nLet's switch from python to R[^stay]. \n\n\n```{r}\n#| label: read-into-R\ndf <- read.csv(\"data/gmail_mbox.csv\",\n               header=FALSE, encoding=\"UTF-8\")\n\nnames(df) <- c('subject', 'from', 'date', 'to', 'label', 'thread')\n\n```\n\n\n\nUnfortunately, emails come tagged (things like `\"\\\\?=\"` and other nasty stuff) and you might have to deal with different encodings (the perks of speaking multiple languages). As an example, let's see what Quora sends me.\n\n\n```{r}\ndf %>% filter(str_detect(from, \"Quora\")) %>% dplyr::select(from) %>% slice(1:10)\n```\n\n\nThat's nasty...Let's do some cleaning. This function comes really handy for text replacement.\n\n\n```{r}\n#| label: sourcing-github\n#| message: false\n\n# originally sourced from\ndevtools::source_url(\"https://raw.githubusercontent.com/hrbrmstr/hrbrmisc/master/R/qp.r\")\n\n```\n\n\n\nWe are going to modify the function a bit, we add `x` as the string we pass for cleaning and we will remove the tags progressively.\n\n\n```{r}\n\nqp_decode <- function(x) {\n  \n  # other email taggs\n  x <- stringr::str_remove_all(x, pattern = \"=\\\\?[U-u][T-t][F-f]-8\\\\?[Q-q]\\\\?\")\n  x <- stringr::str_remove_all(x, pattern = \"\\\\?=\")\n  x <- stringr::str_remove_all(x, \"=\\\\?[I-i][S-s][O-o]-8859-1\\\\?Q\\\\?\")\n  \n  stringi::stri_replace_all_fixed(x, qp_before, qp_after, vectorize_all=FALSE)\n  \n}\n\n```\n\n\nWe are ready to use our super cool function and clean the text! Not perfect, but gets us 90% of the way.\n\n\n```{r}\n#| label: clean-emails\n\ndf <- df %>% mutate_at(.vars = vars(subject, from, to, label), .funs = qp_decode) \n\n```\n\n\nLet's see how emails from Quora changed with this new encoding:\n\n\n```{r}\n#| label: quora-example\ndf %>% filter(str_detect(from, \"Quora\")) %>% dplyr::select(from) %>% slice(1:10)\n```\n\n\nLet's filter those from \"Received\" or \"Sent\" (in Spanish, \"Recibidos\" or \"Enviado\").\n\n\n```{r}\nemails <- df %>%\n  filter(str_detect(label, \"Recibidos|Enviado\"))\n```\n\n\nTo save you from reading a considerable amount of code, I will load the other accounts and modify them accordingly in the background. I will finally merge everything together. Just enjoy the kitten while the code is running in my machine.\n\n![](001-kitty.jpg)\n\n\n\n```{r}\n#| echo: false\n#| message: false\n#| results: hide\n#| warning: false\n#| code-fold: true\n#| code-summary: \"A lot of code for reading/parsing\"\n\nMIT_sent <- read.csv('data/MIT_Sent.csv',\n                     header=FALSE,\n                     stringsAsFactors = FALSE)\nnames(MIT_sent) <- c('subject', 'from', 'date', 'to', 'label')\nMIT_sent$label <- \"Sent\"\n\nMIT_received <- read.csv(\"data/MIT_inbox.csv\",\n                         header=FALSE,\n                         stringsAsFactors = FALSE)\nnames(MIT_received) <- c('subject', 'from', 'date', 'to', 'label')\nMIT_received$label <- \"Received\"\n\nhotmail_sent <- read.csv('data/hotmail_Sent.csv',\n                         header=FALSE,\n                         stringsAsFactors = FALSE)\nnames(hotmail_sent) <- c('subject', 'from', 'date', 'to', 'label')\nhotmail_sent$label <- \"Sent\"\n\nhotmail_received <- read.csv(\"data/hotmail_inbox.csv\",\n                             header=FALSE,\n                             stringsAsFactors = FALSE)\nnames(hotmail_received) <- c('subject', 'from', 'date', 'to', 'label')\nhotmail_received$label <- \"Received\"\n\numass_sent <- read.csv(\"data/umass_Sent.csv\",\n                       header=FALSE,\n                       stringsAsFactors = FALSE)\nnames(umass_sent) <- c('subject', 'from', 'date', 'to', 'label')\numass_sent$label <- \"Sent\"\n\numass_received <- read.csv(\"data/umass_inbox.csv\",\n                           header=FALSE,\n                           stringsAsFactors = FALSE)\nnames(umass_received) <- c('subject', 'from', 'date', 'to', 'label')\numass_received$label <- \"Received\"\n\n\n# Bind everything\n\nall_accounts <- bind_rows(list(MIT_received = MIT_received,\n               MIT_sent = MIT_sent,\n               hotmail_received = hotmail_received,\n               hotmail_sent = hotmail_sent,\n               umass_received = umass_received,\n               umass_sent = umass_sent), .id = \"account\") %>%\n  mutate(account = str_remove(account, \"_received|_sent\"))\n\n\nemails <- emails %>% mutate(account = \"Gmail\") %>%\n  bind_rows(all_accounts) %>%\n  mutate(simple_label = ifelse(\n    str_detect(label, pattern = \"Recibido|Received\"),\n    \"Received\", \n    \"Sent\"))\n\n```\n\n\n## Analysis\n\nThere's still some stuff to clean, but I'd rather go into the analysis. So, let's get some questions to guide our purpose:\n\n1) Who sends me the most emails? Who receives emails from me?\n1) When do I get emails (mostly)?\n1) When should I do something about it (aka, reply)?\n\n> **Warning:** We have to dance with parsing dates and times. \n> I highly recommend being familiar with `lubridate` (for example, see https://rdrr.io/cran/lubridate/man/parse_date_time.html).\n\n### Most frequent senders\n\nJust because I'm curious, let's take a look at who are the all time senders!\n\n\n```{r}\n#| echo: false\n#| warning: false\n#| message: false\nemails %>%\n  # Filter me out\n  filter(!str_detect(from, \"matiasandina|mandina\"))%>%\n  # Count the people\n  count(from) %>% arrange(desc(n)) %>% slice(1:10) %>%\n  mutate(from = str_remove(from, \"\\\\<.+\")) %>%\n  separate(from, into = c(\"from\", \"lala\"), sep=\" \") %>%\n  dplyr::select(-lala)\n```\n\n\nIt's cool to know that my lingering feeling (\"wow...`Quora` just spams the hell out of me\") is supported by data. Other big spammers are, of course, the Bank and Amazon. People I work with and friends come high up too. Funny to see Mendeley and Pubchase on the top ten, it's been a long journey of them sending me papers, thank you for that[^Mendeley].\n\n### From me to you\n\nLet's try to find the people I directly send the most emails to. I tend to send a lot of automatic reminders via email to myself so I removed me from the destination.\n\n\n```{r}\n#| echo: false\n#| message: false\n#| warning: false\nemails %>% filter(str_detect(from, \"matiasandina|mandina\") & !str_detect(to, \"matiasandina|mandina\"))%>%\n  count(to) %>% arrange(desc(n)) %>% slice(1:5) %>%\n  mutate(to = str_remove(to, \"\\\\<.+\")) %>%\n  separate(to, into = c(\"to\", \"lala\"), sep=\" \") %>%\n  mutate(to=ifelse(str_detect(to, \"feld\"), \"Mariana\", to),\n         to=str_remove(to, \"_.+\")) %>%\n  dplyr::select(-lala)\n```\n\n\nLooks like both my former advisers get most of my output (yes, same name first name, not related).\n\n## Working with dates and times\n\nEvery time I have to deal with dates, I have a miniature panic attack. As a general rule, you have to have all the variables that you want to use as separate columns (i.e, year, month, day, week_day, time, ...). The `lubridate` package helps a lot, but it's still quite an effort.\n\nWorking only with times of the day, regardless of date itself is problematic. Working with periods is difficult, so `as.numeric(x, \"hour\")` is a friend.\n\nHere's a hint of how the `date` column in the original data actually looks like. This may or might not look the same way for you, it depends on your date settings.  \n\n\n\n```{r}\n#| label: show-date\n\nemails %>% dplyr::select(date) %>% slice(1:5)\n\n```\n\n\n\nLet's create all the variables we need. It seems like a lot because it *should* work out of the box and it doesn't, but it's actually somewhat straight-forward to get most of what we want.  \n\n\n\n```{r}\n#| label: modify-dates\n#| message: false\n#| warning: false\n#| code-fold: true\n#| code-summary: \"Again, a bunch of cleaning stuff\"\n\nemails <- emails %>%\n  separate(date, into=c(\"wday\", \"date\"), sep = \", \") %>%\n  # Fixing the fact that some dates do not have weekday\n  mutate(to_fix = ifelse(\n    wday %in% wday(1:7, label=TRUE , abbr = TRUE), \n    \"OK\", \n    \"fix\"),\n    date = ifelse(to_fix == \"OK\", date, wday),\n    wday=ifelse(to_fix==\"OK\", wday, NA),\n    wday=fct_relevel(wday,\n                     levels=c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"))) %>%\n  dplyr::select(-to_fix)\n\n\n# remove emails with no date\nemails <- emails %>% \n  dplyr::mutate(date = dplyr::na_if(date, \"\")) %>%\n  filter(!is.na(date))\n\n\n# switch everything to my timezone\n# Sys.timezone() gives \"America/New_York\"\n# some parsing migth fail\n\nemails <- emails %>%\n  mutate(date = str_remove(date, \"\\\\(.+\\\\)\"),\n         my_date = dmy_hms(date, tz = \"America/New_York\"),\n         my_day = day(my_date),\n         my_month = month(my_date),\n         my_month_abb = month(my_date, label=TRUE, abbr = TRUE),\n         my_year = year(my_date),\n         my_time = data.table::as.ITime(my_date),\n         hour_num = my_time %>% as.integer() %>% `/` (60*60),\n         only_date = ymd(paste(my_year, my_month, my_day, sep=\"-\")))\n\n\n```\n\n\n\n## Birdseye\n\nLet's look at how the whole email movement looks like. In the last couple of years, I clearly felt the load rising and rising. The lack of data in the early years is mostly due to me not downloading everything from the Hotmail account (it's too late, too far in the past to fix :P). Besides, the trend likely holds quite well.\n\n\n\n```{r}\n#| label: tile-plot\n#| echo: false\n#| height: 6.0\n# Make summary\ntile_data <- emails %>% \n  group_by(my_year, my_month, my_day) %>%\n  count() %>%\n  ungroup() %>% \n  group_by(my_year, my_month) %>%\n  summarise(month_count = sum(n))\n\nggplot(tile_data, aes(my_month, \n                      my_year,\n                      fill=month_count)) +\n    geom_tile(color=\"white\", size=0.8)+\n    scale_fill_viridis_c()+\n    scale_x_continuous(expand=c(0,0), breaks = 1:12, labels = month.abb)+\n    scale_y_continuous(breaks = 2003:2019)+\n    labs(x=\"\", y=\"\", fill=\"Monthly count\")+\n    theme(axis.ticks = element_blank(), \n          panel.background = element_blank(),\n          legend.position = \"bottom\",\n          legend.key.width = unit(1, \"cm\"),\n          strip.text = element_text(hjust = 0.01,\n                                    face = \"bold\",\n                                    size = 12),\n              text = element_text(size=16))+\n      ggtitle(\"Total emails in all accounts\")\n\n```\n\n\nIf we split by input and output, we can easily see that the input-output ratio went nuts when I moved to the US. \n\n\n```{r}\n#| echo: false\n# sent vs received\nemails %>%\n  group_by(simple_label, my_year) %>%\n  count() %>%\n  ggplot(aes(my_year, n, color=simple_label)) +\n  geom_vline(xintercept = 2016, lty=2)+\n  geom_line()+\n  geom_point(size=2.5) +\n  scale_color_manual(values = c(\"orange\", \"black\"))+\n  theme_bw()+ \n  theme(panel.grid = element_blank())+\n  annotate(\"text\", label=c(\"ARG\",\"US\"), \n           x=c(2015, 2016.8), y=-1)+\n  labs(\n    title = \"Moving to the US means a lot more mail\",\n    x=element_blank(), \n    y=\"Number of emails\", color=\"\")+\n  scale_x_continuous(breaks = seq(2003, 2019, by=2))\n\n```\n\n\nThis is not really surprising, given the amount of unsolicited advertising I started getting since the move. Yes, I'm talking to you again Quora/Amazon/people trying to sell me stuff[^my_fault]. Of course, University related chains likely take a big chunk of the pie. \n\nI don't feel like parsing out each sender out of the sheer amount. I have had the Gmail and Hotmail accounts for more than 10 years, but the University email is something relatively recent. All in all, considering the time I've had each account, the input rate coming from universities worries me. Here are the total email for each account: \n\n\n```{r}\ntable(emails$account)\n```\n\n\n\n### When\n\nLet's add the time of the day to the equation. This plot was made using `ggbeeswarm` package, I highly recommend checking it, it's pure power. I got help to put the labels in the `y` axis from '00:00' to '24:00'. You can find a toy example in [this StackOverflow question](https://stackoverflow.com/questions/55463735/how-to-force-scale-y-datetime-scale-to-show-2400-instead-of-0000-in-r/55464995#55464995).\n\n\n```{r}\n#| echo: false\n#| fig.height: 8.0\n\np1 <- ggplot(emails, aes(factor(my_year),\n                   hour_num)) +\n  ggbeeswarm::geom_quasirandom(alpha=0.1)+\n  #scale_y_datetime(breaks = \"2 hour\",\n  #                 date_labels= \"%H:%M\")+\n  #scale_x_discrete(position = \"top\")+\n  theme_bw(base_size = 16)+\n  facet_wrap(~simple_label, nrow=2)+\n  xlab(\"\") + ylab(\"Time of the day\")+\n  #scale_color_manual(values=c(\"darkorange\", \"black\"))+\n  theme(legend.position = \"none\")+\n  scale_y_continuous(breaks = 2*0:12,\n                     labels = function(x) {paste0(floor(x),\":00\")})\n\n\np2 <- ggplot(emails, aes(factor(my_year), as.POSIXct(my_time, format = \"%H:%M:%S\"))) +\n  geom_jitter(alpha=0.1)+\n  ylab(\"\") +\n  scale_y_datetime(breaks = \"2 hour\",\n                   date_labels= \"%H:%M\")+\n  theme_bw()+\n  facet_wrap(~simple_label, nrow = 2)\n\n\n# We need to make the account into factor\nemails$account <- factor(emails$account, levels=c(\"Gmail\", \"MIT\", \"umass\", \"hotmail\"))\n\nhistogram <- emails %>% group_by(account, my_year) %>%\n    count() %>% #ungroup() %>% group_by(my_year) %>%\n    #mutate(rel_count = n/sum(n)) %>%\n    ggplot(aes(my_year, y=n, fill=account))+\n    geom_col()+\n    theme(panel.background = element_blank(), legend.position = \"bottom\",\n          text = element_text(size=16))+\n    scale_fill_grey() +\n    scale_x_continuous(expand = c(0, 0.1), breaks=2003:2019)+\n    labs(x=\"\", y=\"Number of emails\")\n\n#cowplot::plot_grid(p1,histogram, nrow = 2, rel_heights = c(2, #1))\n\n```\n\n\n![](email-analysis-beeswarm.png)\n\n#### Daily news\n\nWhat's the average number of emails per day? I'm including all the emails in from 2015 to 2019, including those that go directly to trash.\n\n\n```{r}\n#| echo: false\n#| warning: false\n#| message: false\nlala <- df %>% \n  bind_rows(all_accounts) %>%\n  separate(date, into=c(\"wday\", \"date\"), sep = \", \") %>%\n  # Fixing the fact that some dates do not have weekday\n  mutate(\n    to_fix = ifelse(\n    wday %in% wday(1:7, label=TRUE , abbr = TRUE), \n    \"OK\",\n    \"fix\"),\n     date = ifelse(to_fix == \"OK\", date, wday),\n     wday=ifelse(to_fix==\"OK\", wday, NA),\n     wday=fct_relevel(wday,\n                      levels=c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"))) %>%\n  dplyr::select(-to_fix) %>%\n  mutate(date = dplyr::na_if(date, \"\")) %>%\n  filter(!is.na(date)) %>%\n  mutate(date = str_remove(date, \"\\\\(.+\\\\)\"),\n         my_date = dmy_hms(date, tz = \"America/New_York\"),\n         my_day = day(my_date),\n         my_month = month(my_date),\n         my_month_abb = month(my_date, label=TRUE, abbr = TRUE),\n         my_year = year(my_date),\n         my_time = data.table::as.ITime(my_date),\n         hour_num = my_time %>% as.integer() %>% `/` (60*60),\n         only_date = ymd(paste(my_year, my_month, my_day, sep=\"-\")))\n\n\nlala %>% group_by(my_year, only_date) %>%\n  count() %>%\n  filter(my_year %in% c(2015:2019)) %>%\n  ggplot(aes(n)) + geom_density(fill=\"gray50\") + \n  theme_bw()+\n  scale_x_continuous(breaks=seq(0, 150, 15))+\n  labs(title = \"Email amount per day\", \n       x = \"Emails/day\")\n\n\n```\n\n\nFor those looking for some tabulated info, here it is:  \n\n\n```{r}\n#| echo: false\nlala_sum <- lala %>% group_by(my_year, only_date) %>%\n  count() %>% filter(my_year %in% c(2015:2019)) %>%\n  group_by(my_year) %>%\n  summarise(mean_n = mean(n),\n            min_n = range(n)[1],\n            max_n = range(n)[2],\n            mode_n = which.max(tabulate(n)))\n\nknitr::kable(lala_sum, \n             caption = \"Number of emails per day received in all accounts\",\n             digits = 1,\n             col.names = c(\n               \"Year\", \"Mean\", \"Min\", \"Max\", \"Mode\"\n             ))\n  \n```\n\n\nI am more inclined to graphics, the following figure shows not only an increasing mean, but, surprisingly, a widening range.  \n\n\n```{r}\n#| echo: false\nlala_sum %>% ggplot(aes(my_year, mean_n))+\n  geom_point(size=3)+\n  geom_segment(aes(x=my_year, \n                   xend=my_year, \n                   y=min_n, \n                   yend=max_n))+\n  coord_flip()+\n  theme_bw()+\n  labs(title = \"Mean emails per day\",\n       subtitle = \"Segment shows the range\",\n       x = element_blank(),\n       y = element_blank())\n\n\n```\n\n\n### All days were not created equal\n\nOf course, the number of emails somewhat depends on the day of the week. We can easily see a decreasing trend.  \n\n\n```{r}\ntable(emails$wday)\n```\n\n\n\nAlthough the day of the week has influence on the amount of emails received, the time of the day seems to have a stronger, more permanent effect.  \n\n\n```{r}\n#| warning: false\n#| fig.height: 6.0\n\nemails %>%\nfilter(!is.na(wday)) %>%\nggplot(aes(wday, hour_num)) +\n  ggbeeswarm::geom_quasirandom(alpha=0.1)+\n  theme_bw(base_size = 16)+\n  facet_wrap(~simple_label, nrow=2)+\n  labs(y = \"Time of the day\", \n        x = element_blank()) +\n  theme(legend.position = \"none\")+\n  scale_y_continuous(breaks = 2*0:12,\n                     labels = function(x) {paste0(floor(x),\":00\")})\n```\n\n\n### Everything together\n\nIf we pool all the data together, it seems that I receive/send emails at all times, although there is more movement in the accounts around 10:00 and 16:30. Overall, the distributions are quite similar[^little_prince].  \n\n\n```{r}\n#| echo: false\n\nggplot(emails,\n       aes(hour_num)) +\n  geom_density(aes(fill=simple_label), alpha=0.7) +\n  labs(\n    title = \"✉ Email is an all-day activity️\", \n    x = \"Time of the Day\", \n    y = element_blank()) +\n  theme(axis.ticks.y = element_blank(),\n        axis.text.y = element_blank(),\n        panel.grid = element_blank(),\n        panel.background = element_rect(\n          fill=NA, color=\"black\"))+\n   scale_x_continuous(breaks = 2*0:12,\n                     labels = function(x) {paste0(floor(x),\":00\")})+\n  facet_wrap(~simple_label, nrow=2) +\n  scale_fill_manual(values=c(\"orange\", \"gray20\"))+\n  theme(legend.position = \"none\")\n\n\n```\n\n\n#### Just for fun\n\nJust for the fun of data visualization. Here's the same plot but adding `coord_polar` to it. I believe it creates a very weird but good looking thing. It's not really a clock but there's something about it I can't stop looking at[^fix_zero].  \n\n\n```{r}\n#| label: polar-plot\n\ntheme_clock <- theme(axis.ticks.y = element_blank(),\n          axis.ticks.x = element_line(color=\"black\"),\n          axis.text.x = element_text(size=12, face=\"bold\"),\n          axis.text.y = element_blank(),\n          panel.background = element_rect(fill=NA, color=\"black\"),\n          panel.grid.major.y = element_line(color=\"gray50\"),\n          panel.grid.major.x = element_line(color=\"gray10\", linetype = 2),\n          legend.position = \"none\")\n\n\nggplot(emails,\n       aes(x=hour_num)) +\n    geom_density(aes(fill=simple_label), alpha=0.7) +\n    xlab(\"\") + ylab(\"\") +\n    theme_clock +\n    scale_x_continuous(breaks = 2*0:12,\n                     labels = function(x) {paste0(floor(x),\":00\")})+\n    scale_y_continuous(breaks = c(0, 50))+\n    facet_wrap(~simple_label) +\n    scale_fill_manual(values=c(\"orange\", \"gray20\"))+\n    coord_polar() \n\n```\n\n\n### Split in two\n\nAs you can see from the figures above, the emails in the received bucket have two humps (wink, Bactrian camel, little prince), but I send emails at almost all times (except maybe between 2 AM and 5 AM). This is a **bad habit**, I should not be sending emails all the time, I should batch to diminish the costs associated with shifting tasks. I could just put a rule of thumb and check emails only once a day (e.g, 12:00:00). However, this might not be the best decision, because it chunks the response time in two very broad categories (either I get back to you somewhat quick, within 2 hours, or I take almost a full day to reply).  \n\n\n\n```{r}\n\nemails %>% filter(simple_label == \"Received\") %>%\n  mutate(response_period = seconds_to_period(data.table::as.ITime(\"12:00:00\") - my_time),\n         response_seconds = as.numeric(response_period),\n         # If the delay between response and received time is negative\n         # Means the email got in the day before\n         # Adjust for 24 hours delay (86400 seconds in 24 hs)\n         response_seconds = ifelse(response_seconds < 0,\n                                   abs(response_seconds) + 86400,\n                                   response_seconds),\n         response_hours = response_seconds/3600) %>%\n  ggplot(aes(response_hours)) +\n  geom_histogram(bins=30, fill=\"gray80\", color= \"black\")+\n  scale_x_continuous(breaks=seq(0, 36, 2))+\n  labs(title = \"Chunking means either right away or very late replies\", \n       x=\"Time to reply (hours)\") + \n  theme_clock + \n  theme(panel.grid.minor.y = element_blank(),\n        panel.grid.major.y = element_blank(),\n        panel.grid.major.x = element_line(color = \"gray80\", linetype = 2))\n```\n\n\nAdditionally, checking emails only once might make me miss something somewhat fleeting. In general, I want to read things during the time they are relevant (did anybody say free pizza?). \n\nThe primary goal, then, is to minimize the times I check/send emails without **1)** impacting my perceived response rate and **2)** missing out too much info during the day. But that optimization problem is hard to solve and likely a waste of time (trust me, I tried and I'm not that smart). \n\nI believe we can solve it with a rule of thumb anyway. Let's say, I would check emails *twice* a day and respond immediately, unless I need to harness some brain power to create an elaborate response[^checked].  \n\nI just wrote a \"cost function\" and calculated the cost for several combinations of times.\n\n\n```{r}\n#| echo: true\n#| code-fold: true\n#| code-summary: \"Show the code\" \n#| \nvalues <- emails %>%\n  filter(simple_label==\"Received\") %>%\n  mutate(val = as.numeric(seconds_to_period(my_time))) %>%\n  pull(val)\n\n# calculate linear distance to minimize \n\ndist_to_min <- function(values, possible_times){\n \n  min_time <- min(possible_times)\n  max_time <- max(possible_times)\n  # do nothing to first batch\n  corrected_values <- ifelse(values < max_time,\n                           values,\n  # shift the ones answered on next day, this already gives positive distance\n                          86400 - values + min_time)\n\n  \n  to_second <- between(corrected_values, min_time, max_time)\n  second_batch <- corrected_values[to_second]\n  first_batch <- corrected_values[!to_second]  \n  \n  # Calculate distance (should be all positive)\n  dist_second <- max_time - second_batch \n  \n  dist_first <- ifelse(first_batch < min_time,\n                       min_time - first_batch,\n                       corrected_values)\n\n  total_dist <- sum(c(dist_first, dist_second))\n  \n  return(total_dist)\n}\n\n\n```\n\n\nNow we can use our `dist_to_min` function in a loop. We'll calculate from the first second of the day, to the last (86400) every half hour (1800 sec).\n\n\n```{R}\n#| echo: true\n#| code-fold: true\n#| code-summary: \"Show the code\" \n#| \n# Create the data to iterate over\nval <- seq(1, 86400, 1800)\nval <- data.frame(t(combn(val,2)))\nnames(val) <- c(\"Var1\", \"Var2\")\ndistance <- numeric(length=nrow(val))\n\n# For loop...\nfor (i in 1:nrow(val)){\n  possible_times <- val[i, ]\n  \n  distance[i] <- dist_to_min(values, possible_times)\n  \n}\n\n```\n\n\nThe function calculates the distance we want to minimize. The output looks like this. \n\n\n```{R}\n#| echo: false\n#| width: 8.0\n\nggplot(data.frame(distance=distance),\n       aes(x=1:length(distance), distance)) + \n  geom_line() +\n  theme_bw() +\n  labs(x=\"Combinations\", y=\"Latency to reply (seconds)\")\n\n\n```\n\n\nSounds like the combinations we care about are those below 2.5e+8.\n\n\n```{r}\n#| echo: false\n\npp <- data.frame(val[which(distance < 2.5e+8),],\n           distance = distance[which(distance < 2.5e+8)]) %>%\n  arrange(distance) %>%\n  mutate_at(vars(Var1,Var2), .funs = seconds_to_period) \n\npp <- rename(pp, first_batch = Var1, second_batch=Var2)\n\nggplot(pp, aes(as.numeric(first_batch, \"hours\"),\n               as.numeric(second_batch, \"hours\"))) + geom_point(aes(size=distance))+\n  theme_bw()+\n  theme(legend.position = \"none\")+\n#  scale_color_gradient(low=\"green\", high=\"black\")+\n  xlab(\"First Batch (hours)\") + ylab(\"Second Batch (hours)\")+\n  ggtitle(\"When should I answer emails?\",\n          subtitle = \"Increasing size shows increasing cost\")\n\n```\n\n\nAll this long post is to say that, from now on, I will be answering my emails in either one of these combinations.  \n\n\n```{r}\n#| label: answer\n#| echo: false\n#| \npp[1:2, 1:2]\n\n```\n\n\n### A finer grain\n\nJust for the fun of it, let's take a closer look, a second by second analysis. It seems like machine programmed emails peak at 2 and 3 seconds past midnight.\n\n\n```{r}\n#| echo: false\ndata.frame(table(as.POSIXct(emails$my_time), emails$simple_label)) %>%\n  rename(date=Var1, simple_label = Var2) %>%\n  mutate(time = as.character(date),\n         time = str_extract(time, \"[0-9]{2}:[0-9]{2}:[0-9]{2}\")) %>%\n  dplyr::select(time, simple_label, Freq) %>%\n  arrange(desc(Freq)) %>% slice(1:10)\n```\n\n\nWho are these emails coming from anyway? \n\n\n```{r}\n\nemails %>% filter(str_detect(as.character(my_time),  \"00:00:02|00:00:03\")) %>%\n  group_by(from) %>%\n  count() %>% ungroup() %>% arrange(desc(n)) %>%\n  mutate(from = str_extract(from, \"@.+edu\")) %>%\n  slice(1:10)\n  \n```\n\n\nLooks like people at MIT programmed news to be sent seconds after midnight.\n\n## Summary\n\nI have had *a lot* of fun doing this project. I also experienced an enormous amount of frustration with dates. Moreover, every time I thought this project was over, a new little idea for a *not so little* graph came into my mind. Of course, I went after it. I hope this info helps other people take a look at their own personal analytics and make some decisions. I am somewhat happy I have almost all notifications turned off (hence, no Facebook/Twitter/Slack/whatever appearing as top senders). In fact, turning email notifications off is the first thing I do when I sign up for a service/site, I encourage you to do the same.\n\nBatching is something I will start testing. I can't control my input but, hopefully, the distributions of my sent email will start matching the times I designated. More importantly, people will not notice, even if the email input keeps increasing.\n\n\n***\n\nSome people requested me to do the following scatter-plot. I went with the ggbeeswarm version on the text because I find it more appealing.\n\n\n```{r}\n#| echo: false\nlala %>%\n  ggplot(aes(only_date, hour_num))+\n  geom_point(size=0.1)+\n  theme_bw()+\n  scale_y_continuous(breaks = 2*0:12,\n                     labels = function(x) {paste0(floor(x),\":00\")})+\n  scale_x_date(date_breaks = \"1 year\", labels = date_format(\"%Y\"))+\n  xlab(\"\") + ylab(\"Time of the day\")\n\n```\n\n\n***\n\nI excluded parts of the code because it was too much. I am happy to share if requested!\n\n***\n\n**Sources:**  \n\nhttps://jellis18.github.io/post/2018-01-17-mail-analysis/  \nhttps://blog.stephenwolfram.com/2012/03/the-personal-analytics-of-my-life/  \nhttps://uc-r.github.io/kmeans_clustering  \n\n[^stay]: You could actually stay in python (follow https://jellis18.github.io/post/2018-01-17-mail-analysis/). I'm way more comfortable with R for analysis and I only wanted python because I had the copy-paste version of getting my `.mbox` file to `.csv` fast.\n[^Mendeley]: I had signed up for Mendeley *before* Elsevier bought it...I'm not quite happy about it now, but at least I still get paper recommendations.\n[^my_fault]: I know I could just *unsubscribe* to these kind of things, just the way I do with 99% of all other aggressive garbage. I just didn't do it for these senders.\n[^little_prince]: Do they look like an elephant inside a boa or a hat? \n[^fix_zero]: Please, if you know how to make the 0:00 or 24:00 appear on the center, reach out! I couldn't figure it out.\n[^checked]: If you have as much free time as me, you can run a `kmeans(...)`. My emails actually turned out to be around 2 clusters. \n\n","srcMarkdownNoYaml":"\n\n```{r}\n#| echo: false\n#| warning: false\n#| message: false\n#| results: hide\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(scales) # for time scaling\n```\n\n\nI am keen on tracking things. Some people call this *personal analytics*, I call it fun. In this post, I will explore how to extract your email information using python and R. My goal is to do some analysis on the data and, hopefully, explore different visualizations that can inform future behavior. \n\n\n## Emails from Gmail\n\nReading emails in python is quite simple, we need to import the mailbox library. My file is called `correo` and comes from downloading my Gmail stuff out of https://takeout.google.com/settings/takeout. It can take a while so be patient.\n\n\n```{python}\n#| eval: false\nimport mailbox\n\n# read in mbox\nmbox = mailbox.mbox('correo')\n\n# Show what we have here\nfor key in mbox[0].keys():\n    print(key)\n\n```\n\n\nThis should return something that looks like:\n\n```\nX-GM-THRID\nX-Gmail-Labels\nReceived\nMIME-Version\nDate\nTo\nFrom\nSubject\nContent-Type\nContent-Disposition\nMessage-Id\n```\n\nLet's save the info we care about into a new file. We select `subject`, `from`, `date`, `to`, and some variables that allow us to keep track of things (i.e, labels and threads).\n\n\n```{python}\n#| eval: false\nimport csv\nwith open(\"gmail_mbox.csv\", \"w\") as outfile:\n    writer = csv.writer(outfile)\n    for message in mbox:\n        writer.writerow([message['subject'], message['from'],\n        message['date'], message['to'],\n        message['X-Gmail-Labels'], message['X-GM-THRID']])\n\n\n```\n\n\n\n## Emails from Thunderbird\n\nI keep 3 accounts in Thunderbird. Using the [ImportExportTools Add-on](https://addons.thunderbird.net/en-US/thunderbird/addon/importexporttools/), I exported things into `.mbox` format. Following a similar procedure to the one depicted above, I got the other three accounts exported to `.csv` files. Just be sure you select the correct keys (see example below, this might change for other email clients). \n\n\n```{python}\n#| eval: false\n\nwith open(\"MIT_mbox.csv\", \"w\") as outfile:\n    writer = csv.writer(outfile)\n    for message in mbox:\n        writer.writerow([message['Subject'],\n        message['From'], message['Date'],\n        message['To'],message['Status']])\n\n```\n\n\n\n## Data cleaning\n\nLet's switch from python to R[^stay]. \n\n\n```{r}\n#| label: read-into-R\ndf <- read.csv(\"data/gmail_mbox.csv\",\n               header=FALSE, encoding=\"UTF-8\")\n\nnames(df) <- c('subject', 'from', 'date', 'to', 'label', 'thread')\n\n```\n\n\n\nUnfortunately, emails come tagged (things like `\"\\\\?=\"` and other nasty stuff) and you might have to deal with different encodings (the perks of speaking multiple languages). As an example, let's see what Quora sends me.\n\n\n```{r}\ndf %>% filter(str_detect(from, \"Quora\")) %>% dplyr::select(from) %>% slice(1:10)\n```\n\n\nThat's nasty...Let's do some cleaning. This function comes really handy for text replacement.\n\n\n```{r}\n#| label: sourcing-github\n#| message: false\n\n# originally sourced from\ndevtools::source_url(\"https://raw.githubusercontent.com/hrbrmstr/hrbrmisc/master/R/qp.r\")\n\n```\n\n\n\nWe are going to modify the function a bit, we add `x` as the string we pass for cleaning and we will remove the tags progressively.\n\n\n```{r}\n\nqp_decode <- function(x) {\n  \n  # other email taggs\n  x <- stringr::str_remove_all(x, pattern = \"=\\\\?[U-u][T-t][F-f]-8\\\\?[Q-q]\\\\?\")\n  x <- stringr::str_remove_all(x, pattern = \"\\\\?=\")\n  x <- stringr::str_remove_all(x, \"=\\\\?[I-i][S-s][O-o]-8859-1\\\\?Q\\\\?\")\n  \n  stringi::stri_replace_all_fixed(x, qp_before, qp_after, vectorize_all=FALSE)\n  \n}\n\n```\n\n\nWe are ready to use our super cool function and clean the text! Not perfect, but gets us 90% of the way.\n\n\n```{r}\n#| label: clean-emails\n\ndf <- df %>% mutate_at(.vars = vars(subject, from, to, label), .funs = qp_decode) \n\n```\n\n\nLet's see how emails from Quora changed with this new encoding:\n\n\n```{r}\n#| label: quora-example\ndf %>% filter(str_detect(from, \"Quora\")) %>% dplyr::select(from) %>% slice(1:10)\n```\n\n\nLet's filter those from \"Received\" or \"Sent\" (in Spanish, \"Recibidos\" or \"Enviado\").\n\n\n```{r}\nemails <- df %>%\n  filter(str_detect(label, \"Recibidos|Enviado\"))\n```\n\n\nTo save you from reading a considerable amount of code, I will load the other accounts and modify them accordingly in the background. I will finally merge everything together. Just enjoy the kitten while the code is running in my machine.\n\n![](001-kitty.jpg)\n\n\n\n```{r}\n#| echo: false\n#| message: false\n#| results: hide\n#| warning: false\n#| code-fold: true\n#| code-summary: \"A lot of code for reading/parsing\"\n\nMIT_sent <- read.csv('data/MIT_Sent.csv',\n                     header=FALSE,\n                     stringsAsFactors = FALSE)\nnames(MIT_sent) <- c('subject', 'from', 'date', 'to', 'label')\nMIT_sent$label <- \"Sent\"\n\nMIT_received <- read.csv(\"data/MIT_inbox.csv\",\n                         header=FALSE,\n                         stringsAsFactors = FALSE)\nnames(MIT_received) <- c('subject', 'from', 'date', 'to', 'label')\nMIT_received$label <- \"Received\"\n\nhotmail_sent <- read.csv('data/hotmail_Sent.csv',\n                         header=FALSE,\n                         stringsAsFactors = FALSE)\nnames(hotmail_sent) <- c('subject', 'from', 'date', 'to', 'label')\nhotmail_sent$label <- \"Sent\"\n\nhotmail_received <- read.csv(\"data/hotmail_inbox.csv\",\n                             header=FALSE,\n                             stringsAsFactors = FALSE)\nnames(hotmail_received) <- c('subject', 'from', 'date', 'to', 'label')\nhotmail_received$label <- \"Received\"\n\numass_sent <- read.csv(\"data/umass_Sent.csv\",\n                       header=FALSE,\n                       stringsAsFactors = FALSE)\nnames(umass_sent) <- c('subject', 'from', 'date', 'to', 'label')\numass_sent$label <- \"Sent\"\n\numass_received <- read.csv(\"data/umass_inbox.csv\",\n                           header=FALSE,\n                           stringsAsFactors = FALSE)\nnames(umass_received) <- c('subject', 'from', 'date', 'to', 'label')\numass_received$label <- \"Received\"\n\n\n# Bind everything\n\nall_accounts <- bind_rows(list(MIT_received = MIT_received,\n               MIT_sent = MIT_sent,\n               hotmail_received = hotmail_received,\n               hotmail_sent = hotmail_sent,\n               umass_received = umass_received,\n               umass_sent = umass_sent), .id = \"account\") %>%\n  mutate(account = str_remove(account, \"_received|_sent\"))\n\n\nemails <- emails %>% mutate(account = \"Gmail\") %>%\n  bind_rows(all_accounts) %>%\n  mutate(simple_label = ifelse(\n    str_detect(label, pattern = \"Recibido|Received\"),\n    \"Received\", \n    \"Sent\"))\n\n```\n\n\n## Analysis\n\nThere's still some stuff to clean, but I'd rather go into the analysis. So, let's get some questions to guide our purpose:\n\n1) Who sends me the most emails? Who receives emails from me?\n1) When do I get emails (mostly)?\n1) When should I do something about it (aka, reply)?\n\n> **Warning:** We have to dance with parsing dates and times. \n> I highly recommend being familiar with `lubridate` (for example, see https://rdrr.io/cran/lubridate/man/parse_date_time.html).\n\n### Most frequent senders\n\nJust because I'm curious, let's take a look at who are the all time senders!\n\n\n```{r}\n#| echo: false\n#| warning: false\n#| message: false\nemails %>%\n  # Filter me out\n  filter(!str_detect(from, \"matiasandina|mandina\"))%>%\n  # Count the people\n  count(from) %>% arrange(desc(n)) %>% slice(1:10) %>%\n  mutate(from = str_remove(from, \"\\\\<.+\")) %>%\n  separate(from, into = c(\"from\", \"lala\"), sep=\" \") %>%\n  dplyr::select(-lala)\n```\n\n\nIt's cool to know that my lingering feeling (\"wow...`Quora` just spams the hell out of me\") is supported by data. Other big spammers are, of course, the Bank and Amazon. People I work with and friends come high up too. Funny to see Mendeley and Pubchase on the top ten, it's been a long journey of them sending me papers, thank you for that[^Mendeley].\n\n### From me to you\n\nLet's try to find the people I directly send the most emails to. I tend to send a lot of automatic reminders via email to myself so I removed me from the destination.\n\n\n```{r}\n#| echo: false\n#| message: false\n#| warning: false\nemails %>% filter(str_detect(from, \"matiasandina|mandina\") & !str_detect(to, \"matiasandina|mandina\"))%>%\n  count(to) %>% arrange(desc(n)) %>% slice(1:5) %>%\n  mutate(to = str_remove(to, \"\\\\<.+\")) %>%\n  separate(to, into = c(\"to\", \"lala\"), sep=\" \") %>%\n  mutate(to=ifelse(str_detect(to, \"feld\"), \"Mariana\", to),\n         to=str_remove(to, \"_.+\")) %>%\n  dplyr::select(-lala)\n```\n\n\nLooks like both my former advisers get most of my output (yes, same name first name, not related).\n\n## Working with dates and times\n\nEvery time I have to deal with dates, I have a miniature panic attack. As a general rule, you have to have all the variables that you want to use as separate columns (i.e, year, month, day, week_day, time, ...). The `lubridate` package helps a lot, but it's still quite an effort.\n\nWorking only with times of the day, regardless of date itself is problematic. Working with periods is difficult, so `as.numeric(x, \"hour\")` is a friend.\n\nHere's a hint of how the `date` column in the original data actually looks like. This may or might not look the same way for you, it depends on your date settings.  \n\n\n\n```{r}\n#| label: show-date\n\nemails %>% dplyr::select(date) %>% slice(1:5)\n\n```\n\n\n\nLet's create all the variables we need. It seems like a lot because it *should* work out of the box and it doesn't, but it's actually somewhat straight-forward to get most of what we want.  \n\n\n\n```{r}\n#| label: modify-dates\n#| message: false\n#| warning: false\n#| code-fold: true\n#| code-summary: \"Again, a bunch of cleaning stuff\"\n\nemails <- emails %>%\n  separate(date, into=c(\"wday\", \"date\"), sep = \", \") %>%\n  # Fixing the fact that some dates do not have weekday\n  mutate(to_fix = ifelse(\n    wday %in% wday(1:7, label=TRUE , abbr = TRUE), \n    \"OK\", \n    \"fix\"),\n    date = ifelse(to_fix == \"OK\", date, wday),\n    wday=ifelse(to_fix==\"OK\", wday, NA),\n    wday=fct_relevel(wday,\n                     levels=c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"))) %>%\n  dplyr::select(-to_fix)\n\n\n# remove emails with no date\nemails <- emails %>% \n  dplyr::mutate(date = dplyr::na_if(date, \"\")) %>%\n  filter(!is.na(date))\n\n\n# switch everything to my timezone\n# Sys.timezone() gives \"America/New_York\"\n# some parsing migth fail\n\nemails <- emails %>%\n  mutate(date = str_remove(date, \"\\\\(.+\\\\)\"),\n         my_date = dmy_hms(date, tz = \"America/New_York\"),\n         my_day = day(my_date),\n         my_month = month(my_date),\n         my_month_abb = month(my_date, label=TRUE, abbr = TRUE),\n         my_year = year(my_date),\n         my_time = data.table::as.ITime(my_date),\n         hour_num = my_time %>% as.integer() %>% `/` (60*60),\n         only_date = ymd(paste(my_year, my_month, my_day, sep=\"-\")))\n\n\n```\n\n\n\n## Birdseye\n\nLet's look at how the whole email movement looks like. In the last couple of years, I clearly felt the load rising and rising. The lack of data in the early years is mostly due to me not downloading everything from the Hotmail account (it's too late, too far in the past to fix :P). Besides, the trend likely holds quite well.\n\n\n\n```{r}\n#| label: tile-plot\n#| echo: false\n#| height: 6.0\n# Make summary\ntile_data <- emails %>% \n  group_by(my_year, my_month, my_day) %>%\n  count() %>%\n  ungroup() %>% \n  group_by(my_year, my_month) %>%\n  summarise(month_count = sum(n))\n\nggplot(tile_data, aes(my_month, \n                      my_year,\n                      fill=month_count)) +\n    geom_tile(color=\"white\", size=0.8)+\n    scale_fill_viridis_c()+\n    scale_x_continuous(expand=c(0,0), breaks = 1:12, labels = month.abb)+\n    scale_y_continuous(breaks = 2003:2019)+\n    labs(x=\"\", y=\"\", fill=\"Monthly count\")+\n    theme(axis.ticks = element_blank(), \n          panel.background = element_blank(),\n          legend.position = \"bottom\",\n          legend.key.width = unit(1, \"cm\"),\n          strip.text = element_text(hjust = 0.01,\n                                    face = \"bold\",\n                                    size = 12),\n              text = element_text(size=16))+\n      ggtitle(\"Total emails in all accounts\")\n\n```\n\n\nIf we split by input and output, we can easily see that the input-output ratio went nuts when I moved to the US. \n\n\n```{r}\n#| echo: false\n# sent vs received\nemails %>%\n  group_by(simple_label, my_year) %>%\n  count() %>%\n  ggplot(aes(my_year, n, color=simple_label)) +\n  geom_vline(xintercept = 2016, lty=2)+\n  geom_line()+\n  geom_point(size=2.5) +\n  scale_color_manual(values = c(\"orange\", \"black\"))+\n  theme_bw()+ \n  theme(panel.grid = element_blank())+\n  annotate(\"text\", label=c(\"ARG\",\"US\"), \n           x=c(2015, 2016.8), y=-1)+\n  labs(\n    title = \"Moving to the US means a lot more mail\",\n    x=element_blank(), \n    y=\"Number of emails\", color=\"\")+\n  scale_x_continuous(breaks = seq(2003, 2019, by=2))\n\n```\n\n\nThis is not really surprising, given the amount of unsolicited advertising I started getting since the move. Yes, I'm talking to you again Quora/Amazon/people trying to sell me stuff[^my_fault]. Of course, University related chains likely take a big chunk of the pie. \n\nI don't feel like parsing out each sender out of the sheer amount. I have had the Gmail and Hotmail accounts for more than 10 years, but the University email is something relatively recent. All in all, considering the time I've had each account, the input rate coming from universities worries me. Here are the total email for each account: \n\n\n```{r}\ntable(emails$account)\n```\n\n\n\n### When\n\nLet's add the time of the day to the equation. This plot was made using `ggbeeswarm` package, I highly recommend checking it, it's pure power. I got help to put the labels in the `y` axis from '00:00' to '24:00'. You can find a toy example in [this StackOverflow question](https://stackoverflow.com/questions/55463735/how-to-force-scale-y-datetime-scale-to-show-2400-instead-of-0000-in-r/55464995#55464995).\n\n\n```{r}\n#| echo: false\n#| fig.height: 8.0\n\np1 <- ggplot(emails, aes(factor(my_year),\n                   hour_num)) +\n  ggbeeswarm::geom_quasirandom(alpha=0.1)+\n  #scale_y_datetime(breaks = \"2 hour\",\n  #                 date_labels= \"%H:%M\")+\n  #scale_x_discrete(position = \"top\")+\n  theme_bw(base_size = 16)+\n  facet_wrap(~simple_label, nrow=2)+\n  xlab(\"\") + ylab(\"Time of the day\")+\n  #scale_color_manual(values=c(\"darkorange\", \"black\"))+\n  theme(legend.position = \"none\")+\n  scale_y_continuous(breaks = 2*0:12,\n                     labels = function(x) {paste0(floor(x),\":00\")})\n\n\np2 <- ggplot(emails, aes(factor(my_year), as.POSIXct(my_time, format = \"%H:%M:%S\"))) +\n  geom_jitter(alpha=0.1)+\n  ylab(\"\") +\n  scale_y_datetime(breaks = \"2 hour\",\n                   date_labels= \"%H:%M\")+\n  theme_bw()+\n  facet_wrap(~simple_label, nrow = 2)\n\n\n# We need to make the account into factor\nemails$account <- factor(emails$account, levels=c(\"Gmail\", \"MIT\", \"umass\", \"hotmail\"))\n\nhistogram <- emails %>% group_by(account, my_year) %>%\n    count() %>% #ungroup() %>% group_by(my_year) %>%\n    #mutate(rel_count = n/sum(n)) %>%\n    ggplot(aes(my_year, y=n, fill=account))+\n    geom_col()+\n    theme(panel.background = element_blank(), legend.position = \"bottom\",\n          text = element_text(size=16))+\n    scale_fill_grey() +\n    scale_x_continuous(expand = c(0, 0.1), breaks=2003:2019)+\n    labs(x=\"\", y=\"Number of emails\")\n\n#cowplot::plot_grid(p1,histogram, nrow = 2, rel_heights = c(2, #1))\n\n```\n\n\n![](email-analysis-beeswarm.png)\n\n#### Daily news\n\nWhat's the average number of emails per day? I'm including all the emails in from 2015 to 2019, including those that go directly to trash.\n\n\n```{r}\n#| echo: false\n#| warning: false\n#| message: false\nlala <- df %>% \n  bind_rows(all_accounts) %>%\n  separate(date, into=c(\"wday\", \"date\"), sep = \", \") %>%\n  # Fixing the fact that some dates do not have weekday\n  mutate(\n    to_fix = ifelse(\n    wday %in% wday(1:7, label=TRUE , abbr = TRUE), \n    \"OK\",\n    \"fix\"),\n     date = ifelse(to_fix == \"OK\", date, wday),\n     wday=ifelse(to_fix==\"OK\", wday, NA),\n     wday=fct_relevel(wday,\n                      levels=c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"))) %>%\n  dplyr::select(-to_fix) %>%\n  mutate(date = dplyr::na_if(date, \"\")) %>%\n  filter(!is.na(date)) %>%\n  mutate(date = str_remove(date, \"\\\\(.+\\\\)\"),\n         my_date = dmy_hms(date, tz = \"America/New_York\"),\n         my_day = day(my_date),\n         my_month = month(my_date),\n         my_month_abb = month(my_date, label=TRUE, abbr = TRUE),\n         my_year = year(my_date),\n         my_time = data.table::as.ITime(my_date),\n         hour_num = my_time %>% as.integer() %>% `/` (60*60),\n         only_date = ymd(paste(my_year, my_month, my_day, sep=\"-\")))\n\n\nlala %>% group_by(my_year, only_date) %>%\n  count() %>%\n  filter(my_year %in% c(2015:2019)) %>%\n  ggplot(aes(n)) + geom_density(fill=\"gray50\") + \n  theme_bw()+\n  scale_x_continuous(breaks=seq(0, 150, 15))+\n  labs(title = \"Email amount per day\", \n       x = \"Emails/day\")\n\n\n```\n\n\nFor those looking for some tabulated info, here it is:  \n\n\n```{r}\n#| echo: false\nlala_sum <- lala %>% group_by(my_year, only_date) %>%\n  count() %>% filter(my_year %in% c(2015:2019)) %>%\n  group_by(my_year) %>%\n  summarise(mean_n = mean(n),\n            min_n = range(n)[1],\n            max_n = range(n)[2],\n            mode_n = which.max(tabulate(n)))\n\nknitr::kable(lala_sum, \n             caption = \"Number of emails per day received in all accounts\",\n             digits = 1,\n             col.names = c(\n               \"Year\", \"Mean\", \"Min\", \"Max\", \"Mode\"\n             ))\n  \n```\n\n\nI am more inclined to graphics, the following figure shows not only an increasing mean, but, surprisingly, a widening range.  \n\n\n```{r}\n#| echo: false\nlala_sum %>% ggplot(aes(my_year, mean_n))+\n  geom_point(size=3)+\n  geom_segment(aes(x=my_year, \n                   xend=my_year, \n                   y=min_n, \n                   yend=max_n))+\n  coord_flip()+\n  theme_bw()+\n  labs(title = \"Mean emails per day\",\n       subtitle = \"Segment shows the range\",\n       x = element_blank(),\n       y = element_blank())\n\n\n```\n\n\n### All days were not created equal\n\nOf course, the number of emails somewhat depends on the day of the week. We can easily see a decreasing trend.  \n\n\n```{r}\ntable(emails$wday)\n```\n\n\n\nAlthough the day of the week has influence on the amount of emails received, the time of the day seems to have a stronger, more permanent effect.  \n\n\n```{r}\n#| warning: false\n#| fig.height: 6.0\n\nemails %>%\nfilter(!is.na(wday)) %>%\nggplot(aes(wday, hour_num)) +\n  ggbeeswarm::geom_quasirandom(alpha=0.1)+\n  theme_bw(base_size = 16)+\n  facet_wrap(~simple_label, nrow=2)+\n  labs(y = \"Time of the day\", \n        x = element_blank()) +\n  theme(legend.position = \"none\")+\n  scale_y_continuous(breaks = 2*0:12,\n                     labels = function(x) {paste0(floor(x),\":00\")})\n```\n\n\n### Everything together\n\nIf we pool all the data together, it seems that I receive/send emails at all times, although there is more movement in the accounts around 10:00 and 16:30. Overall, the distributions are quite similar[^little_prince].  \n\n\n```{r}\n#| echo: false\n\nggplot(emails,\n       aes(hour_num)) +\n  geom_density(aes(fill=simple_label), alpha=0.7) +\n  labs(\n    title = \"✉ Email is an all-day activity️\", \n    x = \"Time of the Day\", \n    y = element_blank()) +\n  theme(axis.ticks.y = element_blank(),\n        axis.text.y = element_blank(),\n        panel.grid = element_blank(),\n        panel.background = element_rect(\n          fill=NA, color=\"black\"))+\n   scale_x_continuous(breaks = 2*0:12,\n                     labels = function(x) {paste0(floor(x),\":00\")})+\n  facet_wrap(~simple_label, nrow=2) +\n  scale_fill_manual(values=c(\"orange\", \"gray20\"))+\n  theme(legend.position = \"none\")\n\n\n```\n\n\n#### Just for fun\n\nJust for the fun of data visualization. Here's the same plot but adding `coord_polar` to it. I believe it creates a very weird but good looking thing. It's not really a clock but there's something about it I can't stop looking at[^fix_zero].  \n\n\n```{r}\n#| label: polar-plot\n\ntheme_clock <- theme(axis.ticks.y = element_blank(),\n          axis.ticks.x = element_line(color=\"black\"),\n          axis.text.x = element_text(size=12, face=\"bold\"),\n          axis.text.y = element_blank(),\n          panel.background = element_rect(fill=NA, color=\"black\"),\n          panel.grid.major.y = element_line(color=\"gray50\"),\n          panel.grid.major.x = element_line(color=\"gray10\", linetype = 2),\n          legend.position = \"none\")\n\n\nggplot(emails,\n       aes(x=hour_num)) +\n    geom_density(aes(fill=simple_label), alpha=0.7) +\n    xlab(\"\") + ylab(\"\") +\n    theme_clock +\n    scale_x_continuous(breaks = 2*0:12,\n                     labels = function(x) {paste0(floor(x),\":00\")})+\n    scale_y_continuous(breaks = c(0, 50))+\n    facet_wrap(~simple_label) +\n    scale_fill_manual(values=c(\"orange\", \"gray20\"))+\n    coord_polar() \n\n```\n\n\n### Split in two\n\nAs you can see from the figures above, the emails in the received bucket have two humps (wink, Bactrian camel, little prince), but I send emails at almost all times (except maybe between 2 AM and 5 AM). This is a **bad habit**, I should not be sending emails all the time, I should batch to diminish the costs associated with shifting tasks. I could just put a rule of thumb and check emails only once a day (e.g, 12:00:00). However, this might not be the best decision, because it chunks the response time in two very broad categories (either I get back to you somewhat quick, within 2 hours, or I take almost a full day to reply).  \n\n\n\n```{r}\n\nemails %>% filter(simple_label == \"Received\") %>%\n  mutate(response_period = seconds_to_period(data.table::as.ITime(\"12:00:00\") - my_time),\n         response_seconds = as.numeric(response_period),\n         # If the delay between response and received time is negative\n         # Means the email got in the day before\n         # Adjust for 24 hours delay (86400 seconds in 24 hs)\n         response_seconds = ifelse(response_seconds < 0,\n                                   abs(response_seconds) + 86400,\n                                   response_seconds),\n         response_hours = response_seconds/3600) %>%\n  ggplot(aes(response_hours)) +\n  geom_histogram(bins=30, fill=\"gray80\", color= \"black\")+\n  scale_x_continuous(breaks=seq(0, 36, 2))+\n  labs(title = \"Chunking means either right away or very late replies\", \n       x=\"Time to reply (hours)\") + \n  theme_clock + \n  theme(panel.grid.minor.y = element_blank(),\n        panel.grid.major.y = element_blank(),\n        panel.grid.major.x = element_line(color = \"gray80\", linetype = 2))\n```\n\n\nAdditionally, checking emails only once might make me miss something somewhat fleeting. In general, I want to read things during the time they are relevant (did anybody say free pizza?). \n\nThe primary goal, then, is to minimize the times I check/send emails without **1)** impacting my perceived response rate and **2)** missing out too much info during the day. But that optimization problem is hard to solve and likely a waste of time (trust me, I tried and I'm not that smart). \n\nI believe we can solve it with a rule of thumb anyway. Let's say, I would check emails *twice* a day and respond immediately, unless I need to harness some brain power to create an elaborate response[^checked].  \n\nI just wrote a \"cost function\" and calculated the cost for several combinations of times.\n\n\n```{r}\n#| echo: true\n#| code-fold: true\n#| code-summary: \"Show the code\" \n#| \nvalues <- emails %>%\n  filter(simple_label==\"Received\") %>%\n  mutate(val = as.numeric(seconds_to_period(my_time))) %>%\n  pull(val)\n\n# calculate linear distance to minimize \n\ndist_to_min <- function(values, possible_times){\n \n  min_time <- min(possible_times)\n  max_time <- max(possible_times)\n  # do nothing to first batch\n  corrected_values <- ifelse(values < max_time,\n                           values,\n  # shift the ones answered on next day, this already gives positive distance\n                          86400 - values + min_time)\n\n  \n  to_second <- between(corrected_values, min_time, max_time)\n  second_batch <- corrected_values[to_second]\n  first_batch <- corrected_values[!to_second]  \n  \n  # Calculate distance (should be all positive)\n  dist_second <- max_time - second_batch \n  \n  dist_first <- ifelse(first_batch < min_time,\n                       min_time - first_batch,\n                       corrected_values)\n\n  total_dist <- sum(c(dist_first, dist_second))\n  \n  return(total_dist)\n}\n\n\n```\n\n\nNow we can use our `dist_to_min` function in a loop. We'll calculate from the first second of the day, to the last (86400) every half hour (1800 sec).\n\n\n```{R}\n#| echo: true\n#| code-fold: true\n#| code-summary: \"Show the code\" \n#| \n# Create the data to iterate over\nval <- seq(1, 86400, 1800)\nval <- data.frame(t(combn(val,2)))\nnames(val) <- c(\"Var1\", \"Var2\")\ndistance <- numeric(length=nrow(val))\n\n# For loop...\nfor (i in 1:nrow(val)){\n  possible_times <- val[i, ]\n  \n  distance[i] <- dist_to_min(values, possible_times)\n  \n}\n\n```\n\n\nThe function calculates the distance we want to minimize. The output looks like this. \n\n\n```{R}\n#| echo: false\n#| width: 8.0\n\nggplot(data.frame(distance=distance),\n       aes(x=1:length(distance), distance)) + \n  geom_line() +\n  theme_bw() +\n  labs(x=\"Combinations\", y=\"Latency to reply (seconds)\")\n\n\n```\n\n\nSounds like the combinations we care about are those below 2.5e+8.\n\n\n```{r}\n#| echo: false\n\npp <- data.frame(val[which(distance < 2.5e+8),],\n           distance = distance[which(distance < 2.5e+8)]) %>%\n  arrange(distance) %>%\n  mutate_at(vars(Var1,Var2), .funs = seconds_to_period) \n\npp <- rename(pp, first_batch = Var1, second_batch=Var2)\n\nggplot(pp, aes(as.numeric(first_batch, \"hours\"),\n               as.numeric(second_batch, \"hours\"))) + geom_point(aes(size=distance))+\n  theme_bw()+\n  theme(legend.position = \"none\")+\n#  scale_color_gradient(low=\"green\", high=\"black\")+\n  xlab(\"First Batch (hours)\") + ylab(\"Second Batch (hours)\")+\n  ggtitle(\"When should I answer emails?\",\n          subtitle = \"Increasing size shows increasing cost\")\n\n```\n\n\nAll this long post is to say that, from now on, I will be answering my emails in either one of these combinations.  \n\n\n```{r}\n#| label: answer\n#| echo: false\n#| \npp[1:2, 1:2]\n\n```\n\n\n### A finer grain\n\nJust for the fun of it, let's take a closer look, a second by second analysis. It seems like machine programmed emails peak at 2 and 3 seconds past midnight.\n\n\n```{r}\n#| echo: false\ndata.frame(table(as.POSIXct(emails$my_time), emails$simple_label)) %>%\n  rename(date=Var1, simple_label = Var2) %>%\n  mutate(time = as.character(date),\n         time = str_extract(time, \"[0-9]{2}:[0-9]{2}:[0-9]{2}\")) %>%\n  dplyr::select(time, simple_label, Freq) %>%\n  arrange(desc(Freq)) %>% slice(1:10)\n```\n\n\nWho are these emails coming from anyway? \n\n\n```{r}\n\nemails %>% filter(str_detect(as.character(my_time),  \"00:00:02|00:00:03\")) %>%\n  group_by(from) %>%\n  count() %>% ungroup() %>% arrange(desc(n)) %>%\n  mutate(from = str_extract(from, \"@.+edu\")) %>%\n  slice(1:10)\n  \n```\n\n\nLooks like people at MIT programmed news to be sent seconds after midnight.\n\n## Summary\n\nI have had *a lot* of fun doing this project. I also experienced an enormous amount of frustration with dates. Moreover, every time I thought this project was over, a new little idea for a *not so little* graph came into my mind. Of course, I went after it. I hope this info helps other people take a look at their own personal analytics and make some decisions. I am somewhat happy I have almost all notifications turned off (hence, no Facebook/Twitter/Slack/whatever appearing as top senders). In fact, turning email notifications off is the first thing I do when I sign up for a service/site, I encourage you to do the same.\n\nBatching is something I will start testing. I can't control my input but, hopefully, the distributions of my sent email will start matching the times I designated. More importantly, people will not notice, even if the email input keeps increasing.\n\n\n***\n\nSome people requested me to do the following scatter-plot. I went with the ggbeeswarm version on the text because I find it more appealing.\n\n\n```{r}\n#| echo: false\nlala %>%\n  ggplot(aes(only_date, hour_num))+\n  geom_point(size=0.1)+\n  theme_bw()+\n  scale_y_continuous(breaks = 2*0:12,\n                     labels = function(x) {paste0(floor(x),\":00\")})+\n  scale_x_date(date_breaks = \"1 year\", labels = date_format(\"%Y\"))+\n  xlab(\"\") + ylab(\"Time of the day\")\n\n```\n\n\n***\n\nI excluded parts of the code because it was too much. I am happy to share if requested!\n\n***\n\n**Sources:**  \n\nhttps://jellis18.github.io/post/2018-01-17-mail-analysis/  \nhttps://blog.stephenwolfram.com/2012/03/the-personal-analytics-of-my-life/  \nhttps://uc-r.github.io/kmeans_clustering  \n\n[^stay]: You could actually stay in python (follow https://jellis18.github.io/post/2018-01-17-mail-analysis/). I'm way more comfortable with R for analysis and I only wanted python because I had the copy-paste version of getting my `.mbox` file to `.csv` fast.\n[^Mendeley]: I had signed up for Mendeley *before* Elsevier bought it...I'm not quite happy about it now, but at least I still get paper recommendations.\n[^my_fault]: I know I could just *unsubscribe* to these kind of things, just the way I do with 99% of all other aggressive garbage. I just didn't do it for these senders.\n[^little_prince]: Do they look like an elephant inside a boa or a hat? \n[^fix_zero]: Please, if you know how to make the 0:00 or 24:00 appear on the center, reach out! I couldn't figure it out.\n[^checked]: If you have as much free time as me, you can run a `kmeans(...)`. My emails actually turned out to be around 2 clusters. \n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"message":false,"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"filters":["social-embeds","social-share"],"include-after-body":["../../footer.html"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.433","editor":"visual","comments":{"utterances":{"repo":"matiasandina/Webpage","label":"Comment","theme":"github-dark"}},"theme":{"light":"cosmo","dark":"cosmo"},"code-copy":true,"title-block-banner":true,"license":"CC BY","toc-title":"Table of contents","toc-location":"right","author":[{"name":"Matias Andina","url":"https://matiasandina.netlify.app","affiliation":"Massachusetts Institute of Technology","orcid":"0000-0002-1996-2539"}],"citation":true,"share":{"permalink":"https://matiasandina.netlify.app/","description":"Matias Andina","twitter":true,"facebook":true,"reddit":true,"stumble":true,"tumblr":false,"linkedin":true,"email":true,"mastodon":true},"title":"Email analysis","date":"2019-04-06","categories":["R","learning"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}