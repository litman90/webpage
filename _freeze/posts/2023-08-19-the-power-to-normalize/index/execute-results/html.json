{
  "hash": "400dc693b5fa8fac8408fee9ecde7818",
  "result": {
    "markdown": "---\ntitle: \"The Power to Normalize\"\ndescription: \"A tidytuesday inspired foray into the YeoJohnson transformation\"\ndate: \"2023-08-19\"\ncategories: [\"R\", 'dataviz']\ncode-fold: true\necho: true\n---\n\n\nI started participating in the [Tidytuesday project](https://github.com/rfordatascience/tidytuesday) to practice my visualization skills, while using datasets that come from sources that I'm not used to. In addition, I enjoy checking what *other* people do with the same dataset. I find that others are way more creative than I am, and I borrow heavily!\n\nThe challenge for Week 33 of 2023 was to perform viz on the `spam` dataset.\n\n## When PCA fails\n\nThe `spam` dataset presents heavily skewed distributions for variables that serve as predictors of spam email. Because it was a dataset with 6 numeric variables and a single binary predictor, my initial idea was to perform a quick and dirty PCA.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse, warn.conflicts = FALSE)\nlibrary(tidytuesdayR)\nlibrary(paletteer)\nlibrary(FactoMineR)\nlibrary(factoextra)\nlibrary(scales, warn.conflicts = FALSE)\n\n# load the data\nspam <- tt_load(2023, week=33)$spam\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tDownloading file 1 of 1: `spam.csv`\n```\n:::\n\n```{.r .cell-code}\nspam$yesno <- dplyr::if_else(spam$yesno == \"y\", \"spam\", \"email\")\npc <- prcomp(spam[, 1:6], center = TRUE, scale. = TRUE)\n# make it a tibble for ggplot plotting\npc_data <- pc$x[, 1:2] %>% as_tibble()\npc_data$yesno <- spam$yesno\n\npc_ori_plot <- ggplot(pc_data, \n       aes(PC1, PC2, color = yesno)) +\n  geom_point() +\n  coord_equal() +\n  scale_color_paletteer_d(\"awtools::a_palette\") +\n  ggthemes::theme_base()+\n  theme(legend.position = \"bottom\",\n        plot.background =  element_rect(color = NA),\n        legend.background = element_rect(fill = \"gray90\"),\n        legend.key = element_rect(fill = \"gray90\"),\n        panel.background = element_rect(fill=\"#81AE5C\")) +\n  labs(color = element_blank())\npc_ori_plot\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/setup-1.png){width=672}\n:::\n:::\n\n\nIf you are inclined to do so, you can check that `fviz_screeplot(pc)` gives you a horrible scree plot with very little variance explained and use `fviz_pca_contrib(pc, choice = 'var')` to check that the contributions of the different variables are also close to random.\n\n## Skewed Data Distributions\n\nThe vanilla PCA does nothing to help us visualize a separation between the. Why is that the case?\n\nUpon a closer inspection of the regular variables, which I should have done before diving into the PCA, we see that we are dealing with heavily skewed distributions\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspam %>% \n  pivot_longer(-yesno) %>% \n  ggplot(aes(yesno, value, fill = yesno)) +\n  geom_violin() +\n  facet_wrap(~name, scales = \"free\", nrow=3) +\n  scale_y_continuous(labels = label_number(scale_cut = cut_short_scale()))+\n  scale_fill_paletteer_d(\"awtools::a_palette\") +\n  ggthemes::theme_base() +\n  theme(legend.position = \"bottom\",\n        plot.background =  element_rect(color = NA),\n        legend.background = element_rect(fill = \"gray90\"),\n        legend.key = element_rect(fill = \"gray90\"),\n        panel.background = element_rect(fill=\"#81AE5C\")) +\n  labs(fill = element_blank(), x = element_blank(), y = element_blank())\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/skewed-1.png){width=672}\n:::\n:::\n\n\nThe distributions are so skewed we can barely see them.\n\n## Transform\n\nEnter the [Yeo--Johnson transformation](https://en.wikipedia.org/wiki/Power_transform#Yeo%E2%80%93Johnson_transformation), a type of Power Transform[^1] that will come handy to normalize the data.\n\n[^1]: Yes, the title of this post is 100% pun intended.\n\n[As a side note, I had a bit of trouble running this using the more conventional `caret` or `recipes` packages, you can read my StackOverflow question [here](https://stackoverflow.com/questions/76925607/understanding-yeojohnson-transformation-in-r/76933102#76933102) and the nice answer about estimating parameters properly. For this post, I will be using `bestNormalize::yeojohnson` to normalize all columns in the dataset.]{.aside}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# quickly apply transformation to the data itself\ndf_transformed <- select(spam, where(is.numeric)) %>% \n  mutate_all(.funs = function(x) predict(bestNormalize::yeojohnson(x), newdata = x))\n# check the new distributions\ndf_transformed$yesno <- spam$yesno\ndf_transformed %>% \n  pivot_longer(-yesno) %>% \n  ggplot(aes(yesno, value, fill = yesno)) +\n  geom_violin() +\n  facet_wrap(~name, scales = \"free\", nrow=3) +\n  scale_y_continuous(labels = label_number(scale_cut = cut_short_scale()))+\n  scale_fill_paletteer_d(\"awtools::a_palette\") +\n  ggthemes::theme_base() +\n  theme(legend.position = \"bottom\",\n        plot.background =  element_rect(color = NA),\n        legend.background = element_rect(fill = \"gray90\"),\n        legend.key = element_rect(fill = \"gray90\"),\n        panel.background = element_rect(fill=\"#81AE5C\")) +\n  labs(fill = element_blank(), x = element_blank(), y = element_blank())\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/violin-transformed-1.png){width=672}\n:::\n:::\n\n\nI am not a huge fan of data transformations, but that is a very nice transformation. We often deal with skewed data, which produces difficulties when visualizing and performing data analysis. Having a tool like this power transform comes really handy[^2].\n\n[^2]: The devil is on the details. Always check the parameters and be careful on data interpretation when transforming your data!\n\n## Second PCA\n\nWe can now check how the second PCA looks like. It's not a panacea, but we have made large improvements. Check the side by side comparisons:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npc <- prcomp(df_transformed[, 1:6])\npc_data <- pc$x[, 1:2] %>% as_tibble()\npc_data$yesno <- spam$yesno\n\npc_second_plot <- ggplot(pc_data, \n       aes(PC1, PC2, color = yesno)) +\n  geom_point() +\n  coord_equal() +\n  scale_color_paletteer_d(\"awtools::a_palette\") +\n  ggthemes::theme_base()+\n  theme(legend.position = \"bottom\",\n        plot.background =  element_rect(color = NA),\n        legend.background = element_rect(fill = \"gray90\"),\n        legend.key = element_rect(fill = \"gray90\"),\n        panel.background = element_rect(fill=\"#81AE5C\")) +\n  labs(color = element_blank())\nlibrary(patchwork)\npc_ori_plot + pc_second_plot\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/second-pca-1.png){width=672}\n:::\n:::\n\n\nIn terms of separating data, the second PCA is not the best PCA in the world. We can still see that there is a bunch of points all clustered together:\n\n\n::: {.cell}\n\n```{.r .cell-code}\np1 <- ggplot(pc_data, \n       aes(PC1, PC2, color = yesno)) +\n  geom_point(color = 'gray50', alpha = 0.5)  + \n  labs(title = \"All Data\") + \n  coord_equal()+\n  ggthemes::theme_few(base_family = \"Ubuntu\")\nspam_color <- paletteer::paletteer_d(\"awtools::a_palette\")[2]\nemail_color <- paletteer::paletteer_d(\"awtools::a_palette\")[1]\np2 <- ggplot(pc_data, \n       aes(PC1, PC2, color = yesno)) +\n  geom_point(color = 'gray50', alpha = 0.5)  + \n  geom_point(data=filter(pc_data, yesno==\"spam\"),\n             color = spam_color, alpha = 0.5)  + \n  labs(title = \"Spam\") + \n  coord_equal()+\n  ggthemes::theme_few(base_family = \"Ubuntu\")\np3 <- ggplot(pc_data, \n       aes(PC1, PC2, color = yesno)) +\n  geom_point(color = 'gray50', alpha = 0.5)  + \n  geom_point(data=filter(pc_data, yesno==\"email\"),\n             color = email_color, alpha = 0.5)  + \n  labs(title = \"Emails\") + \n  coord_equal() +\n  ggthemes::theme_few(base_family = \"Ubuntu\")\np1 + p2 + p3\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nHowever, I encourage you to check `fviz_screeplot(pc)` to see how dramatically better this second PCA is.\n\n## Ending remarks\n\nRegardless of the final separation that we could achieve in this particular dataset, the normalization performed using Yeo--Johnson transform was quite powerful. You have been given the Power to Normalize, I hope you try it on your own skewed datasets!\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}