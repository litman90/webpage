[
  {
    "objectID": "posts/2018-08-30-writing-challenge-day-1/index.html",
    "href": "posts/2018-08-30-writing-challenge-day-1/index.html",
    "title": "Writing challenge Day 1",
    "section": "",
    "text": "It’s been a while since I have several things to write in the back of my mind. I finally got the strength to commit myself to one of those goal oriented challenges: Writing 2000 words a day.\nIt might sound like a bit much, now that I think about it, but today is Day 1 so I might as well get to do it. I have not limited the type of content I will write, my aim is to get as fluent as possible and to have writing become habitual. That means I will increase the number of posts/short stories/book chapters/papers/review papers/twitter posts.\nDoes coding count? Is it a line by line instead of word count? I have not started writing and I am already thinking how to cheat about it…I am not quite confident about fortune cookie writer, but, since I foresee writing to be paramount for my future, I’m 100% down to master it.\nI’m a bit obsessed about measuring stuff. Thus, I will also be implementing a suggested technique to keep a record of the data that such writing endeavor produces1. It goes like this:\nMake a table with\nIt sounds fairly easy to keep up with recording (I record so many other stuff) and there are some pieces of information that sound like really interesting to have. I have always considered myself better at night. Is it really the case? Do I write better at night or in the morning before work? Do I write better at home or somewhere else? Should I go to the international space station? Maybe that would give enough inspiration to put those works together into text like it’s no big deal.\nRecording means a lot of plots (oh yes it does) and I have played a bit with calendar/github like heatmap plots before (see here), so I will be happy to show the trends in future posts. I will also be commenting about my impressions with the technique and if I actually manage to make it a habit of mine.\nSo…having written 390 words and finished my first slotted time, I should go back to my duties. Every long journey starts with a small step, writing will continue in the next scheduled period."
  },
  {
    "objectID": "posts/2018-08-30-writing-challenge-day-1/index.html#footnotes",
    "href": "posts/2018-08-30-writing-challenge-day-1/index.html#footnotes",
    "title": "Writing challenge Day 1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSuggested here↩︎"
  },
  {
    "objectID": "posts/2018-05-27-github-style-calendar-heatmap/index.html",
    "href": "posts/2018-05-27-github-style-calendar-heatmap/index.html",
    "title": "Github style calendar heatmaps",
    "section": "",
    "text": "I like how the commit heatmap looks in Github. I wanted to play with something that could be plotted that way. I’ve seen some beautiful things done in d3 and javascript. But, of course, I wanted to make it in R. Turns out a bunch of other people have great ideas for how to go about it. Thus, I’m borrowing heavily from them1."
  },
  {
    "objectID": "posts/2018-05-27-github-style-calendar-heatmap/index.html#the-commit-heatmap",
    "href": "posts/2018-05-27-github-style-calendar-heatmap/index.html#the-commit-heatmap",
    "title": "Github style calendar heatmaps",
    "section": "",
    "text": "I like how the commit heatmap looks in Github. I wanted to play with something that could be plotted that way. I’ve seen some beautiful things done in d3 and javascript. But, of course, I wanted to make it in R. Turns out a bunch of other people have great ideas for how to go about it. Thus, I’m borrowing heavily from them1."
  },
  {
    "objectID": "posts/2018-05-27-github-style-calendar-heatmap/index.html#loading-packages",
    "href": "posts/2018-05-27-github-style-calendar-heatmap/index.html#loading-packages",
    "title": "Github style calendar heatmaps",
    "section": "Loading packages",
    "text": "Loading packages\nWe will need a few packages to generate this plot.\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(viridis)  # Color palette\nlibrary(ggthemes) # theme tufte"
  },
  {
    "objectID": "posts/2018-05-27-github-style-calendar-heatmap/index.html#the-data",
    "href": "posts/2018-05-27-github-style-calendar-heatmap/index.html#the-data",
    "title": "Github style calendar heatmaps",
    "section": "The data",
    "text": "The data\nLet’s generate a data.frame for May 2018. We want the date as datetime and we also want to extract values from that date (month, year, week, …).\n\n\nCode\n# choose dates\nstart_date &lt;- ymd(\"2018-05-01\")\nend_date &lt;- ymd(\"2018-05-31\")\n\nd &lt;- tibble::tibble(\n    date = seq(start_date, end_date, by = \"days\"),\n    month = month(date),\n    year = format(date, \"%Y\"),\n    week = as.integer(format(date, \"%W\")) + 1,  # Week starts at 1\n    day = factor(weekdays(date, T), \n                 levels = rev(c(\"Mon\", \"Tue\", \"Wed\", \"Thu\",\n                                \"Fri\", \"Sat\", \"Sun\"))))\n\n\nThis is how the data we generated looks like:\n\n\nCode\nhead(d)\n\n\n# A tibble: 6 × 5\n  date       month year   week day  \n  &lt;date&gt;     &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt;\n1 2018-05-01     5 2018     19 Tue  \n2 2018-05-02     5 2018     19 Wed  \n3 2018-05-03     5 2018     19 Thu  \n4 2018-05-04     5 2018     19 Fri  \n5 2018-05-05     5 2018     19 Sat  \n6 2018-05-06     5 2018     19 Sun  \n\n\nNow, let’s assume I registered some events in my life and that my data looks something like: A date column date, and the number of events that happened on a particular date (n).\nAgain, here’s how the data looks like.\n\n\nCode\ndf\n\n\n# A tibble: 13 × 2\n   date           n\n   &lt;date&gt;     &lt;int&gt;\n 1 2018-05-15     1\n 2 2018-05-16     1\n 3 2018-05-17     1\n 4 2018-05-18     2\n 5 2018-05-19     4\n 6 2018-05-20     2\n 7 2018-05-21     2\n 8 2018-05-22     2\n 9 2018-05-23     2\n10 2018-05-24     5\n11 2018-05-25     2\n12 2018-05-26     1\n13 2018-05-27     4\n\n\nI can join both data.frames and visualize!\n\n\nCode\ndf_plot &lt;- d %&gt;% left_join(df, by = \"date\") \n\ndf_plot %&gt;%\n  mutate(n=ifelse(is.na(n), 0, n)) %&gt;% ## Fill the NAs with zeros\n  ggplot(aes(date, n)) +\n  geom_line(lwd=0.7)+\n  geom_point(size=2, shape=21, fill=\"black\", colour=\"white\", stroke=2)+\n    theme_classic() +\n  theme(panel.background = element_rect(colour = \"black\"))+\n  ylab(\"Number of events\")\n\n\n\n\n\nMy goal is not to analyze long term trends like seasonality. Thus, this plot is rather unremarkable. Not only because it is a small toy-like dataset, but because it fails to inform calendar information. Let’s try to make it better!"
  },
  {
    "objectID": "posts/2018-05-27-github-style-calendar-heatmap/index.html#abstracting-into-functions",
    "href": "posts/2018-05-27-github-style-calendar-heatmap/index.html#abstracting-into-functions",
    "title": "Github style calendar heatmaps",
    "section": "Abstracting into functions",
    "text": "Abstracting into functions\nA good way of improving the procedure is to abstract things into a function we can call calendar_plot().\n\n\nCode\ncalendar_plot &lt;- function(data, color.scale = \"viridis\",\n                          viridis.pal = \"D\", dir = 1){\n  \n  p &lt;- ggplot(data, aes(x = week, y = day, fill = n)) +\n    geom_tile(color = \"white\", size = 0.8) +\n    facet_wrap(\"year\", ncol = 1) +\n    theme_tufte() +\n    theme(axis.ticks = element_blank(),\n          legend.position = \"bottom\",\n          legend.key.width = unit(1, \"cm\"),\n          strip.text = element_text(hjust = 0.01,\n                                    face = \"bold\", size = 12),\n          text = element_text(size=16)) + \n    ylab(\"\")\n  \n  \n  \n  # Let's add more than one possible pallete. Default keeps being viridis\n  # Add case switch? or add 'none' for user to define their own ?\n  \n  if(color.scale==\"viridis\"){\n    \n    \n    p &lt;- p + scale_fill_viridis(name=\"Number of Events\", \n                                # Variable color palette\n                                option = viridis.pal,  \n                                # Variable color direction\n                                direction = dir,  \n                                na.value = \"grey93\",\n                                limits = c(1, max(data$n)))\n    \n  } else if(color.scale == 'greens') {\n    \n    p &lt;- p + \n      scale_fill_gradient(name=\"Number of Events\",\n                          low=\"lightyellow2\", \n                          high=\"darkgreen\", \n                          na.value = \"grey93\")\n  } else{\n    \n    error(\"Accepted color.scale are 'viridis' and 'greens'\")\n    \n  }\n  \n  \n  ## x axis control of labels\n  \n  \n  num_months &lt;- length(unique(data$month))\n  \n  if(num_months &gt; 1){\n    \n    p &lt;- p + scale_x_continuous(\n      expand = c(0, 0),\n      breaks = seq(1, 52, length = 12),\n      labels = c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\",\n                 \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"))\n    \n  } else {\n    \n    # do nothing\n    \n    p &lt;- p + xlab(\"Week Number.\")\n    \n  }\n  \n  \n  \n  print(p)\n}\n\n\nWe can use calendar_plot() function now to make a plot in calendar-like shape. It is easier to see the data, even with such as small dataset. Below there are two color scale representations of the same data.\n\n\nCode\ncalendar_plot(df_plot, 'greens')\n\n\n\n\n\nCode\ncalendar_plot(df_plot, viridis.pal = \"B\")"
  },
  {
    "objectID": "posts/2018-05-27-github-style-calendar-heatmap/index.html#update",
    "href": "posts/2018-05-27-github-style-calendar-heatmap/index.html#update",
    "title": "Github style calendar heatmaps",
    "section": "Update",
    "text": "Update\nI was curious about how data would look like for a longer span. Here’s the data for a longer time interval.\n\n\nCode\ncalendar_plot(df_plot)\n\n\n\n\n\nBy no means this is a perfect function and is far from tested. For example, when I did this update, I realized that my calendar_plot() function should handle internally the creation of the data.frame named d that serves as a placeholder. I guess that will happen in a following update :)."
  },
  {
    "objectID": "posts/2018-05-27-github-style-calendar-heatmap/index.html#footnotes",
    "href": "posts/2018-05-27-github-style-calendar-heatmap/index.html#footnotes",
    "title": "Github style calendar heatmaps",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGreat resources here, and here↩︎"
  },
  {
    "objectID": "posts/2018-07-13-XY-density-maps/index.html",
    "href": "posts/2018-07-13-XY-density-maps/index.html",
    "title": "XY-density-maps",
    "section": "",
    "text": "I have been playing around with tracking software and the ways to visualize the position of an animal in an arena over time. Even with normal cameras (30 Hz) and relatively small periods of time (~5 min) we get massive datasets and the only way to make sense of them is to aggregate data over time. I have been interested in neat visualizations (using R), thus here are some ways/comments. I will explore different packages and doing comparisons of the results."
  },
  {
    "objectID": "posts/2018-07-13-XY-density-maps/index.html#packages",
    "href": "posts/2018-07-13-XY-density-maps/index.html#packages",
    "title": "XY-density-maps",
    "section": "Packages",
    "text": "Packages\n\nsuppressPackageStartupMessages(library(RColorBrewer))\nsuppressPackageStartupMessages(library(KernSmooth))\nsuppressPackageStartupMessages(library(raster))\nsuppressPackageStartupMessages(library(ggplot2))\nsuppressPackageStartupMessages(library(dplyr))"
  },
  {
    "objectID": "posts/2018-07-13-XY-density-maps/index.html#data",
    "href": "posts/2018-07-13-XY-density-maps/index.html#data",
    "title": "XY-density-maps",
    "section": "Data",
    "text": "Data\nI have an example dataset with XY position of the animal. I also have other morphological data that is irrelevant for this case. See example below:\n\n\n         X        Y Orientation MinorAxis MajorAxis\n1 325.5522 158.9700   -1.550285  50.47925  130.5679\n2 323.3055 156.6896   -1.569706  51.29840  130.3467\n3 321.7411 154.8107    1.545683  52.13492  130.3881\n4 320.8373 153.3294    1.512549  52.67011  130.6403\n5 320.0381 152.5523    1.477299  53.07783  130.1023\n6 319.3961 152.2019    1.439171  53.75891  129.6706\n\n\nI will do some setup that is common for all the graphs (aka, color palette and the dimensions of the arena).\n\n# Get color palette \n\nrefCol &lt;- colorRampPalette(rev(brewer.pal(6,'Spectral')))\nmycol &lt;- refCol(6)\n\n# define bin sizes\nbin_size &lt;- 40\n\n# Camera resolution is 640x480. Hence...\nxbins &lt;- 640/bin_size \nybins &lt;- 480/bin_size"
  },
  {
    "objectID": "posts/2018-07-13-XY-density-maps/index.html#ggplot2",
    "href": "posts/2018-07-13-XY-density-maps/index.html#ggplot2",
    "title": "XY-density-maps",
    "section": "ggplot2",
    "text": "ggplot2\nThe reason I always go to ggplot2 first is because it’s awesome, I buy into the grammar and find it intuitive to accumulate layers over layers. The underlying thought is that ggplot2 handles all problems. In this case, the result has some pros (layers, layers and more layer), and some cons (basically, it doesn’t look amazing).\n\np &lt;- ggplot(rat_pos, aes(X,Y)) + \n  stat_density2d(geom = 'tile', \n                 aes(fill = after_stat(density)), \n                 contour = FALSE,\n                 n = c(xbins, ybins)) +\n  #geom_point() + \n  #geom_path(alpha=0.1) +\n  coord_equal() +\n  theme_minimal() +\n  scale_fill_gradientn(colors =  mycol) +\n  geom_vline(xintercept = c(0,640))+\n  geom_hline(yintercept = c(0,480))\n\nprint(p)\n\n\n\n\nThis is interesting because we can overlay things into the plot. For example the trace:\n\np + geom_path(alpha=0.1)\n\n\n\n\nWe can further remove the axis (or any other modifications we feel like doing).\n\np + geom_path(alpha=0.5) + theme_void()\n\n\n\n\n\nHelping ggplot from outside\nI found that, if we calculate the density externally, it looks smoother. This is a mixed, bkde2D mediated, ggplot2 approach (aka the best of 2 worlds).\n\nbins &lt;- bkde2D(as.matrix(rat_pos[,1:2]), \n               bandwidth = c(xbins, ybins),\n               gridsize = c(640L, 480L))\n\n# reshape\nbins_to_plot &lt;- tidyr::pivot_longer(\n  data.frame(bins$fhat) %&gt;% tibble::rownames_to_column(), \n  cols = -rowname) %&gt;% \n  # fix the name column\n  mutate(name = stringr::str_remove(name, \"X\")) %&gt;% \n  # convert to numeric\n  mutate_all(as.numeric)\n\n\nggplot(bins_to_plot, aes(rowname, name, fill = value)) +\n  geom_raster()+\n  coord_equal() +\n  theme_minimal() +\n  scale_fill_gradientn(colors =  mycol) +\n  geom_vline(xintercept = c(0,640))+\n  geom_hline(yintercept = c(0,480))+\n  theme_void()\n\n\n\n\n\n\n2023 Update\nAs of 2023, ggplot is able to handle 2D density heatmaps much better. I haven’t played too much with it, but here are some examples\n\nggplot(rat_pos, aes(X,Y)) + \n           stat_density_2d(geom = \"polygon\", \n                           contour = TRUE,\n                           aes(fill = after_stat(level)),\n                  bins = 8)+\n  coord_equal() +\n  scale_fill_gradientn(colors =  mycol) +\n  theme_minimal() +\n  geom_vline(xintercept = c(0,640))+\n  geom_hline(yintercept = c(0,480))+\n  theme_void()\n\n\n\n\n\nggplot(rat_pos, aes(X,Y)) + \n  geom_density_2d_filled(bins = 8)+\n  coord_equal() +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Spectral\", direction = -1)+\n  geom_vline(xintercept = c(0,640))+\n  geom_hline(yintercept = c(0,480))+\n  theme_void()"
  },
  {
    "objectID": "posts/2018-07-13-XY-density-maps/index.html#using-raster-package",
    "href": "posts/2018-07-13-XY-density-maps/index.html#using-raster-package",
    "title": "XY-density-maps",
    "section": "Using raster package",
    "text": "Using raster package\nThe raster package is maybe an older solution, which is surprisingly low demand. In 3 lines of code we get a perfectly functional plot. On the other hand, it’s not the best looking graph and we get the caveats (yes, base graphics).\n\nr &lt;- raster(xmn=0, ymn=0, xmx=640, ymx=480, res=20)\nx &lt;- rasterize(rat_pos[,1:2], r, fun='count')\nplot(x, xlim=c(0,640), ylim=c(0,480), xaxt='n', ann=FALSE, yaxt='n')"
  },
  {
    "objectID": "posts/2018-07-13-XY-density-maps/index.html#documentation",
    "href": "posts/2018-07-13-XY-density-maps/index.html#documentation",
    "title": "XY-density-maps",
    "section": "Documentation",
    "text": "Documentation\nI have found a good amount of advice and inspiration in the links below.\n\nhttp://stat405.had.co.nz/ggmap.pdf\nhttps://stackoverflow.com/questions/24078774/overlay-two-ggplot2-stat-density2d-plots-with-alpha-channels?lq=1\nhttps://www.r-bloggers.com/5-ways-to-do-2d-histograms-in-r/\nhttps://stackoverflow.com/questions/24845652/specifying-the-scale-for-the-density-in-ggplot2s-stat-density2d"
  },
  {
    "objectID": "posts/2018-10-02-skype-a-scientist-1/index.html",
    "href": "posts/2018-10-02-skype-a-scientist-1/index.html",
    "title": "Skype a Scientist part 1",
    "section": "",
    "text": "I have been lucky to participate in the project lead by Sarah McAnulty called Skype a scientist. So far, I have done calls with two grades:\nI should say it was great to be able to talk with students, tell them about science in general and my research. It was really fun to answer their questions. I felt they were surprised about talking with a normal human being that also happens to do science. I feel really grateful to Sonia and Cindy, they are wonderful teachers, making efforts to connect their students with science and those who make it possible. We also did a Q&A session, which I compiled below."
  },
  {
    "objectID": "posts/2018-10-02-skype-a-scientist-1/index.html#rd-grade",
    "href": "posts/2018-10-02-skype-a-scientist-1/index.html#rd-grade",
    "title": "Skype a Scientist part 1",
    "section": "3rd Grade",
    "text": "3rd Grade\nHow old are you?\nI am 26 years old.\nDo you dissect rats?\nYes, I dissect rats.\nWhat’s your favorite animal?\nMy favorite animal is the Lion.\nWhat’s your favorite color?\nMy favorite color is green!\nWhy do you like to learn about rats?\nRats are fantastic animals. They are super smart and very similar to humans.\nHow and why did you become a scientist?\nI have always been into questions, I am super curious! Scientists work to ask interesting questions and that’s the best job ever for curious people. Scientists also like to fix stuff. If we find ways to do it, we can help people in need.\nIs being a scientist fun?\nIt’s super fun! You get to see crazy stuff nobody else has ever seen. You are always learning and discovering new things.\nDo you like being a scientist?\nYes, I love it!\nDo you want to be a scientist forever?\nI am 26 years old…I think I have plenty time, but forever is too long\nIs your mom or dad a scientist?\nNo, my mom is an accountant and my dad is a salesman. I am the only scientist in my whole family!\nDo you have any kids?\nNot (yet)!\nDo you have friends?\nYes, I do!\nDo you have any pets?\nNope. But I had a dog before coming to the US and I’m looking into having a cat!\nWhat do you do on your breaks?\nI like to go hiking, I have seen BEAUTIFUL places in nature!\nWhen is your birthday?\nAugust the 12th (Candy is welcome)\nWhat do you like to eat?\nMeat, and Pasta, and Ice-Cream, and Candy, and Chocolate, and Pizza, I eat a lot !\nDo you play sports? Do you like sports?\nYes!\nDo you like anything else besides science?\nYes! I play soccer and I run, I cook, I dance, and I am a drummer!\nDo you invent things like robots, mouse traps…?\nI invent things that look like a robot to trap rats. Does it count?\nDo you have a Youtube channel?\nI had a YouTube channel (in Spanish). I teach different things (Math, Science, …) there! My channel is Profesores Gama.\nDid you like science since you were a child?\nActually, I liked Math the most. But math is most beautiful in Nature, it’s everywhere!\nIs rats the only animal you like to observe and study?\nNope, I like most animals! I worked with crabs before working with rats. Hopefully I will be able to work with mice, flies, and fish.\nHow many rats do you have in your lab?\nIt depends on the number of babies! We can have hundreds!\nWhat do rats like to eat?\nThey eat a lot of things, they go crazy about M&Ms and Cheerios.\nWhat kind of rats do you observe?\nI observe a particular type of albino rat called Sprague Dawley.\nDo you name your rats?\nYes! My favorite one is called SD7A4B1E3F3G1M2 but I prefer Amelia\nDo you observe boy rats too?\nSometimes. I mostly observe mother rats.\nWhat do you do with the baby rats?\nI put the baby rats with mothers and see how they take care of them.\nIs your lab big or small?\nWe have several rooms in two floors, fairly big.\nHow do you observe your rats?\nSometimes just looking, sometimes filming, sometimes with the microscope.\nWhere do you find your rats?\nWe breed them.\nDo your rats wear clothes?\nNope, any ideas in mind?\nWhere do you keep the rats so they will not escape?\nThey leave in comfortable cages with rat friends.\nWhat clothes do you use inside your lab? Uniform?\nLab coat, gloves, shoe protection, and eye protection. Full package."
  },
  {
    "objectID": "posts/2018-10-02-skype-a-scientist-1/index.html#th-grade",
    "href": "posts/2018-10-02-skype-a-scientist-1/index.html#th-grade",
    "title": "Skype a Scientist part 1",
    "section": "7th Grade",
    "text": "7th Grade\nWhy did you want to become a scientist?\nI don’t know…I just felt really curious about how the natural world works. I could see beauty in Math and later in Chemistry, and wanted to learn more. Living things are by far the most complex systems we know of, so I was totally down for the challenge of figuring out how they work.\nWhen did you become a scientist?\nThis question is really challenging. The correct answer is I don’t know. I am not sure there was a date. I could choose a random date when I was around 10 or I could say I’m working to be a scientist. As it happens with everything, there are some days when I feel more scientist than others.\nHow many years did you have to go to school before you became a scientist?\nI can tell you how many years I’ve been in school. Elementary school was 7, middle/high-school was 5, then university added another 5-6 years, 2 years for my Master’s of Science degree. School is just the beginning.\nIs science different in Argentina than in the US?\nScience in Argentina is different. To start, we speak in Spanish and drink mate instead of coffee or tea during breaks. One thing that the US has is people from many different places, I’ve met people from all the continents, all of them working here, together, to make the best science they can. In Argentina, things are a bit complicated because of the rough economic situation in the country.\nWhat fascinates you about neuroscience?\nEvolution has given animals a biological machine that is far more advanced and efficient than anything we’ve ever built or known. We have the most complex machine between our ears and, curiously, we can only understand the world using that machine. We only perceive what our brain interprets, we can’t be sure about things being out there. For all we know, there could be nothing out there, everything could be an illusion recreated by our minds, playing inside our heads. How do you know the color I call red is the color you call red? The first time I heard that question I was 12 years old, and it continues to blow my mind.\nWhat is the thing that surprised you the most about studying the rodents?\nRodents are extremely robust. Also, I can’t help but to have the feeling that their life is so much faster than ours. Their metabolism is higher, their lifespan is shorter. Five minutes is a lot of time for a mouse, much more for the tiny neurons in mice brains. Still, five minutes is not enough for long term events that influence 99.999% of the biology of animals.\nAre rodent brains different sizes?\nYes, different species have different brain sizes. This is true for almost all living things. Here’s a cool plot about brain size1!\n\nIt is also impressive that brain sizes grow predictably with body mass23.\n\nThe brain size question is tricky because we try to jump into conclusions about intelligence or cognitive abilities (also see below). The truth is, most of what a bigger brain is there for has nothing to do with what we humans call intelligence. For example, if you are a bigger animal, you need to receive signals from more places and you have larger muscles to control (aka, there’s a lot of room to tickle an elephant and moving an elephant needs a lot of neurons). Hence, bigger animals will tend to have a bigger brain.\nAre brains smarter if they are bigger?\nThis is a frequently asked question. The key thing to consider, as previously stated, is the size of the animal that carries that brain. A mosquito can’t carry a whale’s brain, simply because it’s own size is smaller than the whale’s brain. However, mosquitoes have a nervous system that allows them to solve complex tasks and have successful reproductive endeavors.\nBrains are there to sense, process the information we receive from the outside world and make a decision (aka move towards/away). We also have to consider what smart means. But that’s a discussion for another day. Briefly, we tend to think about being smart as having higher cognitive abilities. For example, a higher cognitive abilities might usually depend on integration of several sensory stimuli and abstraction, like appreciating the regular patterns of a particular instrument in a symphony. Those activities tend to correlate well with the size of a region of the brain called the cortex, which is pretty big in primates.\nCan you show us a rat? Can you show us its brain?\nNo, but I can show you pictures of brains.\nThese are sections of a mouse brain.\n\n\nBegin again pic.twitter.com/kfflg6R8XE\n\n— Matias Andina (@NeuroMLA) September 13, 2018\n\n\nThis is a picture of a special staining showing different kinds of neurons in a rat brain.\n\n\nPaint…? Nah, Antibodies :) #sciart pic.twitter.com/LlxBOPlzs5\n\n— Matias Andina (@NeuroMLA) August 24, 2018\n\n\nCan rats swim underwater? Where do they live?\nYes they can, check this cool picture of rats swimming. We actually use the fact that rats are good swimmers to challenge them in a maze. It’s called the water maze (also known as Morris maze).\nRats are highly adapted to many different ecosystems. They can also do pretty well near human populations, scavenging city waste for a living. The map below shows an estimate of where rats live (aka everywhere):\n\nWhat is the difference between a rodent brain and a human brain?\nThey are pretty similar, both contain massive amount of cells called neurons and glia. A key distinction is the size of the olfactory bulbs of a rodent and a human. While rodents have huge amount of brain devoted to the task of smelling the outside world, human bulbs are tiny. We are visual animals and hence, much of our brain is devoted to seeing things.\nHow is it possible to learn from a rodent’s brain how our brain works?\nHumans and rodents are pretty related. We have a huge amount of DNA in common. We both have spinal cords and a similar brain organization. Many of the things that keep us alive have been functioning since many many years ago and are shared in the evolution of both animals.\nStudying the brain of a rodent can tell us many ways in which human brains function, and can help us build models of human diseases to try potential treatment before risking human lives. If it works in mice, there’s a good chance that it will work in humans. At least it’s worth a shot. Moreover, if it doesn’t work in mice, especially if mice show detrimental effects, there are good reasons not to try it in humans.\nWhat equipment do you use in your lab to study rodents?\nLots. Most commons are test tubes, pipettes, and syringes. The microscopes are the fancy equipment we use to take great pictures.\nCan rodents learn math?\nThis is a pretty challenging question. I don’t want to get into the What is math question. People tried to teach horses how to do math, which was a commercial success, but turned out that horse was not actually doing it for real, it was responding to the signals of its owner (see Clever Hans here). Crows can solve puzzles that need a pretty deep understanding of the physical world, like fluid displacement, compare object lengths/weights. In my opinion, that comes super close to our math. Youtube is full of awesome crow videos!\n\n\n\nCrow solving a puzzle that involves fluid displacement\n\n\nWe know mathematical abilities are not fully related to language, cultures that don’t have the words for specific numbers can distinguish different amounts and do simple math without any formal structure. Many animals can definitely learn simple tasks that are related to number of events or a certain amount of time. They can distinguish amounts (e.g., 10 M&Ms is more food than 2 M&Ms), and they can keep a count. However, we often face difficulties to be certain of what is going on in the rat’s head. We can’t ask, we can’t be rats. Thus, the best question would be whether we can test unequivocally that rats are performing mathematical operations, counting, or have any representation of numbers.\nFor those of you really tempted to dig further into this question:\n\nhttps://en.wikipedia.org/wiki/Number_sense_in_animals\nhttps://www.ncbi.nlm.nih.gov/pubmed/26463617"
  },
  {
    "objectID": "posts/2018-10-02-skype-a-scientist-1/index.html#footnotes",
    "href": "posts/2018-10-02-skype-a-scientist-1/index.html#footnotes",
    "title": "Skype a Scientist part 1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe source of the brain pictures can be found here↩︎\nThis is called brain::body-mass ratio and it is a famous law of scaling. More info here https://en.wikipedia.org/wiki/Brain-to-body_mass_ratio.↩︎\nThe source of the graph can be found here. Many other places will have similiar graphs. Also, I recommend a book called Scale by Geoffrey West.↩︎"
  },
  {
    "objectID": "posts/2019-04-06-email-analysis/index.html",
    "href": "posts/2019-04-06-email-analysis/index.html",
    "title": "Email analysis",
    "section": "",
    "text": "I am keen on tracking things. Some people call this personal analytics, I call it fun. In this post, I will explore how to extract your email information using python and R. My goal is to do some analysis on the data and, hopefully, explore different visualizations that can inform future behavior."
  },
  {
    "objectID": "posts/2019-04-06-email-analysis/index.html#emails-from-gmail",
    "href": "posts/2019-04-06-email-analysis/index.html#emails-from-gmail",
    "title": "Email analysis",
    "section": "Emails from Gmail",
    "text": "Emails from Gmail\nReading emails in python is quite simple, we need to import the mailbox library. My file is called correo and comes from downloading my Gmail stuff out of https://takeout.google.com/settings/takeout. It can take a while so be patient.\nThis should return something that looks like:\nX-GM-THRID\nX-Gmail-Labels\nReceived\nMIME-Version\nDate\nTo\nFrom\nSubject\nContent-Type\nContent-Disposition\nMessage-Id\nLet’s save the info we care about into a new file. We select subject, from, date, to, and some variables that allow us to keep track of things (i.e, labels and threads)."
  },
  {
    "objectID": "posts/2019-04-06-email-analysis/index.html#emails-from-thunderbird",
    "href": "posts/2019-04-06-email-analysis/index.html#emails-from-thunderbird",
    "title": "Email analysis",
    "section": "Emails from Thunderbird",
    "text": "Emails from Thunderbird\nI keep 3 accounts in Thunderbird. Using the ImportExportTools Add-on, I exported things into .mbox format. Following a similar procedure to the one depicted above, I got the other three accounts exported to .csv files. Just be sure you select the correct keys (see example below, this might change for other email clients)."
  },
  {
    "objectID": "posts/2019-04-06-email-analysis/index.html#data-cleaning",
    "href": "posts/2019-04-06-email-analysis/index.html#data-cleaning",
    "title": "Email analysis",
    "section": "Data cleaning",
    "text": "Data cleaning\nLet’s switch from python to R1.\nUnfortunately, emails come tagged (things like \"\\\\?=\" and other nasty stuff) and you might have to deal with different encodings (the perks of speaking multiple languages). As an example, let’s see what Quora sends me.\n\n\n                                                             from\n1                         Quora Digest &lt;digest-noreply@quora.com&gt;\n2                         Quora Digest &lt;digest-noreply@quora.com&gt;\n3                         Quora Digest &lt;digest-noreply@quora.com&gt;\n4  =?utf-8?q?Selecci=C3=B3n_de_Quora?= &lt;digest-noreply@quora.com&gt;\n5                         Quora Digest &lt;digest-noreply@quora.com&gt;\n6                         Quora Digest &lt;digest-noreply@quora.com&gt;\n7                         Quora Digest &lt;digest-noreply@quora.com&gt;\n8  =?utf-8?q?Selecci=C3=B3n_de_Quora?= &lt;digest-noreply@quora.com&gt;\n9  =?utf-8?q?Selecci=C3=B3n_de_Quora?= &lt;digest-noreply@quora.com&gt;\n10 =?utf-8?q?Selecci=C3=B3n_de_Quora?= &lt;digest-noreply@quora.com&gt;\n\n\nThat’s nasty…Let’s do some cleaning. This function comes really handy for text replacement.\nWe are going to modify the function a bit, we add x as the string we pass for cleaning and we will remove the tags progressively.\nWe are ready to use our super cool function and clean the text! Not perfect, but gets us 90% of the way.\nLet’s see how emails from Quora changed with this new encoding:\n\n\n                                            from\n1        Quora Digest &lt;digest-noreply@quora.com&gt;\n2        Quora Digest &lt;digest-noreply@quora.com&gt;\n3        Quora Digest &lt;digest-noreply@quora.com&gt;\n4  Selección_de_Quora &lt;digest-noreply@quora.com&gt;\n5        Quora Digest &lt;digest-noreply@quora.com&gt;\n6        Quora Digest &lt;digest-noreply@quora.com&gt;\n7        Quora Digest &lt;digest-noreply@quora.com&gt;\n8  Selección_de_Quora &lt;digest-noreply@quora.com&gt;\n9  Selección_de_Quora &lt;digest-noreply@quora.com&gt;\n10 Selección_de_Quora &lt;digest-noreply@quora.com&gt;\n\n\nLet’s filter those from “Received” or “Sent” (in Spanish, “Recibidos” or “Enviado”).\nTo save you from reading a considerable amount of code, I will load the other accounts and modify them accordingly in the background. I will finally merge everything together. Just enjoy the kitten while the code is running in my machine."
  },
  {
    "objectID": "posts/2019-04-06-email-analysis/index.html#analysis",
    "href": "posts/2019-04-06-email-analysis/index.html#analysis",
    "title": "Email analysis",
    "section": "Analysis",
    "text": "Analysis\nThere’s still some stuff to clean, but I’d rather go into the analysis. So, let’s get some questions to guide our purpose:\n\nWho sends me the most emails? Who receives emails from me?\nWhen do I get emails (mostly)?\nWhen should I do something about it (aka, reply)?\n\n\nWarning: We have to dance with parsing dates and times. I highly recommend being familiar with lubridate (for example, see https://rdrr.io/cran/lubridate/man/parse_date_time.html).\n\n\nMost frequent senders\nJust because I’m curious, let’s take a look at who are the all time senders!\n\n\n                    from   n\n1                  Quora 393\n2                 Maggie 316\n3                   Yair 216\n4                Luciano 173\n5                  Sarah 167\n6                  \"Bank 161\n7           \"Amazon.com\" 139\n8                Mariana 138\n9  pubchase@zappylab.com 131\n10            \"Mendeley\" 126\n\n\nIt’s cool to know that my lingering feeling (“wow…Quora just spams the hell out of me”) is supported by data. Other big spammers are, of course, the Bank and Amazon. People I work with and friends come high up too. Funny to see Mendeley and Pubchase on the top ten, it’s been a long journey of them sending me papers, thank you for that2.\n\n\nFrom me to you\nLet’s try to find the people I directly send the most emails to. I tend to send a lot of automatic reminders via email to myself so I removed me from the destination.\n\n\n       to   n\n1 Mariana 192\n2 Mariana 126\n3    Yair  86\n4 Mélanie  64\n5   Beata  59\n\n\nLooks like both my former advisers get most of my output (yes, same name first name, not related)."
  },
  {
    "objectID": "posts/2019-04-06-email-analysis/index.html#working-with-dates-and-times",
    "href": "posts/2019-04-06-email-analysis/index.html#working-with-dates-and-times",
    "title": "Email analysis",
    "section": "Working with dates and times",
    "text": "Working with dates and times\nEvery time I have to deal with dates, I have a miniature panic attack. As a general rule, you have to have all the variables that you want to use as separate columns (i.e, year, month, day, week_day, time, …). The lubridate package helps a lot, but it’s still quite an effort.\nWorking only with times of the day, regardless of date itself is problematic. Working with periods is difficult, so as.numeric(x, \"hour\") is a friend.\nHere’s a hint of how the date column in the original data actually looks like. This may or might not look the same way for you, it depends on your date settings.\n\n\n                             date\n1 Sat, 23 Mar 2019 08:57:48 -0700\n2 Sat, 23 Mar 2019 08:57:32 -0700\n3 Sat, 23 Mar 2019 20:25:31 -0400\n4 Sat, 23 Mar 2019 08:57:46 -0700\n5 Sat, 23 Mar 2019 08:57:35 -0700\n\n\nLet’s create all the variables we need. It seems like a lot because it should work out of the box and it doesn’t, but it’s actually somewhat straight-forward to get most of what we want."
  },
  {
    "objectID": "posts/2019-04-06-email-analysis/index.html#birdseye",
    "href": "posts/2019-04-06-email-analysis/index.html#birdseye",
    "title": "Email analysis",
    "section": "Birdseye",
    "text": "Birdseye\nLet’s look at how the whole email movement looks like. In the last couple of years, I clearly felt the load rising and rising. The lack of data in the early years is mostly due to me not downloading everything from the Hotmail account (it’s too late, too far in the past to fix :P). Besides, the trend likely holds quite well.\n\n\n\n\n\nIf we split by input and output, we can easily see that the input-output ratio went nuts when I moved to the US.\n\n\n\n\n\nThis is not really surprising, given the amount of unsolicited advertising I started getting since the move. Yes, I’m talking to you again Quora/Amazon/people trying to sell me stuff3. Of course, University related chains likely take a big chunk of the pie.\nI don’t feel like parsing out each sender out of the sheer amount. I have had the Gmail and Hotmail accounts for more than 10 years, but the University email is something relatively recent. All in all, considering the time I’ve had each account, the input rate coming from universities worries me. Here are the total email for each account:\n\n\n\n  Gmail hotmail     MIT   umass \n   7481   10331    1297    7122 \n\n\n\nWhen\nLet’s add the time of the day to the equation. This plot was made using ggbeeswarm package, I highly recommend checking it, it’s pure power. I got help to put the labels in the y axis from ‘00:00’ to ‘24:00’. You can find a toy example in this StackOverflow question.\n\n\nDaily news\nWhat’s the average number of emails per day? I’m including all the emails in from 2015 to 2019, including those that go directly to trash.\n\n\n\n\n\nFor those looking for some tabulated info, here it is:\n\n\n\nNumber of emails per day received in all accounts\n\n\nYear\nMean\nMin\nMax\nMode\n\n\n\n\n2015\n6.8\n1\n30\n3\n\n\n2016\n10.2\n1\n45\n2\n\n\n2017\n14.5\n1\n68\n2\n\n\n2018\n13.0\n1\n66\n6\n\n\n2019\n18.9\n1\n132\n7\n\n\n\n\n\nI am more inclined to graphics, the following figure shows not only an increasing mean, but, surprisingly, a widening range.\n\n\n\n\n\n\n\n\nAll days were not created equal\nOf course, the number of emails somewhat depends on the day of the week. We can easily see a decreasing trend.\n\n\nNA \nNA  Mon  Tue  Wed  Thu  Fri  Sat  Sun \nNA 4720 4748 4469 4326 3928 1938 1970\n\n\nAlthough the day of the week has influence on the amount of emails received, the time of the day seems to have a stronger, more permanent effect.\n\n\n\n\n\n\n\nEverything together\nIf we pool all the data together, it seems that I receive/send emails at all times, although there is more movement in the accounts around 10:00 and 16:30. Overall, the distributions are quite similar4.\n\n\n\n\n\n\nJust for fun\nJust for the fun of data visualization. Here’s the same plot but adding coord_polar to it. I believe it creates a very weird but good looking thing. It’s not really a clock but there’s something about it I can’t stop looking at5.\n\n\n\n\n\n\n\n\nSplit in two\nAs you can see from the figures above, the emails in the received bucket have two humps (wink, Bactrian camel, little prince), but I send emails at almost all times (except maybe between 2 AM and 5 AM). This is a bad habit, I should not be sending emails all the time, I should batch to diminish the costs associated with shifting tasks. I could just put a rule of thumb and check emails only once a day (e.g, 12:00:00). However, this might not be the best decision, because it chunks the response time in two very broad categories (either I get back to you somewhat quick, within 2 hours, or I take almost a full day to reply).\n\n\n\n\n\nAdditionally, checking emails only once might make me miss something somewhat fleeting. In general, I want to read things during the time they are relevant (did anybody say free pizza?).\nThe primary goal, then, is to minimize the times I check/send emails without 1) impacting my perceived response rate and 2) missing out too much info during the day. But that optimization problem is hard to solve and likely a waste of time (trust me, I tried and I’m not that smart).\nI believe we can solve it with a rule of thumb anyway. Let’s say, I would check emails twice a day and respond immediately, unless I need to harness some brain power to create an elaborate response6.\nI just wrote a “cost function” and calculated the cost for several combinations of times.\n\n\nShow the code\nvalues &lt;- emails %&gt;%\n  filter(simple_label==\"Received\") %&gt;%\n  mutate(val = as.numeric(seconds_to_period(my_time))) %&gt;%\n  pull(val)\n\n# calculate linear distance to minimize \n\ndist_to_min &lt;- function(values, possible_times){\n \n  min_time &lt;- min(possible_times)\n  max_time &lt;- max(possible_times)\n  # do nothing to first batch\n  corrected_values &lt;- ifelse(values &lt; max_time,\n                           values,\n  # shift the ones answered on next day, this already gives positive distance\n                          86400 - values + min_time)\n\n  \n  to_second &lt;- between(corrected_values, min_time, max_time)\n  second_batch &lt;- corrected_values[to_second]\n  first_batch &lt;- corrected_values[!to_second]  \n  \n  # Calculate distance (should be all positive)\n  dist_second &lt;- max_time - second_batch \n  \n  dist_first &lt;- ifelse(first_batch &lt; min_time,\n                       min_time - first_batch,\n                       corrected_values)\n\n  total_dist &lt;- sum(c(dist_first, dist_second))\n  \n  return(total_dist)\n}\n\n\nNow we can use our dist_to_min function in a loop. We’ll calculate from the first second of the day, to the last (86400) every half hour (1800 sec).\n\n\nShow the code\n# Create the data to iterate over\nval &lt;- seq(1, 86400, 1800)\nval &lt;- data.frame(t(combn(val,2)))\nnames(val) &lt;- c(\"Var1\", \"Var2\")\ndistance &lt;- numeric(length=nrow(val))\n\n# For loop...\nfor (i in 1:nrow(val)){\n  possible_times &lt;- val[i, ]\n  \n  distance[i] &lt;- dist_to_min(values, possible_times)\n  \n}\n\n\nThe function calculates the distance we want to minimize. The output looks like this.\n\n\n\n\n\nSounds like the combinations we care about are those below 2.5e+8.\n\n\n\n\n\nAll this long post is to say that, from now on, I will be answering my emails in either one of these combinations.\n\n\nNA   first_batch second_batch\nNA 1   12H 0M 1S    18H 0M 1S\nNA 2   11H 0M 1S   17H 30M 1S\n\n\n\n\nA finer grain\nJust for the fun of it, let’s take a closer look, a second by second analysis. It seems like machine programmed emails peak at 2 and 3 seconds past midnight.\n\n\nNA        time simple_label Freq\nNA 1  00:00:02     Received   77\nNA 2  00:00:03     Received   43\nNA 3  15:32:51     Received    6\nNA 4  10:17:11     Received    5\nNA 5  12:05:40     Received    5\nNA 6  12:09:11     Received    5\nNA 7  12:42:03     Received    5\nNA 8  15:30:29     Received    5\nNA 9  17:17:06     Received    5\nNA 10 09:07:15     Received    4\n\n\nWho are these emails coming from anyway?\n\n\n# A tibble: 10 × 2\n   from         n\n   &lt;chr&gt;    &lt;int&gt;\n 1 @mit.edu    33\n 2 @mit.edu    24\n 3 @mit.edu    23\n 4 @mit.edu    10\n 5 @mit.edu     6\n 6 @mit.edu     6\n 7 @mit.edu     4\n 8 @mit.edu     3\n 9 @mit.edu     3\n10 @mit.edu     2\n\n\nLooks like people at MIT programmed news to be sent seconds after midnight."
  },
  {
    "objectID": "posts/2019-04-06-email-analysis/index.html#summary",
    "href": "posts/2019-04-06-email-analysis/index.html#summary",
    "title": "Email analysis",
    "section": "Summary",
    "text": "Summary\nI have had a lot of fun doing this project. I also experienced an enormous amount of frustration with dates. Moreover, every time I thought this project was over, a new little idea for a not so little graph came into my mind. Of course, I went after it. I hope this info helps other people take a look at their own personal analytics and make some decisions. I am somewhat happy I have almost all notifications turned off (hence, no Facebook/Twitter/Slack/whatever appearing as top senders). In fact, turning email notifications off is the first thing I do when I sign up for a service/site, I encourage you to do the same.\nBatching is something I will start testing. I can’t control my input but, hopefully, the distributions of my sent email will start matching the times I designated. More importantly, people will not notice, even if the email input keeps increasing.\n\nSome people requested me to do the following scatter-plot. I went with the ggbeeswarm version on the text because I find it more appealing.\n\n\n\n\n\n\nI excluded parts of the code because it was too much. I am happy to share if requested!\n\nSources:\nhttps://jellis18.github.io/post/2018-01-17-mail-analysis/\nhttps://blog.stephenwolfram.com/2012/03/the-personal-analytics-of-my-life/\nhttps://uc-r.github.io/kmeans_clustering"
  },
  {
    "objectID": "posts/2019-04-06-email-analysis/index.html#footnotes",
    "href": "posts/2019-04-06-email-analysis/index.html#footnotes",
    "title": "Email analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou could actually stay in python (follow https://jellis18.github.io/post/2018-01-17-mail-analysis/). I’m way more comfortable with R for analysis and I only wanted python because I had the copy-paste version of getting my .mbox file to .csv fast.↩︎\nI had signed up for Mendeley before Elsevier bought it…I’m not quite happy about it now, but at least I still get paper recommendations.↩︎\nI know I could just unsubscribe to these kind of things, just the way I do with 99% of all other aggressive garbage. I just didn’t do it for these senders.↩︎\nDo they look like an elephant inside a boa or a hat?↩︎\nPlease, if you know how to make the 0:00 or 24:00 appear on the center, reach out! I couldn’t figure it out.↩︎\nIf you have as much free time as me, you can run a kmeans(...). My emails actually turned out to be around 2 clusters.↩︎"
  },
  {
    "objectID": "posts/2019-04-14-data-visualization-challenge-2/index.html",
    "href": "posts/2019-04-14-data-visualization-challenge-2/index.html",
    "title": "Data Visualization Challenge 2",
    "section": "",
    "text": "This post is made as a backup for the data visualization challenge number 2. Data comes from the daily posts of the members of the Data Visualization Society (DVS) on the DVS Slack channels. You can see everybody’s submissions for the challenge here.\nI am also very motivated to explore the dark versions of the ggplot themes. The package I’m going to be using is called ggdark."
  },
  {
    "objectID": "posts/2019-04-14-data-visualization-challenge-2/index.html#dive-in",
    "href": "posts/2019-04-14-data-visualization-challenge-2/index.html#dive-in",
    "title": "Data Visualization Challenge 2",
    "section": "Dive in",
    "text": "Dive in\nThese are the libraries we’ll need:\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggdark)\nlibrary(lubridate)\n\n\nWe read the data from the repository.\n\n\nCode\ndf &lt;- read_csv(\"https://raw.githubusercontent.com/data-visualization-society/datavizsociety/master/challenge_data/dvs_challenge_2_channel_topics_over_time/flattened_channel_data.csv\")\n\n\nLet’s perform some summary stats. There’s 62 channels, but I will focus on the top 15 channels as ranked by their total volume of characters. I’m using this metric because the correlation between characters and the number of posts is, naturally, good.\n\n\nCode\nggplot(df, aes(posts+responses, characters))+\n  geom_smooth(method = \"lm\", se=FALSE, lty=2)+\n  geom_point(alpha=0.4)+\n  dark_theme_bw()+\n  scale_y_continuous(labels = scales::label_number_si())\n\n\n\n\n\nSummary.\n\n\nCode\nsum_df &lt;- df %&gt;% group_by(channel) %&gt;%\n  summarise(total_channel = sum(characters),\n         median_channel = median(characters)) %&gt;%\n  top_n(15, wt =  total_channel) %&gt;%\n  arrange(desc(total_channel))\n\n\nModify the original data and do some stats.\n\n\nCode\ndf &lt;- df %&gt;% group_by(channel) %&gt;%\n  mutate(total_channel = sum(characters),\n         median_channel = median(characters),\n         char_per_ping = characters/(posts+responses))   %&gt;%\n  ungroup() %&gt;%\n  group_by(date) %&gt;%\n  mutate(daily_flow = sum(characters),\n         daily_posts = sum(posts+responses))%&gt;%\n  ungroup()"
  },
  {
    "objectID": "posts/2019-04-14-data-visualization-challenge-2/index.html#first-pair",
    "href": "posts/2019-04-14-data-visualization-challenge-2/index.html#first-pair",
    "title": "Data Visualization Challenge 2",
    "section": "First pair",
    "text": "First pair\nThe idea behind the first pair of plots is to see the sheer amount of volume on certain channels.\nA good way of seeing how the top channels are ordered according to output is to do an ordered boxplot.\n\n\nCode\ntop_box &lt;-  df %&gt;%\n  filter(channel %in% unique(sum_df$channel)) %&gt;%\n  mutate(channel=fct_reorder(factor(channel), median_channel)) %&gt;%\nggplot(aes(channel, log10(characters)))+\n  geom_boxplot()+\n  coord_flip()+\n  dark_theme_bw()+\n  labs(x=\"\")+\n  ggtitle(sprintf(\"Top %s Channels\",\n                  length(unique(sum_df$channel))),\n          \"Metric: median characters\")\n\n\nI’m also curious about how persistent in time the flow is.\n\n\nCode\nwave &lt;- df %&gt;%\n  filter(channel %in% unique(sum_df$channel)) %&gt;%\n  mutate(channel=fct_reorder(factor(channel), total_channel)) %&gt;% \n  ggplot(aes(date, channel, color=log10(characters))) +\n  geom_line(aes(lwd=characters))+\n  dark_theme_bw()+\n  labs(y = \"\", x=\"Date\")+\n  guides(color = FALSE)+\n  scale_color_gradient(low = \"#613A00\", high=\"#FA9800\")+\n  ggtitle(\"Top 15 channels\", \n          \"Metric: total characters\")+\n  scale_y_discrete(position = \"right\")+\n  theme(legend.position = \"none\")\n\n\nWe put everything together with the cowplot package.\n\n\nCode\ncowplot::plot_grid(top_box, wave)\n# Save the plot\n#ggsave(\"box_wave.svg\", width = 8, height = 4, units = \"in\", dpi=\"retina\")\n\n\nI later modified this output a bit using Inkscape."
  },
  {
    "objectID": "posts/2019-04-14-data-visualization-challenge-2/index.html#lengthy-channels",
    "href": "posts/2019-04-14-data-visualization-challenge-2/index.html#lengthy-channels",
    "title": "Data Visualization Challenge 2",
    "section": "Lengthy channels",
    "text": "Lengthy channels\nWhile most of the channels have a low median, even below a full tweet, it looks like some channels tend to have very lengthy posts.\n\n\nCode\n# Calculate median\nmedian_post &lt;- median(\n  df$characters/(df$posts +df$responses))\n\n# Do the plot  \nlengthy &lt;- ggplot(df, aes(log10(total_channel),\n               char_per_ping))+\n  dark_theme_bw()+\n  geom_hline(yintercept = 280, lty=2)+\n  geom_hline(yintercept = median(\n    df$characters/(df$posts +df$responses)), lty=2)+\n  annotate(\"text\", x = 3, y= c(200, 340), label=c(\"Median post\",\n                                            \"One tweet\"))+\n         geom_point(aes(color=channel), alpha=0.9)+\n         scale_color_viridis_d(direction = -1)+\n         theme(legend.position = \"none\")+\n  ggrepel::geom_text_repel(data=filter(df,\n                                       char_per_ping &gt; 850),\n            aes(label = channel, color=channel))+\n  labs(x=bquote(\n    log10 ~\"(total characters)\"),\n    y=\"characeters per post\")+\n  ggtitle(\"Channels with lengthy posts\")\n\n\n# Save\n# ggsave(\"lengthy.svg\", width = 8, height = 4, units = \"in\",dpi=\"retina\")"
  },
  {
    "objectID": "posts/2019-04-14-data-visualization-challenge-2/index.html#share-of-flow",
    "href": "posts/2019-04-14-data-visualization-challenge-2/index.html#share-of-flow",
    "title": "Data Visualization Challenge 2",
    "section": "Share of flow",
    "text": "Share of flow\nWhat is the share of each channel on the total flow within the DataViz Slack?\n\n\nCode\ntop_top_channels &lt;- sum_df %&gt;%\n  arrange(desc(total_channel)) %&gt;%\n  slice(1:5)\n\nshare &lt;- df %&gt;% group_by(date) %&gt;%\n  mutate(big_channel = ifelse(channel %in% top_top_channels$channel,\n                              channel, \"other\"),\n    total=sum(characters),\n    rel_char = characters/total) %&gt;%\n  ggplot(aes(date, rel_char, fill=big_channel))+\n  geom_col(width = 1)+\n  scale_fill_viridis_d(direction = -1)+\n  dark_theme_bw()+\n  theme(legend.position=c(.85,.5))+\n  labs(x=\"\", y=\"Relative share\", fill=\"Channel\")+\n  ggtitle(\"Share of the conversation\",\n          \"Relative share of the total characters per day\")+\n  scale_x_date(limits = c(as.Date(\"2019-02-18\"),\n                          as.Date(\"2019-04-23\")),\n               date_breaks = \"1 week\", \n               date_labels = \"%b-%d\")\n\n\nIt seems the initial bump was driven by many (lengthy) introductions, and nowadays the discussion has moved towards other channels.\n\n\nCode\nintro_decay &lt;-  ggplot(df, aes(date, daily_flow))+\n  geom_line()+\n  geom_line(data=filter(df, channel %in% c(\n    \"-introductions\")),\n            aes(date, characters), color=\"yellow\")+\n  dark_theme_bw()+\n  xlab(\"\") + \n  ylab(\"Daily characters\")+\n  annotate(\"text\", x=as.Date(c(\"2019-04-10\",\n                               \"2019-04-08\")),\n           y = c(1000, 50000),\n           label=c(\"-introductions\", \"all channels\"),\n           color=c(\"yellow\", \"white\"))+\n    scale_x_date(limits = c(as.Date(\"2019-02-18\"),\n                              as.Date(\"2019-04-23\")),\n                 date_breaks = \"1 week\", \n                 date_labels = \"%b-%d\") +\n  scale_y_continuous(labels = scales::label_number_si())\n\n\nLet’s see how it looks like.\n\n\nCode\n# We put everything together with cowplot\ncowplot::plot_grid(share,intro_decay,\n                   nrow = 2, rel_heights = c(2,1))\n# save\nggsave(filename = \"share_plot.svg\", \n       width = 12, \n       height = 6, \n       dpi=\"retina\")\n\n\nThe final version is this one."
  },
  {
    "objectID": "posts/2019-04-14-data-visualization-challenge-2/index.html#weekday-news",
    "href": "posts/2019-04-14-data-visualization-challenge-2/index.html#weekday-news",
    "title": "Data Visualization Challenge 2",
    "section": "Weekday news",
    "text": "Weekday news\nBecause everything is seasonal, let’s analyze by days of the week. Seems like Tuesday to Thursday are the days with most movement, waning down on Friday and into the weekend.\n\n\nCode\nggplot(df, aes(wday(date, label=TRUE, abbr = TRUE, week_start = 1),\n               daily_posts))+\n  geom_line(color=\"gray80\")+\n  stat_summary(geom = \"point\",\n               fun = median, size=2.5)+\n  dark_theme_bw()+\n  labs(x=\"\", y=\"Number of daily posts\",\n       title = \"Weekly post variations\",\n       subtitle = \"Points represent median daily post.\\nLines show full data range.\")\n\n# ggsave(filename= \"weekly_vars.svg\", width = 8, height = 6 , dpi=\"retina\")"
  },
  {
    "objectID": "posts/2019-05-09-Birthday-Meritocracy/index.html",
    "href": "posts/2019-05-09-Birthday-Meritocracy/index.html",
    "title": "Birthday Meritocracy",
    "section": "",
    "text": "We believe in meritocracy as one of the cornerstones of Western civilization. This idea is so embedded in our culture that we seldom question it. We praise those driven individuals who combine effort and talent to make it to the top. They smile in the cover of the magazines, the dreams of children and the nightmares of adults. Sports might be the activity where the blood, sweat and tears feel the most real and therefore are most notoriously broadcasted to the world.\nThe beautiful game is no exception. But what if we were wrong about the meritocracy assumption? What if there were random events that sneak in the player selection process? What if such events were not negligible, but heavily influenced what players that make it to the top?"
  },
  {
    "objectID": "posts/2019-05-09-Birthday-Meritocracy/index.html#one-trimester",
    "href": "posts/2019-05-09-Birthday-Meritocracy/index.html#one-trimester",
    "title": "Birthday Meritocracy",
    "section": "One trimester",
    "text": "One trimester\nThree or four months of difference at 25 year old is nothing1. Moreover, for most purposes in adult life, such a small difference in age is considered zero. But elite athletes are not ordinary adults and we don’t care about most of their purposes in life. We want them to do one thing extraordinarily well.\nWhat could a trimester difference do to a 25 year old, prime time, football player? You might still find yourself asking this question and this question might sound reasonable. But the data looks quite different. Let’s take a look."
  },
  {
    "objectID": "posts/2019-05-09-Birthday-Meritocracy/index.html#the-database",
    "href": "posts/2019-05-09-Birthday-Meritocracy/index.html#the-database",
    "title": "Birthday Meritocracy",
    "section": "The database",
    "text": "The database\nI compiled a small database of football players from www.soccerwiki.com. This database is not perfectly complete, but contains enough players to provide support to the general idea. Here’s a table of the number of players by country:\n\n\n\n\n\n\n  \n    \n    \n      \n      Country\n      Football Players (n)\n    \n  \n  \n    Brazil\nBrazil\n4606\n    United Kingdom\nUnited Kingdom1\n3967\n    Spain\nSpain\n3322\n    Italy\nItaly\n3297\n    Argentina\nArgentina\n2995\n    France\nFrance\n2453\n    Germany\nGermany\n2038\n    Colombia\nColombia\n1716\n    Uruguay\nUruguay\n1316\n  \n  \n  \n    \n      1 The data for the United Kingdom only comes from England.\n    \n  \n\n\nTotal number of players per country.\n\n\nHere’s a month by month representation of the number of players born in each country.\n\n\n\n\n\nThis looks quite strange, there’s a clear trend, with most players being born in the first months of the year. Maybe it’s a seasonal thing?\nWell…It’s quite difficult to support the idea that players born on any season are better. For example, January in Brazil means blazing hot summer while it’s freezing cold winter in Germany. Still, both countries seem to produce the most number of players in the first month of the year. In fact, the data show that being born on the first trimester after the cutoff date (e.g, January to March if cutoff is January 1st) increases the chances of being a top player by a huge margin."
  },
  {
    "objectID": "posts/2019-05-09-Birthday-Meritocracy/index.html#analysis-by-position",
    "href": "posts/2019-05-09-Birthday-Meritocracy/index.html#analysis-by-position",
    "title": "Birthday Meritocracy",
    "section": "Analysis by position",
    "text": "Analysis by position\nThere might be some logic behind this phenomenon. Maybe the birth effect is big for positions most strongly associated with bare physical strength. Thus, we could expect to find a stronger effect for those in defensive positions (i.e, Goalkeeper up to Defensive Midfielders) and find a broader distribution of birth dates for those positions that are reserved for the creatives and magicians of the ball, the offensive players.\n\n\n\n\n\nAgain, the evidence for a strong bias favoring players born on the first trimester is compelling. What’s going on here?"
  },
  {
    "objectID": "posts/2019-05-09-Birthday-Meritocracy/index.html#the-one-percent-difference",
    "href": "posts/2019-05-09-Birthday-Meritocracy/index.html#the-one-percent-difference",
    "title": "Birthday Meritocracy",
    "section": "The one percent difference",
    "text": "The one percent difference\nThis is not a post about how the top athletes distinguish themselves from the rest because of their talent and effort. This post is about something completely out of their control: their birth dates. Yes, it sounds crazy, but the month of birth plays a huge role in the success of a football player.\nFootball schools around the world are normally organized around the calendar year. It makes sense, if you are going to make a tournament for kids, instead of having kids of different ages competing together, you grab all the kids born on a same year and make them play against each other (5 year-old kids vs other 5 year-old kids). For example, when I was playing these tournaments, I played in the 1991 category.\nScouts and coaches might decide to promote a strong/talented kid. In my case, I could play for the 1990 or 1989 categories. But we all agree that it would be unfair to downgrade me to play for the 1995 category. I would have a huge developmental advantage. So far so good, but there’s something that escapes from everyone: one year is a lot of time. The developmental difference between a 5 year old kid born at the beginning of the year (e.g, January 1st) vs one at the very end (e.g, December 31st) is huge.\nLet’s take a less extreme case, kids born with a 180 days difference. That amount of days corresponds to an extra fraction of life lived (aka experience, aka time kicking the ball) of 0.1. It might sound like very little but that 10% advantage will definitely compound over the competitive career."
  },
  {
    "objectID": "posts/2019-05-09-Birthday-Meritocracy/index.html#logic-behind",
    "href": "posts/2019-05-09-Birthday-Meritocracy/index.html#logic-behind",
    "title": "Birthday Meritocracy",
    "section": "Logic behind",
    "text": "Logic behind\nIt turns out to be that scouts and coaches think they are selecting the best, but they are selecting the oldest. The January 1st cutoff is arbitrary and favors those born close to the date, who will be slightly stronger/faster/better only because they have had more time to play around in the world and, basically, grow. If you look closely, the only country that does not follow the pattern is the UK. But, yes, you guessed it, the cutoff date for the UK is August 31st.\nPlayers that are selected as the best normally play in the starting team and stay in the field more minutes each game [Experience based, Citation needed]. They get better training and more opportunities to play, either because they play more or because coaches pay more attention to them. Each extra minute reinforces the small initial difference, creating a real one. Of course, talent and effort play a role. But it consistently turns out to be that players born earlier on the calendar year end up being better than the other players. This phenomenon is known as relative age effect 2.\nWhenever we see skewed age distributions we should suspect it is likely due to arbitrary cuts. The magic ingredients go like this:\n\nMake an arbitrary cut.\nSelect those that are a tad ahead, those closest to the cut (older people relative to cut).\nDifferentiate training experience by giving more opportunities to those already ahead.\nHarvest the best players (older people relative to cut).\n\nThis is difficult to detect because it’s a self-fulfilled profecy for scouts (after all, they want their picks to end up being good). Basketball doesn’t have this kind of problem, mostly because availability of opportunities to train and the absence of strict cutoff dates. But in other sports, this effect is even more marked. In ice hockey, you are mostly constrained to the winter season to play and you need a rink, which puts more barriers on training3."
  },
  {
    "objectID": "posts/2019-05-09-Birthday-Meritocracy/index.html#market-value",
    "href": "posts/2019-05-09-Birthday-Meritocracy/index.html#market-value",
    "title": "Birthday Meritocracy",
    "section": "Market Value",
    "text": "Market Value\nI couldn’t scrape enough to make a case about market value. But a quick Google search lead me to a paper suggesting that, although month of birth has a big effect in the frequency of players, once the pros made it, their salaries are not related to such factor 4."
  },
  {
    "objectID": "posts/2019-05-09-Birthday-Meritocracy/index.html#the-bad-news",
    "href": "posts/2019-05-09-Birthday-Meritocracy/index.html#the-bad-news",
    "title": "Birthday Meritocracy",
    "section": "The bad news",
    "text": "The bad news\nThis effect transcends beyond sports. Turns out that arbitrary cutoffs during development affect academic performance. Kids born near the cutoff perform better at school and go to better universities. I cannot stop thinking that I was somewhat fortunate in this account. Overall, school never felt like too difficult. But maybe it’s because I had had enough days on earth and was mature enough to grasp concepts. Maybe it’s not only that “Math and Science came easy to me and I liked them”. Maybe I was born at the correct moment and I wouldn’t be pursuing academic endeavors if I hadn’t been born around the cutoff date in my country (July 1st).\nDividing school kids by age is retrograde. I would like to see a continuous progress with no boundaries on dates. I would like to see no kid be made to wait because he isn’t old enough yet and, more importantly, I would like to see no hurrying kids through content because their age says so. I would like to see people moving through school at their own pace5. Maybe we should start looking at performance adjusted by age, it might be the antidote we are looking for."
  },
  {
    "objectID": "posts/2019-05-09-Birthday-Meritocracy/index.html#the-silverlining",
    "href": "posts/2019-05-09-Birthday-Meritocracy/index.html#the-silverlining",
    "title": "Birthday Meritocracy",
    "section": "The silverlining",
    "text": "The silverlining\nWe can account for this effect and attempt to correct it! Some countries have started to make adjustments to the birth effect in football. An easy way of doing it is using two cutoff dates and training coaches (and scouts) against their bias.\nSolving this issue might require clubs to have two teams per year. This measure doesn’t have to go all the way until they are pro, but at least until kids are old enough, so that we keep below the 6-8 month difference. This will possibly increase the costs for everyone and makes logistics more difficult. However, we would see a huge increase in the production of players (and/or their quality) if we just change this arbitrary dates."
  },
  {
    "objectID": "posts/2019-05-09-Birthday-Meritocracy/index.html#wait-a-second",
    "href": "posts/2019-05-09-Birthday-Meritocracy/index.html#wait-a-second",
    "title": "Birthday Meritocracy",
    "section": "Wait a second",
    "text": "Wait a second\nThere’s a possible hypothesis I’ve been evading. The reasoning will be like this:\n\nThe frequency of Football players is a proportion of the people born in a country.\nThe frequency of births in the whole country is higher in the months that the Football Associations of each country use as cutoffs.\n\nIf both things were true, then that explains why we see a peak of players in January (or whatever the specific month of cutoff date is). I will use data from my country to show that births do not follow Football."
  },
  {
    "objectID": "posts/2019-05-09-Birthday-Meritocracy/index.html#argentina",
    "href": "posts/2019-05-09-Birthday-Meritocracy/index.html#argentina",
    "title": "Birthday Meritocracy",
    "section": "Argentina",
    "text": "Argentina\nMy country is well known for the production and exportation of football players. I had access to data on births in Buenos Aires. I later gained access to the data for the whole country, which showed mostly the same trend (you can see it at the very bottom of the article). The number of births in Argentina are somewhat constant. If anything, March seems to be the month with most consistent high amount of births.\nBirths in Argentina"
  },
  {
    "objectID": "posts/2019-05-09-Birthday-Meritocracy/index.html#footnotes",
    "href": "posts/2019-05-09-Birthday-Meritocracy/index.html#footnotes",
    "title": "Birthday Meritocracy",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt’s actually 1.32 % of life.↩︎\nGreat general info here https://en.wikipedia.org/wiki/Relative_age_effect↩︎\nI cannot help but relate this with Science. Limited resources, too many hands trying to pull a piece to themselves… add on some age effect, differential training/opportunities (quite often bought with money and good connections). We are probably loosing so many great minds.↩︎\nYou can find the paper here↩︎\nSalman Kahn expresses it way better than I could ever do in his TED talk.↩︎"
  },
  {
    "objectID": "posts/2019-09-10-los-mismos-de-siempre/index.html",
    "href": "posts/2019-09-10-los-mismos-de-siempre/index.html",
    "title": "Los mismos de siempre",
    "section": "",
    "text": "Hace rato que vengo viendo ciertas tendencias de gente que hace algo de análisis de datos con la actualidad Argentina. Por ejemplo, Daniel Schteingart comúnmente publica este tipo de análisis en diversos medios. Esto me llevó a buscar datos abiertos disponibles para jugar un rato.\nEn general, a mí me interesa más debatir sobre la implementación de políticas y pienso que el Poder Legislativo merece mayor atención. Siento que sobrestimamos (para bien y para mal) el poder de un Ejecutivo que es, al final del día, mucho menos determinante que aquellos que escriben las leyes según las cuales se rige el Estado. Exigimos representantes que nos representen."
  },
  {
    "objectID": "posts/2019-09-10-los-mismos-de-siempre/index.html#senadores-por-período",
    "href": "posts/2019-09-10-los-mismos-de-siempre/index.html#senadores-por-período",
    "title": "Los mismos de siempre",
    "section": "Senadores por período",
    "text": "Senadores por período\nEmpecemos mirando cómo evolucionaron las bancas a lo largo de nuestra historia. Vemos un comienzo con algo de recambio, seguido de un período con bajo recambio. A partir de 1930, empezamos a ver los vaivenes de un país pendular que recae en dictaduras con disolución del Poder Legislativo. Las últimas dos dictaduras están marcadas en rojo, seguidas de un alto recambio en el número de bancas (asignación de nuevos Senadores cuando se restituye el Poder Legislativo). El otro pico que llama la atención es el 2001, máximo evento de recambio en nuestra historia. Hoy por hoy, podría decirse que gozamos de unas dos décadas de estabilidad en las instituciones, en donde vemos un recambio de entre 20 y 30 bancas regularmente."
  },
  {
    "objectID": "posts/2019-09-10-los-mismos-de-siempre/index.html#cuánto-dura-un-período",
    "href": "posts/2019-09-10-los-mismos-de-siempre/index.html#cuánto-dura-un-período",
    "title": "Los mismos de siempre",
    "section": "¿Cuánto dura un período?",
    "text": "¿Cuánto dura un período?\nLa longitud de los mandatos ha cambiado a lo largo de la historia. Además, existieron períodos en los cuales intentar ser senador no parecía una carrera a largo término (cortesía de la inestabilidad de las instituciones argentinas durante gran parte del siglo XX).\nUn gran número de senadores no cumplen su mandato completo, ya sea por muerte, renuncia (a veces para ocupar otro cargo) o por destitución. Afortunadamente, en las últimas décadas, la tendencia parece estabilizarse hacia la longitud de mandato designada en la ley."
  },
  {
    "objectID": "posts/2019-09-10-los-mismos-de-siempre/index.html#quién-quiere-repetir",
    "href": "posts/2019-09-10-los-mismos-de-siempre/index.html#quién-quiere-repetir",
    "title": "Los mismos de siempre",
    "section": "¿Quién quiere repetir?",
    "text": "¿Quién quiere repetir?\nLa permanencia de un senador – o de cualquier otro político – en el cargo es un tema de debate interesante. En primer lugar, uno esperaría que cualquier trabajador con experienca y conocimiento sobre su área sea más eficiente y productivo. En criollo, si me tengo que operar de apendicitis mañana, prefiero un médico que haya realizado 1350 cirugías de apéndice que uno que haya realizado 122. Luego, si trasladamos este razonamiento a nuestras instituciones deberíamos querer que el Senado esté compuesto de políticos con muchos años de experiencia en la cámara alta.\nEl problema radica en identificar cuáles son los puestos que se parecen al del médico haciendo la misma cirugía y aquellos que no. Tomemos el ejemplo de los burócratas del Estado de carrera, los estadístas, los responsables de mantenimiento de infraestructura. Esta es la gente que hace funcionar el aparato, sin ellos, no hay implementación, no hay político que pueda hacer nada. Una cosa podría que identificarlos es que todos ellos deberían ser apolíticos y, como tales, deberían poder cumplir su labor siendo independientes del gobierno de turno3. Los investigadores del CONICET y los maestros también son buenos ejemplos de este tipo de empleados de Estado4.\nEn cambio, el rol de los integrantes del Poder Legislativo está contaminado por el hecho de que, entre otras cosas:\n\nLegislan su propio salario.\nTienen la postestad de regular actividades económicas de manera conveniente.\nTienen re-elección indefinida.\n\nPor supuesto, podemos sumar el cúmulo de beneficios legales (como canjear viáticos por dinero en efectivo y discresionalidad de presupuesto) y aquellos no legales (en criollo, corrupción). Pero éste no es el lugar para analizar eso, focalicemos en los períodos. Los datos mandan, así que veamos qué es lo que dicen. Históricamente, la gran mayoría de senadores no repite su cargo.\n\n\n\n\n\n\nRepetidores por su nombre\nEste fenómeno puede explicarse por múltiples causas. Antes de cualquier análsis, veamos quiénes son los repetidores.\n\n\n\n\n\nAcá vemos por primera vez algunos nombres que suenan más a calle o a localidad que a lo que hicieron. Pero también vemos algunos nombres bastante familiares y actuales. Recordemos que 4 o 5 mandatos son, según la duración de mandato actual, entre 24 y 30 años en el mismo puesto!\n\n\nMáximos reemplazantes\nUna curiosidad ingenua me llevó a preguntarme cuáles son los máximos reemplaznates. Me pregunté si, quizás, existía una caja con senadores de repuesto que pudieran cumplir los roles una vez que la figurita política de turno lograba conseguir los votos.\nEn otras palabras: ¿Es posible que exista un grupo de senadores comodín? Los datos presentan una triste realidad, una en la que la historia del Poder Legislativo Argentino está contaminada por sucesivas disoluciones. Llamémosles por su nombre, Golpes de Estado. Aún así, hay algunos senadores que han tomado la posta como suplente en reiteradas ocasiones.\n\n\n\n\n\n\n  \n    \n    \n      Reemplazante\n      Número de veces\n    \n  \n  \n    DISOLUCION DEL P. LEGISLATIVO,\n269\n    Sin Datos\n55\n    ES SENADOR SUPLENTE,\n18\n    VACANTE HASTA DISOLUC.P.L.,\n15\n    VACANTE HASTA FIN DEL PERIODO,\n9\n    JUAREZ, CARLOS ARTURO\n5\n    MAYANS, JOSÉ MIGUEL ÁNGEL\n5\n    MORALES, GERARDO RUBÉN\n5\n    ROMERO, JUAN CARLOS\n5\n    VERNA, CARLOS ALBERTO\n5"
  },
  {
    "objectID": "posts/2019-09-10-los-mismos-de-siempre/index.html#dias-en-el-poder-legislativo",
    "href": "posts/2019-09-10-los-mismos-de-siempre/index.html#dias-en-el-poder-legislativo",
    "title": "Los mismos de siempre",
    "section": "Dias en el poder legislativo",
    "text": "Dias en el poder legislativo\nQuiero hacer énfasis en la cantidad de días en el poder que implica repetir mandatos. Por eso, ordené a los senadores según el número de días que estuvieron (o aún están). Esta herramienta es simplemente para facilitar la visualización de los casos más extremos.\n\n\n\n\n\nSólo dos ejemplos, el resto de tarea:\nVidal fue gobernador de la provincia de Corrientes (dos veces), diputado, senador por 32 años (literalmente hasta su muerte, a los 80 años). Mendoza fue legislador provincial, diputado, gobernador de San Luis y luego senador por 27 años.\nIndependientemente de lo grandiosas o nefastas de sus contribuciones y el servicio a su País, queda claro que la carrera política les permitió perpetuarse de formas que no sólo son imposibles en la mayoría de otros gremios, sino que son mucho más peligrosas para la Nación.\nUna cosa que resulta interesante es que pareciera que existen dos tipos de senadoes. Existe un quiebre entre aquellos que se quedan 1 o 2 mandatos y los demás, que se perpetúan en el cargo al mejor estilo Julio Grondona (#TodoPasa).\n\n\n\n\n\nEs interesante que en ningún momento busqué los 10 años como punto de quiebre, este valor surgió de hacer una simple regresión partida. Podemos ver que los casos más extremos se desvían bastante de la regresión (sugiriendo un incremento aún más fuerte que uno lineal), pero me pareció una forma de simplificar el asunto: más de dos mandatos y estás para quedarte, no por un rato, para quedarte a vivir."
  },
  {
    "objectID": "posts/2019-09-10-los-mismos-de-siempre/index.html#esto-es-nuevo",
    "href": "posts/2019-09-10-los-mismos-de-siempre/index.html#esto-es-nuevo",
    "title": "Los mismos de siempre",
    "section": "Esto es nuevo",
    "text": "Esto es nuevo\nA pesar que los ejemplos con el récord de días tienen más de un siglo, la realidad es que, desde el retorno a la democracia, el cierto grado de estabilidad institucional ha permitido que estas prácticas se exacerben.\nEn el siguiente gráfico muestra los períodos de los 60 legisladores con mayor cantidad de días en el poder. Vale recalcar que dos mandatos de 9 años te colocan muy arriba en el el ránking, mientras que necesitarías 3 mandatos completos con la nueva longitud de 6 años para llegar a la misma cantidad. Es fácil ver que, entre 1850 y 1930, existieron períodos con ciertos senadores ocupando las bancas de manera prolongada. Sin embargo, estamos hablando de un período político en donde ni siquiera se había legislado la Ley Saenz Peña o recién se había implementado. Ver tanto abuso no es sorprendente si el proceso es fundamentalmente anti-democrático y los períodos son tan largos.\nDesde el último retorno a la democracia, un número importante de senadores se han perpetuado de manera impresionante, incluso integrando más de un partido político (ver también el color de la camiseta)."
  },
  {
    "objectID": "posts/2019-09-10-los-mismos-de-siempre/index.html#color",
    "href": "posts/2019-09-10-los-mismos-de-siempre/index.html#color",
    "title": "Los mismos de siempre",
    "section": "El color de la camiseta",
    "text": "El color de la camiseta\nTradicionalmente, la UCR y el partido Justicialista son los grupos más convocantes. El siguiente gráfico muestra la evolución de las bancas para cada partido.\n\n\n\n\n\nAdemás de demostrar lealtad a su partido, los senadores son representantes según su provincia. Uno esperaría que tengan fuerte arraigo y sean portavoces de las necesidades particulares de aquellos a los que representan. Esto, en gran medida, se ve reflejado en que la mayoría de senadores únicamente se presenta por una provincia. Sin embargo, hay senadores fueron electos en más de una provincia:\n\n\n\n\n\n\n  \n    \n    \n      Senador\n      Número de Provincias Representadas\n    \n  \n  \n    ECHAGÜE, PASCUAL\n2\n    ELIAS, ANGEL\n2\n    FERNÁNDEZ DE KIRCHNER, CRISTINA E.\n2\n    FERRE, PEDRO\n2\n    GARCIA, ALFREDO\n2\n    IGARZABAL, RAFAEL\n2\n    IRIGOYEN, BERNARDO DE\n2\n    LEGUIZAMÓN, MARÍA LAURA\n2\n    LEIVA, MANUEL\n2\n    ROCA, JULIO ARGENTINO\n2\n    SAGUIER, FERNANDO\n2\n    VILLANUEVA, BENITO\n2\n  \n  \n  \n\n\n\n\nUn hecho más interesante es el cambio de partido. No me queda claro si esto habla de la inestabilidad en el tiempo de los partidos o de que algunos senadores se orientan con la corriente (por ponerlo de manera políticamente correcta).\nLos siguientes senadores ejercieron cargo representando a más de un partido:\n\n\n\n\n\n\n  \n    \n    \n      Senador\n      Número de Partidos Representadas\n    \n  \n  \n    ALPEROVICH, JOSÉ JORGE\n2\n    BASUALDO, ROBERTO GUSTAVO\n3\n    BLAS, INÉS IMELDA\n2\n    BRAVO, LEOPOLDO\n2\n    BRITOS, ORALDO NORVEL\n2\n    CARO, JOSE ARMANDO\n2\n    CASTILLO, OSCAR ANÍBAL\n3\n    CASTRO, MARÍA ELISA\n2\n    CLOSS, MAURICE FABIÁN\n2\n    COLAZO, MARIO JORGE\n2\n    ELÍAS DE PEREZ, SILVIA BEATRIZ\n2\n    ESCUDERO, SONIA MARGARITA\n2\n    FELLNER, LILIANA BEATRIZ\n2\n    FERIS, GABRIEL\n2\n    FERNÁNDEZ DE KIRCHNER, CRISTINA E.\n3\n    FERNÁNDEZ, NICOLÁS ALEJANDRO\n2\n    FUENTES, MARCELO JORGE\n2\n    GARCIA, ALFREDO\n2\n    GIACOPPO, SILVIA DEL ROSARIO\n2\n    GIUSTINIANI, RUBÉN HÉCTOR\n2\n    GONZÁLEZ, MARÍA TERESA MARGARITA\n2\n    GUASTAVINO, PEDRO GUILLERMO ÁNGEL\n2\n    GUINLE, MARCELO ALEJANDRO HORACIO\n2\n    JENEFES, GUILLERMO RAÚL\n2\n    LATORRE, ROXANA ITATÍ\n2\n    LEGUIZAMÓN, MARÍA LAURA\n2\n    MARINO, JUAN CARLOS\n3\n    MARTIARENA, JOSE H.\n3\n    MAYANS, JOSÉ MIGUEL ÁNGEL\n3\n    MAZA, ADA MERCEDES\n2\n    MENEM, CARLOS SAÚL\n2\n    MORALES, GERARDO RUBÉN\n2\n    MURGUIA, EDGARDO P.\n2\n    NEGRE DE ALONSO, LILIANA TERESITA\n3\n    PETCOFF NAIDENOFF, LUIS CARLOS\n2\n    PICHETTO, MIGUEL ÁNGEL\n3\n    REUTEMANN, CARLOS ALBERTO\n3\n    RODRÍGUEZ SAÁ, ADOLFO\n3\n    ROMERO, JUAN CARLOS\n3\n    SAADI, VICENTE LEONIDAS\n3\n    SANZ, ERNESTO RICARDO\n2\n    SNOPEK, CARLOS\n2\n    TEISAIRE, ALBERTO\n2\n    VIUDES, ISABEL JOSEFA\n2"
  },
  {
    "objectID": "posts/2019-09-10-los-mismos-de-siempre/index.html#y-luego-2001",
    "href": "posts/2019-09-10-los-mismos-de-siempre/index.html#y-luego-2001",
    "title": "Los mismos de siempre",
    "section": "Y luego, 2001",
    "text": "Y luego, 2001\n\nExplosión de partidos\n\nSi esto no les gusta, hagan un partido político, ganen las elecciones y después hagan lo que quieran – CFK, 2012\n\nDesde el comienzo de los tiempos, el número total de partidos registrados que lograron obtener una banca en el Senado es de 109 partidos.\n\n\n\n\n\n\n  \n    \n    \n      Etapa\n      Partidos (#)\n    \n  \n  \n    pre-2001\n47\n    post-2001\n70\n  \n  \n  \n\n\n\n\n\nAjuste por alianzas\nAún cuando ajustamos por las múltiples alianzas (los justicialistas, la UCR y las distintas formas de Kirchnerismo y PRO). Las diferencias entre antes y después del 2001 son impresionantes.\n\n\n\n\n\n\n  \n    \n    \n      Etapa\n      Partidos (#)\n    \n  \n  \n    post-2001\n48\n    pre-2001\n41\n  \n  \n  \n\n\n\n\nEsta etapa también viene de la mano de la desaparición de los partidos tradicionales y la aparición de otros5."
  },
  {
    "objectID": "posts/2019-09-10-los-mismos-de-siempre/index.html#partidos-por-su-nombre",
    "href": "posts/2019-09-10-los-mismos-de-siempre/index.html#partidos-por-su-nombre",
    "title": "Los mismos de siempre",
    "section": "Partidos por su nombre",
    "text": "Partidos por su nombre\nPor pura diversión, imaginemos que queremos crear un partido político en argentina. Pero no queremos un partido cualquiera, queremos un partido que logre poner bancas en el senado. Si nos olvidamos de la política y las promesas de campaña, ¿cuáles son las palabras que deberían aparecer en el nombre para lograr nuestro cometido?"
  },
  {
    "objectID": "posts/2019-09-10-los-mismos-de-siempre/index.html#la-red-de-partidos",
    "href": "posts/2019-09-10-los-mismos-de-siempre/index.html#la-red-de-partidos",
    "title": "Los mismos de siempre",
    "section": "La red de partidos",
    "text": "La red de partidos\nEl siguiente gráfico muestra cómo están conectados los senadores con los distintos partidos. Es interesante ver la corriente ideológica del justicialismo, desde el Peronismo hasta el Frente para la Victoria. También es bastante claro el bunker de la UCR."
  },
  {
    "objectID": "posts/2019-09-10-los-mismos-de-siempre/index.html#footnotes",
    "href": "posts/2019-09-10-los-mismos-de-siempre/index.html#footnotes",
    "title": "Los mismos de siempre",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDesde Wikipedia a su pantalla: “Los requisitos para ser elegidos senador son: tener la edad de treinta años, haber sido seis años ciudadano de la Nación, y ser natural de la provincia que lo elija, o con dos años de residencia inmediata en ella.”↩︎\nNi hablar de una coronaria a corazón abierto, no?↩︎\nCuando digo apolíticos no me refiero a sus conviciones personales e inclinaciones políticas. Al menos en mi mente, el funcionamiento interno de ciertas dependencias del Estado debe ser autónomo. Un dato es un dato (sea la masa de un átomo o el desempleo), un docente o un estadísta del INDEC no deberían estar afectados en su labor por el partido de turno.↩︎\nIncluso en estas circunstancias, no considero que los investigadores o los maestros deban estar haciendo siempre la misma tarea. Al contrario, sendos gremios deberían funcionar con una estructura que permita aprovechar la experiencia para el mejor funcionamiento en vez de promover una jerarquía absurda por la jerarquía misma. Una científica jóven quizás esté mejor capacitada para hacer experimentos con sus propias manos. Un maestro jóven para interactuar con alumnos sin que haya un salto generacional muy pronunciado. Aquellos que tengan mayor experiencia, quizás pueda impulsar cambios estructurales para mejorar aspectos que no estén funcionando.↩︎\nPara simplificar, elegí partidos que lograron obtener más de 3 bancas.↩︎"
  },
  {
    "objectID": "posts/2019-11-14-on-pipelines/index.html",
    "href": "posts/2019-11-14-on-pipelines/index.html",
    "title": "On pipelines",
    "section": "",
    "text": "I have been thinking about different problems I have when writing code and the things that I normally try to do to keep my projects clean and functional. I wrote this post to put this thoughts out there, hopefully I will receive input from the great software engineers."
  },
  {
    "objectID": "posts/2019-11-14-on-pipelines/index.html#problems-in-mind",
    "href": "posts/2019-11-14-on-pipelines/index.html#problems-in-mind",
    "title": "On pipelines",
    "section": "Problems in mind",
    "text": "Problems in mind\n\nWhere do you live?\nCode files usually live in one folder, which is also a GitHub folder that you and your team commit/push to. So far, so good. But what do you do with the data to feed that monster pipeline of yours?\nI will assume that your concerns with data privacy are minor or you handled them accordingly (only private parties have access to the data).\nNow, you still have the problem of where to put this other folder, which is basically a size problem.\n\nSmall files can live with your data\n\nThis is the case for small and few text files of some thousand rows. Easy enough, you just go with your /repo-name/data/ and live happily ever after.\n\nMedium size files\n\nThese files are big enough to be a problem for hosting on GitHub. File formats start to be an issue here, images and video will not be easily accessible anywhere you take it.\nOptions: the cloud ☁️\nPros: It’s fluffy. Now, seriously, it’s good that your code can point to one place, download the stuff into local and use it. Every computer can do the same and there should be no problem. Because your sizes are not huge, you should be fine.\nCons: You need internet. No, internet it’s not everywhere all the time1. Internet is not in my cellphone on a second basement in a concrete building. Even with the fastest internet, it’s not trivial to setup your-favorite-cloud-service to allow access to the-sketchy-script-you-wrote2.\n\n\n\n\nOptions: Good old-fashioned external hard drive. 💾\nPros: This is a good one if your data size is in the Gb range and you don’t really need to share it with too many people.\nCons: Hard drives fail. Are you ready to lose your data? It starts to get really annoying when you have to do back-ups of your data and your data is big enough that you can’t use your computer’s hard drive (that’s why you chose an external hard drive in the first place). Should you have an external hard drive for the external hard drive? Are you planning to write the output of your code on those hard drives? Brace for impact.\n\n\n\nExternal hard-drives might have paths that change depending on which computer is connected to. This can easily be a path inferno. Moreover, some hard drives don’t work if you try to use them in different OS.\n\nLarge sizes\n\nI work with brains. Last time I checked, one mouse brain is ~2TB, n=1, just a few channels, not even the best resolution we can get.\nI think local/cloud servers are the only way to go here3. I don’t have a lot of experience with this, but I have suffered internet upload/download speed problems when I try to sync with my cloud back-ups or share image/video files with my team.\n\n\nPaths need to be absolute\nBecause your working directory is the folder where your code lives4, but your data folder lives elsewhere, you kind of need to use absolute paths all the time.\nI have only been able to fix this issue using functions that attempt to fix this when running the script.\n\nfix_working_environment &lt;- function(saved_path,\n                                    local_path){\n  # if the folder structure doesn't work as expected...\n  # this will explode \n  stringr::str_replace(saved_path,\n                       \"some_regular_expression\",\n                       local_path)\n}\n\nThis is particularly annoying when you have to run commands that involve calling things from console.\nLet’s call ImageJ from R.\n\nsystem(paste(\"/home/matias/Downloads/Fiji.app/ImageJ-linux64 --run\",\n             macro_to_run))\n\nThe moment somebody changes the Fiji folder, or tries to call ImageJ from another computer, that code brakes. I’m unaware of how to make sure these things bullet-proof, please enlighten me.\nLet’s call python from R. Wait, what version of python do you want? I rest my case.\n\n\nProcesses are identified by the files\nI have this problem quite often. It might be because my pipelines follow this logic.\n\n\n\n\n\n\nIt’s quite difficult to escape the infinite list all files –&gt; apply function to all files –&gt; write computations into new files loop. I don’t really know what’s on the other side.\nThe main problem is that your previous, current, and next files always serve as identifiers and you need to carry over their absolute path (to be able to read them form your data folder). Whenever these paths get corrupted (or you change your computer) things stop working.\nThis problem might stem from the fact that I normally have to process experimental units through the pipeline. I have to do many things to an experimental unit and have many many experimental units composing the data for one pipeline. That’s when my inner voice goes:\n\nBut I would also like to have the possibility to run or re-run just one (or just a few experimental units).\n\nThe way I handle this is by leaving open the door to hand selection of files (aka interactive mode, not fun). However, interactive mode somewhat helps with the problem below.\n\n\nDon’t move my files\nPeople do stuff people normally do, like moving folders around…that’s BAD, REALLY BAD. It’s also quite difficult to communicate the need to keep the file structure without casting the magic spells of everything will break5.\nI don’t feel good with the level of dependency on file structure that my projects always end up having. Please enlighten me on this one too!\n\n\nDon’t rename my files\nDon’t rename my files, except when I do. That would be a better subtitle of this section. A great way of not dealing with multiple copies of the same files. For example, let’s say you applied a mask to an image and then cropped, and rotated it. How many files do you keep? What if your image size was 1 Gb?\nMy hack around this is to rename the files (this include the cases where I just want to move files to specific sub-folders). Because I rely so much on the file names, this renaming usually comes back to bite me. I just 🤷.\n\n\nOperating systems\nI’m writing this in 2019, I thought the OS problem was solved. Turns out it’s not solved at all and developers shy away from it more often than they should. I understand them, developing for every OS is a huge pain and requires you to constantly check in multiple machines (or have access to a teammate that breaks your code as soon as you push it)."
  },
  {
    "objectID": "posts/2019-11-14-on-pipelines/index.html#what-is-your-approach",
    "href": "posts/2019-11-14-on-pipelines/index.html#what-is-your-approach",
    "title": "On pipelines",
    "section": "What is your approach?",
    "text": "What is your approach?\nThis is something I will continue to think for a long time, and my approach might need to be adjusted to each situation. What is your current approach?"
  },
  {
    "objectID": "posts/2019-11-14-on-pipelines/index.html#footnotes",
    "href": "posts/2019-11-14-on-pipelines/index.html#footnotes",
    "title": "On pipelines",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMaybe I should say moderately fast and stable internet.↩︎\nImage borrowed from here↩︎\nAnd yet, for many practical reasons, I never do this.↩︎\nAnd you should never forcefully set the working directory elsewhere.↩︎\nYes, your computer is on the line! And I will get all your passwords (?).↩︎"
  },
  {
    "objectID": "posts/2019-11-22-fitbit-analysis/index.html",
    "href": "posts/2019-11-22-fitbit-analysis/index.html",
    "title": "Fitbit Analysis",
    "section": "",
    "text": "It’s been a bit more than a year since I got a fitbit and I have been pretty excited about tracking my activity and heart rate. I should say I’m quite surprised about the sleep data. Tracking sleep has been, in fact, the most exciting feature, and I now strive to get at least 7 hours of sleep per night.\nLet’s first see a glimpse of the data, just to know what type of data we are dealing with.\n# A tibble: 5 × 8\n  date_time           dateTime   dataset_time variable value total_value\n  &lt;dttm&gt;              &lt;date&gt;     &lt;time&gt;       &lt;chr&gt;    &lt;dbl&gt;       &lt;dbl&gt;\n1 2018-06-10 00:00:00 2018-06-10 00'00\"       steps        0        7256\n2 2018-06-10 00:01:00 2018-06-10 01'00\"       steps        8        7256\n3 2018-06-10 00:02:00 2018-06-10 02'00\"       steps        0        7256\n4 2018-06-10 00:03:00 2018-06-10 03'00\"       steps        0        7256\n5 2018-06-10 00:04:00 2018-06-10 04'00\"       steps        0        7256\n# ℹ 2 more variables: date &lt;date&gt;, time &lt;time&gt;"
  },
  {
    "objectID": "posts/2019-11-22-fitbit-analysis/index.html#density-plots",
    "href": "posts/2019-11-22-fitbit-analysis/index.html#density-plots",
    "title": "Fitbit Analysis",
    "section": "Density plots",
    "text": "Density plots\nLet’s now inspect the overall distribution for heart rate and step values."
  },
  {
    "objectID": "posts/2019-11-22-fitbit-analysis/index.html#when-do-i-move",
    "href": "posts/2019-11-22-fitbit-analysis/index.html#when-do-i-move",
    "title": "Fitbit Analysis",
    "section": "When do I move?",
    "text": "When do I move?\nI will start by focusing on the data for steps.\nI’m curious to see what times of the day have the most activity. Because I have a quite large amount of data points (~751 K) I will use geom_hex() to count for me and simplify rendering1.\n\n\n\n\n\nWell, I should have remembered that for the vast majority of minutes (regardless of the hour of the day), the count is exactly zero. Let’s only look at the positive counts:\n\n\n\n\n\nWe see now some patches that have high activity (&gt; 100 steps), particularly around 9:00, 12:00 and 18:00. These mostly correspond to “going to work”, “activity around lunch time (?)”, and “going home / physical activity”. For all other cases, it looks like I move around 10-20 steps per minute, regardless of the minute within the hour.\n\nLast 10 minutes\nThe result above is interesting because I usually have to be reminded by Fitbit to “move up to 250 steps in the hour”. I receive this notification during the last 10 minutes of the hour and I would think that during those 10 minutes I put more steps than during the first 50. The data show I’m wrong:\n\n\n\n\n\nThat being said, I want to keep my reminder on. I feel like having it turned on definitely adds ~ 1000-2000 steps per day.\n\n\nDaily average\nLet’s get one level above and aggregate each day as a unit. This plot shows a nice trend, with months from May to August showing an increase on the number of steps. Keep in mind that November will show little average steps because for that month we have incomplete data (last day in database is 2019-11-13),\n\n\n\n\n\nAt this part of the analysis, I should make clear that I took vacations from 2019-06-27 to 2019-07-11. We will use this information in the analysis to make some things clear."
  },
  {
    "objectID": "posts/2019-11-22-fitbit-analysis/index.html#distribution",
    "href": "posts/2019-11-22-fitbit-analysis/index.html#distribution",
    "title": "Fitbit Analysis",
    "section": "Distribution",
    "text": "Distribution\nWe looked at the average daily steps for each month, how about the distribution of daily steps? We see that most days I come quite close to the default target of 10K steps. There are some days with very little steps (see below) and, obviously, some days with extreme number of steps."
  },
  {
    "objectID": "posts/2019-11-22-fitbit-analysis/index.html#extreme-events",
    "href": "posts/2019-11-22-fitbit-analysis/index.html#extreme-events",
    "title": "Fitbit Analysis",
    "section": "Extreme events",
    "text": "Extreme events\nUsing the boxplot below, we can define extreme events as instances where I walked more than 20K steps. I chose to plot this by day of the week, to get an insight about periodicity of events.\n\n\n\n\n\nBecause I walked a lot during the vacations, I highlighted the days on top of the previous boxplot. Most of the extreme events are definitely during the vacations. Moreover, none of the days I walked less than 10K steps, pretty amazing!\n\n\n\n\n\nThere are some extreme low events, these are quite likely the days I just don’t wear the fitbit (or days I forget to wear it for most of the day). Just because I can order the data and make another plot, I went ahead and did it!\n\n\n\n\n\nWe usually go for walks on Saturdays and/or Sundays. Knowing this little piece of data, it’s quite expected to see Saturdays being the days with higher number of steps (and hence higher success rate on the 10K target)."
  },
  {
    "objectID": "posts/2019-11-22-fitbit-analysis/index.html#season",
    "href": "posts/2019-11-22-fitbit-analysis/index.html#season",
    "title": "Fitbit Analysis",
    "section": "Season",
    "text": "Season\nI want to turn the focus now to the seasonality of the data. I will use a helper function getSeason() that I took from StackOverflow.\nWe can inspect the effect of season on my walking.\n\n\n\n\n\nThe plot above is not good, it fails to communicate. I think this is a better way to show the data."
  },
  {
    "objectID": "posts/2019-11-22-fitbit-analysis/index.html#a-years-heart-rate-in-one-plot",
    "href": "posts/2019-11-22-fitbit-analysis/index.html#a-years-heart-rate-in-one-plot",
    "title": "Fitbit Analysis",
    "section": "A year’s heart rate in one plot",
    "text": "A year’s heart rate in one plot\nI’m borrowing heavily from Nick here. But I thought it was a brilliant plot, so I took it for a ride with my data. I actually changed a few things, I decided to keep the native sampling rate and use geom_line() instead of down-sampling and using geom_tile(). The overall trend is clear, movements during the morning and the afternoon that correlate well with going and coming back from work. Somewhere around July 2019 you can see the change in timezone when I took vacations. There are a couple of days in late May with continuously high or lacking values, I take this as one of the days I forgot the fitbit at home, likely spurious measures."
  },
  {
    "objectID": "posts/2019-11-22-fitbit-analysis/index.html#code",
    "href": "posts/2019-11-22-fitbit-analysis/index.html#code",
    "title": "Fitbit Analysis",
    "section": "Code",
    "text": "Code\nThe code for this post is quite long and I thought it would get in the way. I am happy to share upon request, hit me up on Twitter or in the comments below."
  },
  {
    "objectID": "posts/2019-11-22-fitbit-analysis/index.html#sources",
    "href": "posts/2019-11-22-fitbit-analysis/index.html#sources",
    "title": "Fitbit Analysis",
    "section": "Sources",
    "text": "Sources\n\nhttps://livefreeordichotomize.com/2017/12/27/a-year-as-told-by-fitbit/"
  },
  {
    "objectID": "posts/2019-11-22-fitbit-analysis/index.html#footnotes",
    "href": "posts/2019-11-22-fitbit-analysis/index.html#footnotes",
    "title": "Fitbit Analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI had been searching for excuses to use this function for a while. Check it out here↩︎"
  },
  {
    "objectID": "posts/2020-01-02-writing-checklist/index.html",
    "href": "posts/2020-01-02-writing-checklist/index.html",
    "title": "Writing Checklist",
    "section": "",
    "text": "I spent a good portion of the end of 2019 and the beginning of 2020 reading Paul Graham’s essays. Many caught my attention (and my fingers to take notes). I want to comment on an essay about advice for good writing. Because it was written in a way that allowed for easy reformatting (ideas separated by ;), I thought it would be cool to convert the prose into a checklist)."
  },
  {
    "objectID": "posts/2020-01-02-writing-checklist/index.html#creating-the-checklist",
    "href": "posts/2020-01-02-writing-checklist/index.html#creating-the-checklist",
    "title": "Writing Checklist",
    "section": "Creating the checklist",
    "text": "Creating the checklist\nThe main text comes from here. For simplicity, I have only used the main paragraph (aka, copy and paste the second paragraph). I omitted it here because it would be long and you can read it in the native format from the source.\nIn the code chunk below I comment the steps I took.\n\n# text is copy-paste of the second paragraph\n# text &lt;- c(\"...\") ... == copy-paste\n\n# split using \"; \"\nsp &lt;- stringr::str_split(text, \"; \")\n\n# To create a checklist\n# DASH SPACE [ ] SPACE SPACE\nsp &lt;- paste0(\"- [ ]  \", unlist(sp))\n# append markdown title\nsp &lt;- c(\"# Paul Graham's Writing Checklist\", sp)\n\n# Capitalize first letter of every sentence\nsp &lt;- stringr::str_to_sentence(sp)\n\nIf you want to write it to file, you can use writeLines()\n\n# write\nfileConn &lt;- file(\"paul_graham_writing_checklist.md\")\nwriteLines(sp, fileConn)\nclose(fileConn)\n\n# If you want to knit to html you can do this\n# knitr::knit2html(\"paul_graham_writing_checklist.md\")\n\nLet’s take a look at the product:\n\n# printing for web post\ncat(sp, sep = \"\\n\")\n\n# Paul graham's writing checklist\n- [ ]  Write a bad version 1 as fast as you can\n- [ ]  Rewrite it over and over\n- [ ]  Cut out everything unnecessary\n- [ ]  Write in a conversational tone\n- [ ]  Develop a nose for bad writing, so you can see and fix it in yours\n- [ ]  Imitate writers you like\n- [ ]  If you can't get started, tell someone what you plan to write about, then write down what you said\n- [ ]  Expect 80% of the ideas in an essay to happen after you start writing it, and 50% of those you start with to be wrong\n- [ ]  Be confident enough to cut\n- [ ]  Have friends you trust read your stuff and tell you which bits are confusing or drag\n- [ ]  Don't (always) make detailed outlines\n- [ ]  Mull ideas over for a few days before writing\n- [ ]  Carry a small notebook or scrap paper with you\n- [ ]  Start writing when you think of the first sentence\n- [ ]  If a deadline forces you to start before that, just say the most important sentence first\n- [ ]  Write about stuff you like\n- [ ]  Don't try to sound impressive\n- [ ]  Don't hesitate to change the topic on the fly\n- [ ]  Use footnotes to contain digressions\n- [ ]  Use anaphora to knit sentences together\n- [ ]  Read your essays out loud to see (a) where you stumble over awkward phrases and (b) which bits are boring (the paragraphs you dread reading)\n- [ ]  Try to tell the reader something new and useful\n- [ ]  Work in fairly big quanta of time\n- [ ]  When you restart, begin by rereading what you have so far\n- [ ]  When you finish, leave yourself something easy to start with\n- [ ]  Accumulate notes for topics you plan to cover at the bottom of the file\n- [ ]  Don't feel obliged to cover any of them\n- [ ]  Write for a reader who won't read the essay as carefully as you do, just as pop songs are designed to sound ok on crappy car radios\n- [ ]  If you say anything mistaken, fix it immediately\n- [ ]  Ask friends which sentence you'll regret most\n- [ ]  Go back and tone down harsh remarks\n- [ ]  Publish stuff online, because an audience makes you write more, and thus generate more ideas\n- [ ]  Print out drafts instead of just looking at them on the screen\n- [ ]  Use simple, germanic words\n- [ ]  Learn to distinguish surprises from digressions\n- [ ]  Learn to recognize the approach of an ending, and when one appears, grab it.\n\n\nThe checklist is not in strict chronological order, so I might try to reshape it later into something that would make more sense as a timeline. It will render as a checklist in something that is GitHub flavored (e.g., on GitHub). However, I will probably just use it as text file or print it as is.\nI also noticed that the link for the Spanish translation is broken/outdated, so I tried my best to translate it. Here it is"
  },
  {
    "objectID": "posts/2020-01-02-writing-checklist/index.html#summary",
    "href": "posts/2020-01-02-writing-checklist/index.html#summary",
    "title": "Writing Checklist",
    "section": "Summary",
    "text": "Summary\nI think this checklist is an awesome learning opportunity and a clear path to improving one’s writing. Quite happy to have found it, looking forward to improving my skills.\nI got to learn a few cool R things like, 1) I can use writeLines() to write a .md file and 2) stringr::str_to_sentence() is an awesome function."
  },
  {
    "objectID": "posts/2020-04-30-complex-fun/index.html",
    "href": "posts/2020-04-30-complex-fun/index.html",
    "title": "Complex Fun",
    "section": "",
    "text": "People say that if you have to explain a joke, it loses value. Probably true. I guess I just wanted to partially comment on how different disciplines analyze different levels of complexity.\nAs molecular biologist, I think we were trained to seldom think about the whole system, we were trained to just give up complexity and focus on a couple of fancy names (yes, YFG fits pretty well as the center of this Universe). Genes are awesome, weak protein-protein interactions and conformational changes rock. Are you missing the whole world by focusing too much?\nAs a biologist, I think we were trained by association (i.e., “If this, then that. Now repeat logic for a million different processes”). This means that our view of every phenomenon tends to be overly complicated. We do this at every scale we analyze Life, not always with the study complexity in mind, but just as a reflex. This is not an intrinsic fault of the biologist, it might be just a result of how data is collected and the empirical nature of the field.\nI think physicists get it often right, as simple as it should be, not less. If the whole thing is a tennis ball, and your system of interest is ruled by macro laws, just approximate and do \\(m\\vec{g}\\).\nAll in all, this is the same dataset, 3 worlds apart (or 3 convention centers apart). I think the joke is on us :smile:.\n\nFor those of you who want to recreate, you can find the R code below:\n\n\nCode\nlibrary(tidyverse)\nlibrary(xkcd)\nlibrary(patchwork)\n\nset.seed(2)\ndf &lt;- tibble(\n  x = rnorm(100, 0, 2),\n  y = rnorm(100, 0, 2)\n) %&gt;%\n  mutate(time = 1:n())\ndf &lt;- tweenr::tween_along(df,'cubic-in-out', 1000, along = time)\n\n\nmy_theme &lt;-\n  theme_xkcd()+\n  theme(panel.border = element_rect(fill=NA, color=\"black\", size=1),\n        axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        plot.title = element_text(hjust = 0.5))\n\n\np0 &lt;- ggplot(df %&gt;% slice(1:100), aes(x,y))+\n  geom_jitter()+\n  geom_line(position=\"jitter\")+\n  my_theme+\n  labs(title=\"MOLECULAR\\nBIOLOGIST\", x=\"\", y=\"\")\n\np1 &lt;- ggplot(df %&gt;% sample_n(1000), aes(x,y))+\n  geom_path(lwd=0.5, alpha=0.8)+\n  my_theme+\n  labs(x=\"\",y=\"\",title=\"BIOLOGIST\")\n\np2 &lt;- ggplot()+\n  ggforce::geom_circle(aes(x0=0,y0=0,r=1), fill=\"gray30\", color=NA)+\n  geom_segment(aes(x=0, xend=0, y=0,yend=-0.5),\n               arrow = arrow(length = unit(0.2, \"cm\"),\n                             type = \"closed\"),\n               arrow.fill = \"black\", size=0.5, color=\"black\")+\n  geom_text(aes(x=0.2,y=-0.5), label=\"mg\", color=\"black\")+\n  my_theme +\n  labs(x=\"\", y=\"\", title=\"PHYSICIST\")\n  \np0 + p1 + p2\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{andina2020,\n  author = {Andina, Matias and Andina, Matias},\n  title = {Complex {Fun}},\n  date = {2020-04-30},\n  url = {https://matiasandina.netlify.app/posts/2020-04-30-complex-fun},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndina, Matias, and Matias Andina. 2020. “Complex Fun.”\nApril 30, 2020. https://matiasandina.netlify.app/posts/2020-04-30-complex-fun."
  },
  {
    "objectID": "posts/2020-06-13-birthday-problem/index.html",
    "href": "posts/2020-06-13-birthday-problem/index.html",
    "title": "Birthday Problem",
    "section": "",
    "text": "We have all been there, classic math riddle:\n\nHow many people need to be in one room so that the probability of two of them having the same birthday is more than 0.5?\n\nIn a recent bootcamp exercise we tackled this in python and I wanted to share, just because it’s fun. I did it for a range of probabilities. My solution is probably not the fastest/shortest/most pythonic, but it’s a little thing I put out there so, if you want to use/improve it, please do!\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef prob(n):\n    numerator = np.math.factorial(365) / np.math.factorial(365-n)\n    denominator = 365 ** n\n    return(1 - numerator/denominator)\n\nprobs = list(map(prob, range(100)))\n\nplt.plot(range(len(probs)), probs)\nplt.show()\n\nA bad translation into R, it would be something like:\n\nprob &lt;- function(n){\n  numerator = exp(lfactorial(365) - lfactorial(365-n))\n    denominator = 365 ** n\n    return(1 - numerator/denominator)\n}\n\nprobs = sapply(0:99, function(n) prob(n))\n\nplot(0:99, probs, type=\"l\",\n     main=\"Probability of 2 people having same birthday\",\n     xlab = \"People in a room\", ylab=\"probability\")\n\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{andina2020,\n  author = {Andina, Matias},\n  title = {Birthday {Problem}},\n  date = {2020-06-13},\n  url = {https://matiasandina.netlify.app/posts/2020-06-13-birthday-problem},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndina, Matias. 2020. “Birthday Problem.” June 13, 2020. https://matiasandina.netlify.app/posts/2020-06-13-birthday-problem."
  },
  {
    "objectID": "posts/2020-06-13-python-keylogger/index.html",
    "href": "posts/2020-06-13-python-keylogger/index.html",
    "title": "Python Keylogger",
    "section": "",
    "text": "I have been struggling a lot with pain in my fingers and wrists1.\nI have tried setting up timers for using the keyboard. But the only thing that seems to work is reducing the typing (aka get away from the computer and stop using the keyboard!).\nTracking hours didn’t work well, so I guess a last resort to force me is counting the actual number of keys per day and trying to keep that to a minimum.\nAfter a little google searching, I found inspiration on this post: Design a Keylogger in Python.\nI wanted something simple. Just count and save to file, in case future me wants to do some sort of analysis. Nothing fancy, no optimization. Most importantly something I can trust is not sending every keystroke I write over the internets.\nYou can read the pynput docs here.\nOf note, pip3 install pynput failed. If only python made it possible for people to install things … 🤷\nAnyway, mystery aside, python -m pip install pynput worked. I kind of trust this library is safe enough (read: blind faith in open-source) . And my passwords are kinda there scrambled somewhere in the text file but I don’t plan to host the file anywhere so it’s reasonably safe (please don’t get remote access to my computer 🙏).\nThe output to console (again, keep it simple) looks like this:\nFor those of you who want to use it, you can find the code below:\nCheck the Code\nfrom pynput.keyboard import Key, Listener\nimport logging\nimport os\nimport datetime\n\n# log_dir defaults to Desktop\nlog_dir = '/home/matias/Desktop'\n# updates every 100 keystrokes\nupdate_every = 1000\nday_count = 0\ntoday = datetime.date.today().isoformat()\n   \ndef on_press(key):\n    logging.info(str(key))\n\ndef key_count(key):\n    global today, day_count, update_every\n    if today == datetime.date.today().isoformat():\n        if day_count % update_every == 0:\n            print(f\"Today is {today} and the key count is: {day_count}\")\n        # always update the counter\n        day_count = day_count + 1\n    else:\n        # update today's value\n        today = datetime.date.today().isoformat()\n        print(\"Today is a brand new day :)\")\n        # reset the counter\n        day_count = 0\n    return\n\ndef main():\n    logging.basicConfig(filename = (os.path.join(log_dir, \"keylog.txt\")),\n     level=logging.DEBUG,\n     format='%(asctime)s: %(message)s')\n    with Listener(\n        on_press=on_press,\n        on_release=key_count) as listener:\n        listener.join()\n\nif __name__ == '__main__':\n    print(\"Starting keylogger\")\n    main()"
  },
  {
    "objectID": "posts/2020-06-13-python-keylogger/index.html#footnotes",
    "href": "posts/2020-06-13-python-keylogger/index.html#footnotes",
    "title": "Python Keylogger",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt’s 2023 and I still struggle with this. Buy a proper keyboard and pay attention to ergonomics. Physical Therapy helps. Get help. Take care of yourself.↩︎"
  },
  {
    "objectID": "posts/2021-01-02-art-in-a-new-year/index.html",
    "href": "posts/2021-01-02-art-in-a-new-year/index.html",
    "title": "Art in a New Year",
    "section": "",
    "text": "I have not made resolutions for this year1. I have instead spent that time playing with patterns on a canvas. It’s been a while since I wanted to have some time to do generative art using geometric figures and I finally got down to it on the very first day of this year. I found a fountain of joy after just a few hours of tinkering, something inside tells me this could be a new New Year’s tradition for me.\nI started out with a simple form like a hexagon and started looking for patterns that were appealing to me, like the color palette.\nSomething that I like about generative art is that I’m not in control, it’s a discovery process. I just set a few rules and pleasant aesthetics appear before me. The patterns might appear by design, but often they are the result of a benign error that created something beautiful. After tinkering with these patterns for a bit, I arrived at these new forms. They take me to a special place.\nI have selected some of the results that I like the most below."
  },
  {
    "objectID": "posts/2021-01-02-art-in-a-new-year/index.html#footnotes",
    "href": "posts/2021-01-02-art-in-a-new-year/index.html#footnotes",
    "title": "Art in a New Year",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWell, at least not in the classical way, mostly just wishing we could get back to normal life sometime before 2022.↩︎"
  },
  {
    "objectID": "posts/2021-01-03-rotating-perspectives/index.html",
    "href": "posts/2021-01-03-rotating-perspectives/index.html",
    "title": "Rotating perspectives",
    "section": "",
    "text": "I have been asked to unveil a bit of what’s under the hood on this post. I decided to make a new post to share how my creative process took place and maybe inspire others to play along.\nSomething interesting about all of this is how well it plays into common sense. Looking at things from a different perspective, in this case adding just a rotation, can yield outstanding results.\n\nThe shapes\nFirst of all, we are going to use the shapes provided by ggforce::geom_regon().\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggforce)\n\ndf &lt;- data.frame(\n  x0=3:8,\n  y0=1,\n  r=0.2\n)\n\nggplot(df)+\n  geom_regon(aes(x0 = x0, y0 = y0,\n                 r = r, sides= x0, angle = 0),\n             fill=\"gray50\", color=\"black\")+\n  coord_equal()+\n  labs(title=\"Regular polygons using ggforce\", \n       x=\"\", y=\"\",\n       caption=\"@NeuroMLA\")+\n  theme(panel.background = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        plot.title = element_text(hjust=0.5))\n\n\n\n\n\n\n\nThe twist\nNow we can implement a rotation to each figure. We will use the previous df and expand_grid() to add an angle rotation to the regular polygon. The greater the number of sides, the closer we get to a circular shape illusion when we rotate and overlap the polygons. For n&gt;6 it didn’t generate the type of look I was looking after during my experimentation.\n\n\nCode\ndf &lt;- expand_grid(df, angle = seq(0, 0.5, 0.1))\n\n \nggplot(df)+\n  # notice we use angle = angle now\n  geom_regon(aes(x0=x0,y0=y0,r=r, sides=x0, angle=angle),\n             fill=\"gray50\", color=\"black\")+\n  coord_equal()+\n  labs(title=\"Rotated regular polygons using ggforce\", \n       x=\"\", y=\"\",\n       caption=\"@NeuroMLA\")+\n  theme(panel.background = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(), \n        plot.title = element_text(hjust=0.5))\n\n\n\n\n\nWe can tinker with the alpha and fill to make some nice looking shapes. I’m not going to modify color but it’s also a possibility.\n\n\nCode\nggplot(df)+\n  # notice we use angle = angle now and fil=factor(x0)\n  geom_regon(aes(x0=x0,y0=y0,r=r, sides=x0, angle=angle,\n                 fill=factor(x0)), \n             alpha=0.1, color=\"black\")+\n  coord_equal()+\n  labs(title=\"Rotated regular polygons using ggforce\", \n       x=\"\", y=\"\",\n       caption=\"@NeuroMLA\")+\n  theme(panel.background = element_blank(),\n        legend.position = \"none\",\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        plot.title = element_text(hjust=0.5)) -&gt; p\n\nprint(p)\n\n\n\n\n\n\n\nMake it pallette\nWe can have unlimited color combinations. Just as a start, two places I like to use when dealing with color pallettes in R:\n\nR Color Brewer Pallettes\nAdobe Color Picker\n\nWe will use scale_fill_*() functions of ggplot. I normally use scale_fill_manual() if I want to handpick the values, but scale_fill_viridis() and scale_fill_brewer() often do a nice job too!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{andina2021,\n  author = {Andina, Matias},\n  title = {Rotating Perspectives},\n  date = {2021-01-03},\n  url = {https://matiasandina.netlify.app/posts/2021-01-03-rotating-perspectives},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndina, Matias. 2021. “Rotating Perspectives.” January 3,\n2021. https://matiasandina.netlify.app/posts/2021-01-03-rotating-perspectives."
  },
  {
    "objectID": "posts/2021-02-07-queens-gambit/index.html",
    "href": "posts/2021-02-07-queens-gambit/index.html",
    "title": "Queen’s Gambit",
    "section": "",
    "text": "I love games. I enjoy the fun in many formats: cards, boards, consoles. Yet, over the years, chess has consolidated as the game I play most often, and it is likely the only game I both really like and can limit the amount I play1.\nIt’s been a couple of months since I noticed something weird going on in my games, a sort of disturbance in the force. I felt I was playing a type of position called Queen’s Gambit way more often than normal.\nBecause there’s currently a sort of chess mania fueled by the very successful new series that gets the name from the gambit2, I thought it would be pretty easy to find out whether the data reflected my gut feeling."
  },
  {
    "objectID": "posts/2021-02-07-queens-gambit/index.html#too-long-didnt-read",
    "href": "posts/2021-02-07-queens-gambit/index.html#too-long-didnt-read",
    "title": "Queen’s Gambit",
    "section": "Too Long — Didn’t Read",
    "text": "Too Long — Didn’t Read\nI apologize for writing long posts. Be my guest and jump directly to the popularity of the Queen’s Gambit across time by clicking here"
  },
  {
    "objectID": "posts/2021-02-07-queens-gambit/index.html#the-data",
    "href": "posts/2021-02-07-queens-gambit/index.html#the-data",
    "title": "Queen’s Gambit",
    "section": "The data",
    "text": "The data\nTo do any sort of analysis, I needed data. Although there are probably many places where I could have found it, www.chess.com has a large dataset and it is somewhat accessible (if you can parse websites, see how). It is also the place where I play, so the dataset of my games comes from there.\nThroughout this post, I will be using two datasets:\n\nMaster Games: games played by Chess Masters where Queen’s Gambit is played.\nMy games: All my games at chess.com, spanning from 2012 to January 2021.\n\nLet’s minimally explore the dataset of Master Games for the sake of DataViz 📈.\nI first started looking at a dataset with 37287 entries, from games played between chess Masters (aka Master Games) that open with Queen’s Gambit. Games were played between 2000 and 2020. After a bit of data cleaning, there were a few quick things I wanted to look at.\nBelow, you can find a violin plot of the number of moves a chess match between masters normally has.\n\n\n\n\n\nIf you are curious, here’s a table with summary statistics for the number of moves in the games by result.\n\n\n\n\n\n\n  \n    \n    \n      Result\n      min\n      q25\n      median\n      mean\n      q75\n      max\n    \n  \n  \n    white wins\n5\n31\n40\n41.85\n50\n161\n    draw\n2\n21\n34\n36.84\n49\n204\n    black wins\n4\n33\n41\n43.82\n53\n183"
  },
  {
    "objectID": "posts/2021-02-07-queens-gambit/index.html#what-is-this-queens-gambit-anyway",
    "href": "posts/2021-02-07-queens-gambit/index.html#what-is-this-queens-gambit-anyway",
    "title": "Queen’s Gambit",
    "section": "What is this “Queen’s Gambit”, anyway?",
    "text": "What is this “Queen’s Gambit”, anyway?\nGenerally speaking, a gambit is a position where both players stand to exchange pieces. In the particular case of the Queen’s Gambit, we are most often referring to this position (although see further below)."
  },
  {
    "objectID": "posts/2021-02-07-queens-gambit/index.html#when-does-it-happen",
    "href": "posts/2021-02-07-queens-gambit/index.html#when-does-it-happen",
    "title": "Queen’s Gambit",
    "section": "When does it happen?",
    "text": "When does it happen?\nQueen’s gambit is an opening, this means it occurs early in the game. In this case, the gambit is overwhelmingly proposed on the second move and just a few games develop into a position that is equivalent to the Queen’s gambit later in the game."
  },
  {
    "objectID": "posts/2021-02-07-queens-gambit/index.html#who-benefits",
    "href": "posts/2021-02-07-queens-gambit/index.html#who-benefits",
    "title": "Queen’s Gambit",
    "section": "Who benefits?",
    "text": "Who benefits?\nIn chess, whoever starts (aka plays white) has advantage. However, Chess Masters are heavily trained in defense, so it’s not a walk in the part for white. It always depends heavily on the initial positions (openings), gambit proposals, and responses (accepted or declined with many variations). In the case of the Queen’s Gambit, the machines predict white is favorite to win, while accepting the gambit increases the likelihood of draws. Interestingly, if we calculate the expected value for black from the observed frequencies, it might be slightly better for black to accept the gambit3.\n\n\n\n\n\nI wouldn’t treat this tiny increment as something real. It is likely just noise. In reality, Chess Masters decline the Queen’s gambit way more often than they accept it (about 2.6 times more often!).\nIn fact, if we check with the chess engines, they seldom accept the Queen’s Gambit either. On a personal, totally amateur note, I don’t feel like accepting the Queen’s Gambit, it just doesn’t feel right."
  },
  {
    "objectID": "posts/2021-02-07-queens-gambit/index.html#where-do-they-go",
    "href": "posts/2021-02-07-queens-gambit/index.html#where-do-they-go",
    "title": "Queen’s Gambit",
    "section": "Where do they go?",
    "text": "Where do they go?\nAfter the opening, chess games branch into a myriad possible positions. I believe a cool way to see this is to use alluvial plots to visualize how the games evolve in time. It would really be a mess to visualize all moves, so I just focused on certain games and kept it short around proposal/response in regards to Queen’s Gambit. I added the result of the game, just out of curiosity.\n\n\n\n\n\nAs you can see, I focused on the two cases where white moves their pawn to c4 (c4) to actually “propose” the gambit. This is how both situations look on the board.\n\n\n\n\n\n\n\n1. d4 d5 2. c4\n1. d4 Nf3 2. c4\n\n\n\n\n\n\n\n\n\n“Accepting” the gambit means taking the pawn at c4 (dxc4). As we said before, this is the least frequent case, and it is interesting to see that almost none of the Nf6 games go to accept the gambit. It’s also quite telling that the vast majority of Masters respond to Queen’s Gambit with e6. This is how it looks in the board.\n\n\n\n\n\n\n\n1. d4 d5 2. c4 e6\n1. d4 Nf6 2. c4 e6\n\n\n\n\n\n\n\n\n\n\nSpecial case\nThere’s a special case where games take a bit longer to reach the gambit. These are the games where the second move is Nf3 Nf6. I made the same alluvial plot for these games. For some reason, this position prompts the majority of Chess Masters to accept the gambit.\nThus, if you like to play the gambit as white, and you like the other side to accept it, you should delay your c4 advance just one move.\n\n\n\n\n\nThis is how your board should look like if you want black to accept the gambit."
  },
  {
    "objectID": "posts/2021-02-07-queens-gambit/index.html#popularity",
    "href": "posts/2021-02-07-queens-gambit/index.html#popularity",
    "title": "Queen’s Gambit",
    "section": "Popularity",
    "text": "Popularity\nEnough with the chess lingo, show me what I came here for! Alright, alright, don’t bark. Here’s the popularity plot on the Masters Games.\n\n\n\n\n\nThis plot shows that the popularity of the gambit increased dramatically. But my prediction was wrong, the gambit was super popular a few years ago and I didn’t notice anything.\nWhat happened between 2017 and 2019? I don’t really know if the total amount of games just skyrocketed, but it’s possible. Maybe the 2018 world championship had something to do with it. I didn’t parse the full database, so I can’t tell you about the frequency of all games.\nWhat happened in 2020? If you are from the future, I would like to remind you that there was a Global Pandemic on 2020, stay safe whenever you are.\nThe popularity graph didn’t show what I was expecting to see, but I still trust my gut feeling. It all started just a few months ago, I’m pretty sure that there is something in the data that needs to be unmasked.\nData from 2020 will not reflect my gut feeling easily. First, the pandemic messed up with the tournament schedule. Second, the TV series might be super popular with normal people, but we shouldn’t expect Chess Masters to change the way they open games because of it.\nSo…what about simple mortals like you and me? Well, my friend, if you made it this far, I trust you to go on with my game database."
  },
  {
    "objectID": "posts/2021-02-07-queens-gambit/index.html#my-games",
    "href": "posts/2021-02-07-queens-gambit/index.html#my-games",
    "title": "Queen’s Gambit",
    "section": "My games",
    "text": "My games\nFirst, you should know that I mostly play blitz games (either 3 minutes in the clock at start or 3 minutes with 2 seconds added per move). Nonetheless, it looks like the move distribution of games is quite similar to what we see in Masters games.\n\n\n\n\n\nThere are two places where I see differences. First, Master Games show a bump quite early, even below 20 moves. This is unusual because it takes more moves to break a good defense. We will come back later to this.\n\n\n\n\n\nSecond, there is another bump on longer games. I attribute this to the nature of my skill level. I can’t play much more than an opening and a few attempts on the “middle game”. Most of my games will evolve pretty quickly (either because the clock forces you to be aggressive or because our chess level is not good). It’s often the case that one player makes a crucial mistake and the game evolves fast after that.\n\n\n\n\n\nThere’s another explanation for these differences. If we separate games according to result. We see that Masters games have almost the same shape as mine but, when they get to a draw, they get to it really fast. For us novices, getting a draw is often an accident. Masters deliberately draw, and they do it early.\n\n\n\n\n\n\nDrawing is a signal of quality\nAt my level, drawing a game is not common. For pros, it’s quite the opposite. Something else that draws my attention is that black is really unlikely to win a game. This is similar to other games, like tennis, where the player that serves (aka, starts the game) has a huge advantage.\n\n\n\n\n\n\n\nMy openings\nIn my database, I have all the games I played. Up to February 2021, it was a total of 15130. Yes, I know, a lot, roughly 4.61 per day since 2012. Anyway, I can get all the openings from my games.\nI selected the ten most frequent first moves when I am playing white and when I am playing black. You can see that d4 d5 doesn’t appear on the top ten at all when playing white. The only times I face the d4 d5 opening (which might lead to Queen’s Gambit) is when I play black.\n\n\n\n\n\n\n\nHow often do I play against the gambit?\nThe amount I play has not been even over the years, but the frequency of games where I play the gambit seems somewhat marginal and independent of the how much I play.\n\n\n\n\n\nMarginal proportions are hard to see on a frequency graph, so maybe this graph is a bit more evident. I have played the Queen’s Gambit in roughly 5.0% of the games. The game sample from 2021 might not be big enough to be representative (only January is present).\n\n\n\n\n\nThe small increases in late years are not big enough for me to notice, especially when spread evenly across the year. But what happens if we only look at 2020? The TV series started in October. In November, we see a big jump in the Queen’s Gambit’s frequency!\n\n\n\n\n\nThis is increase is not big. In fact, during November I also played the Scandinavian Defense4 more often. But even so, the Queen’s Gambit got a ~3X jump on November, something that is way more noticeable when going from ~2.5% to ~ 7.5% than a 5-10% increase in an opening that I already play pretty often."
  },
  {
    "objectID": "posts/2021-02-07-queens-gambit/index.html#wrap",
    "href": "posts/2021-02-07-queens-gambit/index.html#wrap",
    "title": "Queen’s Gambit",
    "section": "Wrap",
    "text": "Wrap\nThis project started with just an intuition and went down the rabbit hole of exploration. It took quite a long time to write (and re-write), but I was a lot of fun to mess around with. I hope you have enjoyed it if you made it this far!"
  },
  {
    "objectID": "posts/2021-02-07-queens-gambit/index.html#more-readings",
    "href": "posts/2021-02-07-queens-gambit/index.html#more-readings",
    "title": "Queen’s Gambit",
    "section": "More readings",
    "text": "More readings\nIf you want to read a nice, data rich, post about the “Chess Boom” that the series brought about, check https://www.bloomberg.com/graphics/2020-chess-boom/.\nEven if you ask, Chess.com does not give you all the games in your database nicely (probably trying to limit the bandwidth). So I slightly adapted the following script https://github.com/arnsholt/chesscom-games for the purpose of “Gimme all my games with almost no clicks involved!”. It uses the API, so I think it’s all cool :).\nI should have probably searched for somebody’s package. Here are some references that might be useful:\n\nhttps://github.com/JaseZiv/chessR\nhttps://github.com/curso-r/chess"
  },
  {
    "objectID": "posts/2021-02-07-queens-gambit/index.html#footnotes",
    "href": "posts/2021-02-07-queens-gambit/index.html#footnotes",
    "title": "Queen’s Gambit",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHowever, after reading this post you might believe I’m a bit of a chess junkie 😝↩︎\nBut mostly because the star is a kick-ass woman.↩︎\nI’m using the observed frequencies and calculating one point per win and 0.5 points per draw. This gives the expected values for each player.↩︎\nScandinavian Defense is a position characterized by 1.e4 d5, it’s by far my prefered opening when playing black.↩︎"
  },
  {
    "objectID": "posts/2022-10-10-Inequality/index.html",
    "href": "posts/2022-10-10-Inequality/index.html",
    "title": "Inequality",
    "section": "",
    "text": "I love Future Crunch. The idea of receiving positive news that I wouldn’t otherwise get is fantastic and I appreciate the work they do. However, during one of the latest editions I got a pyramid plot coming from Credit Suisse which was really hard to read. Moreover, the people who made the plot were trying to make the case that the world is a better place, because there is less inequality.\nYou are welcome to try to interpret the plot on its source, here’s the link again. Because I think the representation is lacking, I do want to produce my own version using their data (aka, the values provided in their plot). Here’s a basic table for their data:\nBracket\n      Bracket2\n      Year\n      People (M)\n    \n  \n  \n    &lt;10K\n&lt;10K\n2011\n3054.0\n    &lt;10K\n&lt;10K\n2021\n2818.0\n    10K-100K\n10K+\n2011\n1066.0\n    10K-100K\n10K+\n2021\n1791.0\n    100K-1M\n10K+\n2011\n369.0\n    100K-1M\n10K+\n2021\n627.0\n    1M+\n10K+\n2011\n29.7\n    1M+\n10K+\n2021\n62.5"
  },
  {
    "objectID": "posts/2022-10-10-Inequality/index.html#why-i-dont-like-the-pyramid-plot",
    "href": "posts/2022-10-10-Inequality/index.html#why-i-dont-like-the-pyramid-plot",
    "title": "Inequality",
    "section": "Why I don’t like the pyramid plot",
    "text": "Why I don’t like the pyramid plot\n\nPyramids have angles, I strive to stay away from angles.\nThe x axis is reversed.\nAll quantities are changing but there are horizontal levels trying to guide us. I think they do more harm than good.\n\nThese horizontal levels play the role of a y axis, only to add confusion because the scale is logarithmic."
  },
  {
    "objectID": "posts/2022-10-10-Inequality/index.html#enter-waffle",
    "href": "posts/2022-10-10-Inequality/index.html#enter-waffle",
    "title": "Inequality",
    "section": "Enter Waffle",
    "text": "Enter Waffle\nI think Waffle plots are a fantastic alternative to all the nasty pie and pyramid plots. The representation of fractions is clear and explicit, and there are no angles that mess things up. I kept their colors to make it as similar as possible. Note that the world’s population has increased, so 1 square (or 1% of the population) is different on the 2011 and 2021 plots."
  },
  {
    "objectID": "posts/2022-10-10-Inequality/index.html#the-case",
    "href": "posts/2022-10-10-Inequality/index.html#the-case",
    "title": "Inequality",
    "section": "The case",
    "text": "The case\nThe case that people are trying to make is: “Good News! There’s more people in the wealthier categories!”. For sure, we can say that these numbers indeed point towards the right direction. Is it fast enough? That depends on how much expectations you had for the global change we can achieve in one decade. If anything, things are better, but we have much room for improvement.\nIn the following plot, I tried to focus attention on the bottom bracket. If we manage to get people out of the bottom bracket, that would be a huge triumph for global development.\n\nWe are making progress. Progress is slow, but the share of people living in the bottom bracket is decreasing. However, looking at these plots, I cannot help but notice the immense task we have ahead: we still have to lift half of the planet out of this bracket. This challenge is something I wanted to convey, and I think it’s more evident in these waffle than the pyramid plots. I think Hans, put it better than anyone:\n\nThings can be bad, and getting better. – Hans Rosling"
  },
  {
    "objectID": "posts/2022-10-10-Inequality/index.html#the-downside",
    "href": "posts/2022-10-10-Inequality/index.html#the-downside",
    "title": "Inequality",
    "section": "The downside",
    "text": "The downside\nThere are downsides of using Waffle Plots. I’m quoting from a nice person who gave me feedback directly, they said: “One disadvantage of the waffle plot is a lack of precision. For instance, your version doesn’t show the change at the top from .5% to 1.2% (more than double!)”.\nI was somewhat saving this idea for another plot. What idea? The idea that all wealthier brackets increased by a lot (~2X!). I decided to go with a bar plot showing the absolute number of people. This is because I want to keep numbers in terms of people, real human beings that belong to each bracket. Each person that we move is a lot, and we have a lot to do!\n\n\n\n\n\nThis is not the only issue with waffle plots. As I said before, the wealth brackets here are logarithmic, and the highest bracket is virtually infinite. Counting methods (aka, counting how many people belong to each bracket), can go far to give a sense of the overall distribution, but cannot bring a picture of the massive differences in wealth between the higher and the lower brackets when the axis is discretized to condense a logarithmic scale. Looking further the actual wealth range exceeds my intentions for this quick makeover, but I couldn’t help to mention it."
  }
]