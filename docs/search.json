[
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Homepage: matiasandina.netlify.com\nAcademic Research: Google Scholar"
  },
  {
    "objectID": "projects/index.html#primary-sites",
    "href": "projects/index.html#primary-sites",
    "title": "Projects",
    "section": "",
    "text": "Homepage: matiasandina.netlify.com\nAcademic Research: Google Scholar"
  },
  {
    "objectID": "projects/index.html#software-development",
    "href": "projects/index.html#software-development",
    "title": "Projects",
    "section": "Software Development",
    "text": "Software Development\n\nR\n\nggethos:\n\ndocs: matiasandina.github.io/ggethos/\n\nfed3:\n\nrepo: matiasandina.github.io/fed3/\n\nnobrainr:\n\nrepo: github.com/matiasandina/nobrainr\n\nchoices:\n\nrepo: github.com/matiasandina/choices\n\nstepfinder\n\nrepo: github.com/matiasandina/stepfinder\n\n\n\n\nPython\n\nsend_ip:\n\nrepo: https://github.com/matiasandina/send_ip\npypi: https://pypi.org/project/send-ip/\n\nrlist_files:\n\ndocs: https://rlist-files.readthedocs.io\npypi: https://pypi.org/project/rlist-files/\n\nphototdt:\n\ndocs: https://phototdt.readthedocs.io\nrepo: github.com/matiasandina/phototdt\n\nFEDWatcher:\n\nrepo: github.com/matiasandina/FEDWatcher"
  },
  {
    "objectID": "projects/index.html#data-visualization",
    "href": "projects/index.html#data-visualization",
    "title": "Projects",
    "section": "Data Visualization",
    "text": "Data Visualization\n\nLife Viz:\n\nlive: matias-andina.shinyapps.io/lifeviz\nrepo: https://github.com/matiasandina/lifeviz"
  },
  {
    "objectID": "projects/index.html#writing",
    "href": "projects/index.html#writing",
    "title": "Projects",
    "section": "Writing",
    "text": "Writing\n\nmatiasandina.netlify.com/writing\nmatiasandina.netlify.com/escritos"
  },
  {
    "objectID": "research/VibrationalSpectroscopy/index.html",
    "href": "research/VibrationalSpectroscopy/index.html",
    "title": "(Non Linear) Vibrational Spectroscopy",
    "section": "",
    "text": "Using light to reveal the mysteries of molecular motion."
  },
  {
    "objectID": "research/VibrationalSpectroscopy/index.html#related-publications",
    "href": "research/VibrationalSpectroscopy/index.html#related-publications",
    "title": "(Non Linear) Vibrational Spectroscopy",
    "section": "Related Publications",
    "text": "Related Publications\n\n\n    \n      \n      \n    \n\n\n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            Surface stratification determines the interfacial water structure of simple electrolyte solutions\n            \n            \n                \n                \n                    Yair Litman, Kuo-Yang Chiang, Takakazu Seki, Yuki Nagata, Mischa Bonn \n                \n                \n                \n                    \n                        Nature Chemistry, 16, 644-650 (2024)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n\n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n  \n              \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            A Hybrid-DFT Study of Intrinsic Point Defects in MX2 (M=Mo, W; X=S, Se) Monolayers\n            \n            \n                \n                \n                    Alaa Akkoush, Yair Litman, Mariana Rossi \n                \n                \n                \n                    \n                        Phys. Status Solidi A, 221, 2300180 (2024)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n\n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n  \n              \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            Fully First-Principles Surface Spectroscopy with Machine Learning\n            \n            \n                \n                \n                    Yair Litman, Jinggang Lan, Yuki Nagata, David M. Wilkins \n                \n                \n                \n                    \n                        J. Phys. Chem. Lett, 14, 8175-8182 (2023)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n\n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n  \n              \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            First-Principles Simulations of Tip Enhanced Raman Scattering Reveal Active Role of Substrate on High-Resolution Images\n            \n            \n                \n                \n                    Yair Litman, Franco P. Bonafé, Alaa Akkoush, Heiko Appel, Mariana Rossi \n                \n                \n                \n                    \n                        J. Phys. Chem. Lett., 14, 6850-6859 (2023)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n\n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n  \n              \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            Is Unified Understanding of Vibrational Coupling of Water Possible? Hyper-Raman  Measurement and Machine Learning Spectra\n            \n            \n                \n                \n                    Kazuki Inoue, Yair Litman, David M. Wilkins, Yuki Nagata, Masanari Okuno \n                \n                \n                \n                    \n                        J. Phys. Chem. Lett., 14, 3063-3068 (2023)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n\n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n  \n              \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            Charge Transfer-Mediated Dramatic Enhancement of Raman Scattering upon Molecular Point Contact Formation\n            \n            \n                \n                \n                    Borja Cirera, Yair Litman, Chenfang Lin, Alaa Akkoush, Adnan Hammud, Martin Wolf, Mariana Rossi, Takashi Kumagai \n                \n                \n                \n                    \n                        Nano Lett.., 22, 2170-2176 (2022)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n\n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n  \n              \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            Temperature dependence of the vibrational spectrum of porphycene: a qualitative  failure of classical-nuclei molecular dynamics\n            \n            \n                \n                \n                    Yair Litman, Jörg Behler, Mariana Rossi \n                \n                \n                \n                    \n                        Faraday Discuss., 221, 526-546 (2020)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n\n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n  \n              \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            i-PI 2.0: A universal force engine for advanced molecular simulations\n            \n            \n                \n                \n                    Venkat Kapil,  Mariana Rossi,  Ondrej Marsalek,  Riccardo Petraglia,  Yair Litman,  Thomas Spura,  Bingqing Cheng,  Alice Cuzzocrea,  Robert H. Meißner,  David M. Wilkins,  Benjamin A. Helfrecht, Przemysław Juda, Sébastien P. Bienvenue, Wei Fang, Jan Kessler, Igor Poltavsky, Steven Vandenbrande, Jelle Wieme, Clemence Corminboeuf, Thomas D. Kühne, David E. Manolopoulos, Thomas E. Markland, Jeremy O. Richardson, Alexandre Tkatchenko, Gareth A. Tribello, Veronique Van Speybroeck, Michele Ceriotti \n                \n                \n                \n                    \n                        Comput. Phys. Commun., 236, 214-223 (2019)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n\n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n  \n              \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            Elucidating the Nuclear Quantum Dynamics of Intramolecular Double Hydrogen Transfer in Porphycene\n            \n            \n                \n                \n                    Yair Litman, Jeremy O. Richardson, Takashi Kumagai, Mariana Rossi \n                \n                \n                \n                    \n                        J. Am. Chem. Soc., 141, 2526-2534 (2019)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n\n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n  \n              \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            Positional Isotope Exchange in HX·(H2O)n (X = F, I) Clusters at Low Temperatures\n            \n            \n                \n                \n                    Yair  Litman, Pablo E. Videla, Javier Rodriguez, Daniel Laria \n                \n                \n                \n                    \n                        J. Phys. Chem. A., 120, 7213-7224 (2016)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n\n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n  \n              \n\n        \n    \n    \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Research Areas",
    "section": "",
    "text": "Yair’s research aims to accelerate the development of current and future energy-related technologies by providing atomistic-level understanding of key processes ocurring at liquid-solid interfaces.\nHis primary system of interest include organic/inorganic, aqueous, and electrified interfaces. To tackle these challenges, Yair utilizes advanced first principles methods and employs machine learning techniques to achieve the accuracy and model complexity required by scientific question. Yair has developed theoretical methods to calculate (quantum) reaction rates and various types of spectroscopies, such as, tip-enhanced Raman, sum-frequency generation, among others. He enjoys collaborating with experimentalists and theoreticians alike.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Non Linear) Vibrational Spectroscopy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAqueous Interfaces\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOrganic/Inorganic Interfaces\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuantum Dynamics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/index.html",
    "href": "publications/index.html",
    "title": "Publications",
    "section": "",
    "text": "i-PI 3.0: a flexible, efficient framework for advanced atomistic simulations\n            \n            \n                \n                \n                    Yair Litman, Venkat Kapil, Yotam MY Feldman, Davide Tisi, Tomislav Begušić, Karen Fidanyan, Guillaume Fraux, Jacob Higer, Matthias Kellner, Tao E Li, Eszter S Pós, Elia Stocco, George Trenins, Barak Hirshberg, Mariana Rossi, Michele Ceriotti \n                \n                \n                \n                    \n                        arXiv, 2405.15224,   (2024)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n            \n                \n                    28 \n                \n\n            \n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n\n            \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            Quantum rates in dissipative systems with spatially varying friction\n            \n            \n                \n                \n                    Oliver Bridge, Paolo Lazzaroni, Rocco Martinazzo, Mariana Rossi, Stuart C. Althorpe, Yair Litman \n                \n                \n                \n                    \n                        J. Chem. Phys., (accepted) ,   (2024)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n            \n                \n                    27 \n                \n\n            \n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n\n            \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            Learning Electronic Polarizations in Aqueous Systems\n            \n            \n                \n                \n                    Arnab Jana, Sam Shepherd, Yair Litman, David M. Wilkins \n                \n                \n                \n                    \n                        J. Chem. Inf. Model., 64, 4426-4435 (2024)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n            \n                \n                    26 \n                \n\n            \n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n\n            \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            Surface stratification determines the interfacial water structure of simple electrolyte solutions\n            \n            \n                \n                \n                    Yair Litman, Kuo-Yang Chiang, Takakazu Seki, Yuki Nagata, Mischa Bonn \n                \n                \n                \n                    \n                        Nature Chemistry, 16, 644-650 (2024)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n            \n                \n                    25 \n                \n\n            \n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n\n            \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            A Hybrid-DFT Study of Intrinsic Point Defects in MX2 (M=Mo, W; X=S, Se) Monolayers\n            \n            \n                \n                \n                    Alaa Akkoush, Yair Litman, Mariana Rossi \n                \n                \n                \n                    \n                        Phys. Status Solidi A, 221, 2300180 (2024)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n            \n                \n                    24 \n                \n\n            \n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n\n            \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            Fully First-Principles Surface Spectroscopy with Machine Learning\n            \n            \n                \n                \n                    Yair Litman, Jinggang Lan, Yuki Nagata, David M. Wilkins \n                \n                \n                \n                    \n                        J. Phys. Chem. Lett, 14, 8175-8182 (2023)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n            \n                \n                    23 \n                \n\n            \n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n\n            \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            First-Principles Simulations of Tip Enhanced Raman Scattering Reveal Active Role of Substrate on High-Resolution Images\n            \n            \n                \n                \n                    Yair Litman, Franco P. Bonafé, Alaa Akkoush, Heiko Appel, Mariana Rossi \n                \n                \n                \n                    \n                        J. Phys. Chem. Lett., 14, 6850-6859 (2023)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n            \n                \n                    22 \n                \n\n            \n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n\n            \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            Is Unified Understanding of Vibrational Coupling of Water Possible? Hyper-Raman  Measurement and Machine Learning Spectra\n            \n            \n                \n                \n                    Kazuki Inoue, Yair Litman, David M. Wilkins, Yuki Nagata, Masanari Okuno \n                \n                \n                \n                    \n                        J. Phys. Chem. Lett., 14, 3063-3068 (2023)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n            \n                \n                    20 \n                \n\n            \n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n\n            \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            Dissipative tunneling rates through the incorporation of first-principles electronic  friction in instanton rate theory. II. Benchmarks and applications\n            \n            \n                \n                \n                    Yair Litman, Eszter S. Pós, Connor L. Box, Rocco Martinazzo, Reinhard J. Maurer, Mariana Rossi \n                \n                \n                \n                    \n                        J. Chem. Phys., 156, 194107 (2022)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n            \n                \n                    19 \n                \n\n            \n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n\n            \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            Dissipative tunneling rates through the incorporation of first-principles electronic  friction in instanton rate theory. I. Theory\n            \n            \n                \n                \n                    Yair Litman, Eszter S. Pós, Connor L. Box, Rocco Martinazzo, Reinhard J. Maurer, Mariana Rossi \n                \n                \n                \n                    \n                        J. Chem. Phys. , 156, 194106 (2022)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n            \n                \n                    18 \n                \n\n            \n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n\n            \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            Charge Transfer-Mediated Dramatic Enhancement of Raman Scattering upon Molecular Point Contact Formation\n            \n            \n                \n                \n                    Borja Cirera, Yair Litman, Chenfang Lin, Alaa Akkoush, Adnan Hammud, Martin Wolf, Mariana Rossi, Takashi Kumagai \n                \n                \n                \n                    \n                        Nano Lett.., 22, 2170-2176 (2022)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n            \n                \n                    17 \n                \n\n            \n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n\n            \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            Multidimensional Hydrogen Tunneling in Supported Molecular Switches: The Role   of Surface Interactions\n            \n            \n                \n                \n                    Yair Litman, Mariana Rossi \n                \n                \n                \n                    \n                        Phys. Rev.  Lett., 125, 216001 (2020)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n            \n                \n                    16 \n                \n\n            \n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n\n            \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n\n                \n      \n\n            \n        \n        \n            \n            Tunneling and Zero-Point Energy Effects in Multidimensional Hydrogen Transfer Reactions: From Gas Phase to Adsorption on Metal Surfaces\n            \n            \n                \n                \n                    Yair Litman \n                \n                \n                \n                    \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n            \n                \n                    15 \n                \n\n            \n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n\n            \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n\n                \n      \n\n            \n        \n        \n            \n            Zero-point energy and tunnelling: general discussion\n            \n            \n                \n                \n                    Stuart C. Althorpe, Antonios M. Alvertis, William Barford, Raz L. Benson, Irene Burghardt, Samuele Giannini, Scott Habershon, Sharon Hammes-Schiffer, Sam Hay, Srinivasan Iyengar, Aaron Kelly, Ksenia Komarova, Joseph Lawrence, Yair Litman, Craig Martens, Reinhard J. Maurer, Dave Plant, Mariana Rossi, Ken Sakaushi, Addison Schile, Simone Sturniolo, David P. Tew, George Trenins, Graham Worth \n                \n                \n                \n                    \n                        Faraday Discuss., 221, 478-500 (2020)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n            \n                \n                    14 \n                \n\n            \n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n\n            \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n\n                \n      \n\n            \n        \n        \n            \n            Quantum coherence in complex environments: general discussion\n            \n            \n                \n                \n                    Antonios M. Alvertis, William Barford, Susannah Bourne Worster, Irene Burghardt, Animesh Datta, Arend Dijkstra, Thomas Fay, Soumya Ghosh, Tobias Grünbaum, Scott Habershon, P. J. Hore, David Hutchinson, Srinivasan Iyengar, Alex R. Jones, Garth Jones, Ksenia Komarova, Joseph Lawrence, Jérémie Léonard, Yair Litman, Jonathan Mannouch, David Manolopoulos, Craig Martens, Manel Mondelo-Martell, David Picconi, Dave Plant, Ken Sakaushi, Maximilian A. C. Saller, Addison Schile, Gregory D. Scholes, Javier Segarra-Martí, Francesco Segatta, Alessandro Troisi, Graham Worth \n                \n                \n                \n                    \n                        Faraday Discuss., 221, 168-201 (2020)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n            \n                \n                    13 \n                \n\n            \n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n\n            \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n\n                \n      \n\n            \n        \n        \n            \n            Emerging opportunities and future directions: general discussion\n            \n            \n                \n                \n                    Stuart C. Althorpe, William Barford, Jochen Blumberger, Callum Bungey, Irene Burghardt, Animesh Datta, Soumya Ghosh, Samuele Giannini, Tobias Grünbaum, Scott Habershon, Sharon Hammes-Schiffer, Sam Hay, Srinivasan Iyengar, Garth Jones, Aaron Kelly, Ksenia Komarova, Joseph Lawrence, Yair Litman, Jonathan Mannouch, David Manolopoulos, Craig Martens, Reinhard J. Maurer, Marko Melander, Mariana Rossi, Ken Sakaushi, Maximilian Saller, Addison Schile, Simone Sturniolo, George Trenins, Graham Worth \n                \n                \n                \n                    \n                        Faraday Discuss., 221, 564-581 (2020)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n            \n                \n                    12 \n                \n\n            \n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n\n            \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n\n                \n      \n\n            \n        \n        \n            \n            Spectroscopic signatures of quantum effects: general discussion\n            \n            \n                \n                \n                    Antonios M. Alvertis, William Barford, Susannah Bourne Worster, Irene Burghardt, Alex Chin, Animesh Datta, Arend Dijkstra, Thomas Fay, Helen Fielding, Tobias Grünbaum, Scott Habershon, Sharon Hammes-Schiffer, Srinivasan Iyengar, Alex R. Jones, Ksenia Komarova, Jérémie Léonard, Yair Litman, David Picconi, Dave Plant, Addison Schile, Gregory D. Scholes, Javier Segarra-Martí, Francesco Segata, Alessandro Trossi, Graham Worth \n                \n                \n                \n                    \n                        Faraday. Discuss., 221, 322-349 (2020)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n            \n                \n                    11 \n                \n\n            \n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n\n            \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            Temperature dependence of the vibrational spectrum of porphycene: a qualitative  failure of classical-nuclei molecular dynamics\n            \n            \n                \n                \n                    Yair Litman, Jörg Behler, Mariana Rossi \n                \n                \n                \n                    \n                        Faraday Discuss., 221, 526-546 (2020)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n            \n                \n                    10 \n                \n\n            \n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n\n            \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            i-PI 2.0: A universal force engine for advanced molecular simulations\n            \n            \n                \n                \n                    Venkat Kapil,  Mariana Rossi,  Ondrej Marsalek,  Riccardo Petraglia,  Yair Litman,  Thomas Spura,  Bingqing Cheng,  Alice Cuzzocrea,  Robert H. Meißner,  David M. Wilkins,  Benjamin A. Helfrecht, Przemysław Juda, Sébastien P. Bienvenue, Wei Fang, Jan Kessler, Igor Poltavsky, Steven Vandenbrande, Jelle Wieme, Clemence Corminboeuf, Thomas D. Kühne, David E. Manolopoulos, Thomas E. Markland, Jeremy O. Richardson, Alexandre Tkatchenko, Gareth A. Tribello, Veronique Van Speybroeck, Michele Ceriotti \n                \n                \n                \n                    \n                        Comput. Phys. Commun., 236, 214-223 (2019)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n            \n                \n                    9 \n                \n\n            \n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n\n            \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            Elucidating the Nuclear Quantum Dynamics of Intramolecular Double Hydrogen Transfer in Porphycene\n            \n            \n                \n                \n                    Yair Litman, Jeremy O. Richardson, Takashi Kumagai, Mariana Rossi \n                \n                \n                \n                    \n                        J. Am. Chem. Soc., 141, 2526-2534 (2019)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n            \n                \n                    8 \n                \n\n            \n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n\n            \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            Photophysics of Xanthene Dyes at High Concentrations in Solid Environments: Charge Transfer Assisted Triplet Formation\n            \n            \n                \n                \n                    Yair  Litman, Hernan B. Rodriguez, Silvia E. Braslavsky, Enrique San Roman \n                \n                \n                \n                    \n                        Photochem. and Photobiol., 94, 865-874 (2018)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n            \n                \n                    7 \n                \n\n            \n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n\n            \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            Decisive role of nuclear quantum effects on surface mediated water dissociation at finite temperature\n            \n            \n                \n                \n                    Yair Litman, Davide Donadio, Michele Ceriotti, Mariana Rossi \n                \n                \n                \n                    \n                        J. Chem. Phys., 148, 102320 (2018)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n            \n                \n                    6 \n                \n\n            \n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n\n            \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            \n            \n            \n                \n                \n                    Takashi Kumagai, Janina N. Ladenthin, Yair Litman, Mariana Rossi, Leonhard Grill, Sylwester Gawinkowski, Jacek Waluk, Mats Persson \n                \n                \n                \n                    \n                        J. Chem. Phys., 148, 102330 (2018)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n            \n                \n                    5 \n                \n\n            \n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n\n            \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            Positional Isotope Exchange in HX·(H2O)n (X = F, I) Clusters at Low Temperatures\n            \n            \n                \n                \n                    Yair  Litman, Pablo E. Videla, Javier Rodriguez, Daniel Laria \n                \n                \n                \n                    \n                        J. Phys. Chem. A., 120, 7213-7224 (2016)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n            \n                \n                    4 \n                \n\n            \n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n\n            \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            Tuning the concentration of dye loaded polymer films for maximum photosensitization efficiency: phloxine B in poly(2-hydroxyethyl methacrylate)\n            \n            \n                \n                \n                    Yair  Litman, Hernan B. Rodriguez, Enrique San Roman \n                \n                \n                \n                    \n                        Photochem. Photobiol. Sci., 15, 80-85 (2016)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n            \n                \n                    3 \n                \n\n            \n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n\n            \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            Effect of Concentration on the Formation of Rose Bengal Triplet State on Microcrystalline Cellulose: A Combined Laser-Induced Optoacoustic Spectroscopy, Diffuse Reflectance Flash Photolysis, and Luminescence Study\n            \n            \n                \n                \n                    Yair  Litman, Matthew G. Voss, Hernan B. Rodriguez, Enrique San Roman \n                \n                \n                \n                    \n                        J. Phys. Chem. A., 118, 10531-10537 (2014)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n            \n                \n                    2 \n                \n\n            \n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n\n            \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            Novel, simple and fast automated synthesis of 18F-choline in a single Synthera module\n            \n            \n                \n                \n                    Yair  Litman, Pablo Pace, Leandro Silva, Carlos Hormigo, Ricardo Caro, Hector Gutierrez, Maria Bastianello, Guillermo Casale \n                \n                \n                \n                    \n                        AIP Conf. Proc., 1509, 223-227 (2012)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n            \n                \n                    1 \n                \n\n            \n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n\n            \n\n        \n    \n    \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "previous_lifes/Photochemistry/index.html",
    "href": "previous_lifes/Photochemistry/index.html",
    "title": "Photochemistry",
    "section": "",
    "text": "I started my research career as an undergraduate in the field of photochemistry under the guidance of Dr. Enrique San Roman. There, I enjoyed measuring the triplet and singlet state quantum yields using a battery of spectroscopic techniques, including laser-induced optoacoustic spectroscopy (LIOAS), diffuse reflectance flash photolysis, and laser-induced luminescence (LIL)."
  },
  {
    "objectID": "previous_lifes/Photochemistry/index.html#related-publications",
    "href": "previous_lifes/Photochemistry/index.html#related-publications",
    "title": "Photochemistry",
    "section": "Related Publications",
    "text": "Related Publications\n\n\n    \n      \n      \n    \n\n\n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            Photophysics of Xanthene Dyes at High Concentrations in Solid Environments: Charge Transfer Assisted Triplet Formation\n            \n            \n                \n                \n                    Yair  Litman, Hernan B. Rodriguez, Silvia E. Braslavsky, Enrique San Roman \n                \n                \n                \n                    \n                        Photochem. and Photobiol., 94, 865-874 (2018)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n\n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n  \n              \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            Tuning the concentration of dye loaded polymer films for maximum photosensitization efficiency: phloxine B in poly(2-hydroxyethyl methacrylate)\n            \n            \n                \n                \n                    Yair  Litman, Hernan B. Rodriguez, Enrique San Roman \n                \n                \n                \n                    \n                        Photochem. Photobiol. Sci., 15, 80-85 (2016)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n\n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n  \n              \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            Effect of Concentration on the Formation of Rose Bengal Triplet State on Microcrystalline Cellulose: A Combined Laser-Induced Optoacoustic Spectroscopy, Diffuse Reflectance Flash Photolysis, and Luminescence Study\n            \n            \n                \n                \n                    Yair  Litman, Matthew G. Voss, Hernan B. Rodriguez, Enrique San Roman \n                \n                \n                \n                    \n                        J. Phys. Chem. A., 118, 10531-10537 (2014)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n\n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n  \n              \n\n        \n    \n    \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "posts/2018-09-13-writing-challenge-day-14/index.html",
    "href": "posts/2018-09-13-writing-challenge-day-14/index.html",
    "title": "Writing challenge Day 14",
    "section": "",
    "text": "During the second week of the challenge, I had a lot of late working days. It has been certainly tiresome. As a consequence, I was less able to write.\nI still have that voice powering my desire to write. Actually, over the weekend I finished a short story that I had in my list for at least a year. That achievement feels really good and pumps the desire to keep writing and, most importantly, finishing more projects than those I start."
  },
  {
    "objectID": "posts/2018-09-13-writing-challenge-day-14/index.html#the-data",
    "href": "posts/2018-09-13-writing-challenge-day-14/index.html#the-data",
    "title": "Writing challenge Day 14",
    "section": "The data",
    "text": "The data\nThe word count has been erratic. This week, I have had a couple of days in which I wasn’t even able to sit down to write. However, I think it has a somewhat consistent trend around a global mean (~500 words per bout). I have high hopes for improvement of this word-count-per-bout metric.\n\n\n\n\n\n\n\n\n\nIf we look at the total writing per day, the data looks a little bit different.\n\n\n\n\n\n\n\n\n\nI think that 2000 words a day is a pretty high standard, not only because it’s a high number of words, but holding a consistent mark of 2000 words requires you to spend a significant amount of time writing every day.\nAlthough I think writing this much is doable, I don’t think it’s possible to maintain the high level if it’s not part of your job (hence you devote a specific amount of time each day to do it). I have not included the words I’ve written in job related writing in this count, only leisure writing. Incorporating the mindset behind this challenge to a job that needs the production of text in massive amounts can definitely produce good results. Successful PIs are machines that are able to write grants and papers for a living (and the improvement of science?) in a consistent manner, a mix of playing the numbers and exceptional management of scientific endeavors."
  },
  {
    "objectID": "posts/2018-09-13-writing-challenge-day-14/index.html#the-distractions",
    "href": "posts/2018-09-13-writing-challenge-day-14/index.html#the-distractions",
    "title": "Writing challenge Day 14",
    "section": "The distractions",
    "text": "The distractions\nDistractions while writing are a well-known issue. I have not been able to turn off distractions (e.g., internet, messages). I am mostly free of social media distractions. However, I have not committed to turn off my real life events in order to write. I attempt (and yes, fail pretty often) to schedule around my life in order to sit, focus, and write."
  },
  {
    "objectID": "posts/2018-09-13-writing-challenge-day-14/index.html#the-good",
    "href": "posts/2018-09-13-writing-challenge-day-14/index.html#the-good",
    "title": "Writing challenge Day 14",
    "section": "The good",
    "text": "The good\nFinishing a short story feels really good. I have also been having ideas that can work in other stories and improve the plot forward. Whenever I encountered a block, I made notes to myself and moved on, trying to advance with another project while my mind works on the background, in order to solve the problem that’s preventing me from writing a particular section.\nSome days I have 2 bouts of writing. It’s either because I move from the living table to the bed or because I wrote a bit during the morning and then came back to it during the afternoon/night. Whenever I do sit to write I have a sense of improved flow. I am able just let ideas transform into words with ease. The production rate is not quite up yet, mostly due to bad planning, lack of ideas or lack of clear outlines that help generating them."
  },
  {
    "objectID": "posts/2018-09-13-writing-challenge-day-14/index.html#to-improve",
    "href": "posts/2018-09-13-writing-challenge-day-14/index.html#to-improve",
    "title": "Writing challenge Day 14",
    "section": "To improve",
    "text": "To improve\nI still need to work on the outline. I am starting to notice the stitches in the books that I read. It is pretty clear when authors use outlines and develop ideas into short paragraphs. Although it is definitely a great way to put many ideas into text and increase the word count, I believe it can be a dangerous way to write, you risk repeating yourself too much (e.g., developing ideas in a new chunk/paragraph that were developed 17 chunks before).\nNote to self: You are not supposed to develop the previous idea that far while doing the outline, continue outlining please."
  },
  {
    "objectID": "posts/2018-01-05-teaching-for-mastery/index.html",
    "href": "posts/2018-01-05-teaching-for-mastery/index.html",
    "title": "Teaching for Mastery",
    "section": "",
    "text": "Grades are typically set in absolute terms: you either get the points or you don’t. It doesn’t matter where you started, it doesn’t matter how fast you learn. You either get the points or you don’t. Grading on progress takes a different approach to certifying achievement.\nIntuitively, it makes sense, if two people made the same amount of progress, they should get the same reward. But the way in which we interact with grades is different. We are trying to get a quantity that represents achievement. Particularly, we are interested in the degree of achievement of the learning goals set at the beginning of the course by the professor and the students. I understand that this treats knowledge as a commodity that can be obtained and accumulated. While I believe that reality is a bit more complex, I think this is a good starting point for our discussion.\nSo…How much do students learn anyway? That is a difficult question, let’s simplify by assuming some conditions:\n\nWe have set achievable learning objectives/goals.\nWe test often.\nOur testing method is valid and graded fairly.\nWe obtain an accurate representation of whatever is inside our students brains (aka “knowledge”).\nStudents put roughly the same amount of effort.\nStudents put enough effort for the task.\nOur teaching method is effective in increasing student’s knowledge of the subject.\n\nIf these were true, we would then likely see that our students make progress at roughly the same rate. But progress rate has its caveats. Let’s choose two students. It happens that one of them came to the course with bit more knowledge on the subject matter than the other. Given the assumptions listed above, here’s one possible outcome.\n\n\n\n\n\n\n\n\n\nWe can look at the performance summary in the following table. Both students performed similarly (change column) , but they didn’t come with the same background (min column). Thus, the cumulative knowledge/score is higher for studentA (max column).\n\n\n\n\n\nstudent\nmin\nmax\nchange\n\n\n\n\nstudentA\n32.06\n71.26\n39.2\n\n\nstudentB\n13.16\n52.36\n39.2\n\n\n\n\n\n\n\nGrading on progress put us in a conflict. If measuring progress, both students should be graded equally. But a criterion-based method tells us otherwise. Whenever there’s a critical amount of knowledge, it might be the case that some students fail to achieve it. Lowering the bar is not an option. Problems like this one do not arise because we chose the limit at 60%. Problems like this one arise because the arbitrary limit exists. There are options to solve the Pass/Fail issue. We can switch to setting a limit for the amount of progress students make in our classroom. However, setting an arbitrary limit for the amount of progress students make will still generate problems. Most importantly, in such a method, passing students fail to achieve criteria.\nMaybe our assumptions are wrong, maybe not all the students learn at the same rate. Let’s take another case.\n\n\n\n\n\n\n\n\n\nLet’s take a look at the summary table.\n\n\n\n\n\nstudent\nmin\nmax\nchange\n\n\n\n\nstudentB\n13.16\n52.36\n39.20\n\n\nstudentC\n78.55\n78.59\n0.04\n\n\n\n\n\nIn this case, if we chose to grade on progress, we would produce a counter-intuitive result. Absurd example aside, we can argue that studentC is overqualified for the course. But the truth is, this student hasn’t mastered the material yet. Wouldn’t it be great if all students got to 100%?"
  },
  {
    "objectID": "posts/2018-01-05-teaching-for-mastery/index.html#grading-on-progress",
    "href": "posts/2018-01-05-teaching-for-mastery/index.html#grading-on-progress",
    "title": "Teaching for Mastery",
    "section": "",
    "text": "Grades are typically set in absolute terms: you either get the points or you don’t. It doesn’t matter where you started, it doesn’t matter how fast you learn. You either get the points or you don’t. Grading on progress takes a different approach to certifying achievement.\nIntuitively, it makes sense, if two people made the same amount of progress, they should get the same reward. But the way in which we interact with grades is different. We are trying to get a quantity that represents achievement. Particularly, we are interested in the degree of achievement of the learning goals set at the beginning of the course by the professor and the students. I understand that this treats knowledge as a commodity that can be obtained and accumulated. While I believe that reality is a bit more complex, I think this is a good starting point for our discussion.\nSo…How much do students learn anyway? That is a difficult question, let’s simplify by assuming some conditions:\n\nWe have set achievable learning objectives/goals.\nWe test often.\nOur testing method is valid and graded fairly.\nWe obtain an accurate representation of whatever is inside our students brains (aka “knowledge”).\nStudents put roughly the same amount of effort.\nStudents put enough effort for the task.\nOur teaching method is effective in increasing student’s knowledge of the subject.\n\nIf these were true, we would then likely see that our students make progress at roughly the same rate. But progress rate has its caveats. Let’s choose two students. It happens that one of them came to the course with bit more knowledge on the subject matter than the other. Given the assumptions listed above, here’s one possible outcome.\n\n\n\n\n\n\n\n\n\nWe can look at the performance summary in the following table. Both students performed similarly (change column) , but they didn’t come with the same background (min column). Thus, the cumulative knowledge/score is higher for studentA (max column).\n\n\n\n\n\nstudent\nmin\nmax\nchange\n\n\n\n\nstudentA\n32.06\n71.26\n39.2\n\n\nstudentB\n13.16\n52.36\n39.2\n\n\n\n\n\n\n\nGrading on progress put us in a conflict. If measuring progress, both students should be graded equally. But a criterion-based method tells us otherwise. Whenever there’s a critical amount of knowledge, it might be the case that some students fail to achieve it. Lowering the bar is not an option. Problems like this one do not arise because we chose the limit at 60%. Problems like this one arise because the arbitrary limit exists. There are options to solve the Pass/Fail issue. We can switch to setting a limit for the amount of progress students make in our classroom. However, setting an arbitrary limit for the amount of progress students make will still generate problems. Most importantly, in such a method, passing students fail to achieve criteria.\nMaybe our assumptions are wrong, maybe not all the students learn at the same rate. Let’s take another case.\n\n\n\n\n\n\n\n\n\nLet’s take a look at the summary table.\n\n\n\n\n\nstudent\nmin\nmax\nchange\n\n\n\n\nstudentB\n13.16\n52.36\n39.20\n\n\nstudentC\n78.55\n78.59\n0.04\n\n\n\n\n\nIn this case, if we chose to grade on progress, we would produce a counter-intuitive result. Absurd example aside, we can argue that studentC is overqualified for the course. But the truth is, this student hasn’t mastered the material yet. Wouldn’t it be great if all students got to 100%?"
  },
  {
    "objectID": "posts/2018-01-05-teaching-for-mastery/index.html#mastery",
    "href": "posts/2018-01-05-teaching-for-mastery/index.html#mastery",
    "title": "Teaching for Mastery",
    "section": "Mastery",
    "text": "Mastery\nWait a minute… Shouldn’t we aim to get our students to know all the material? Aiming for mastery implies that the bar is at 100%. It also means that advancement to the next level requires achieving all learning outcomes.\nThis might sound as too tough, but learning new material involves adding the new information to the network of already established concepts in our brain. We can think of it as adding a new floor to a building. We can’t afford to let gaps in knowledge propagate year after year, just as we can’t add floors to a skyscraper with terrible foundations or gaps in the middle. We can’t afford gap propagation, even for small gaps. Salman Khan explains it better than me here, if you get anything from this article, I wish it’s the next quote:\n\nTeaching for Mastery is not a nice to have, it’s a social imperative. - Salman Khan"
  },
  {
    "objectID": "posts/2018-01-05-teaching-for-mastery/index.html#the-path",
    "href": "posts/2018-01-05-teaching-for-mastery/index.html#the-path",
    "title": "Teaching for Mastery",
    "section": "The path",
    "text": "The path\nThe path matters. We can think about learning in terms of packets of knowledge, cumulative unlocked skills or mini level-up moments. Let’s take a look at the journey of studentA and studentB.\n\n\n\n\n\n\n\n\n\nThese students are very similar. In fact, we can see them struggling to get some concepts (check the colored region). Still, an arbitrary limit makes them look different."
  },
  {
    "objectID": "posts/2018-01-05-teaching-for-mastery/index.html#the-time-constrain",
    "href": "posts/2018-01-05-teaching-for-mastery/index.html#the-time-constrain",
    "title": "Teaching for Mastery",
    "section": "The time constrain",
    "text": "The time constrain\nTeaching for mastery requires that we flip over the constrains. Normally, we would constrain on time. Students have x weeks to get the cumulative percent that we considered good enough. That creates gaps in knowledge. Forcing students to advance to a new level will propagate those gaps.\nInstead of advancing students with gaps in knowledge by constraining time, we could constrain knowledge. All students should get to 100%. If students needed more time to acquire a particular skill or solve a particular type of problem, they should be allowed to take it. By teaching this way, we reinforce resilience and promote the correct mindset for problem solving.\nUsing current technologies, it is possible to teach individualized lessons and track individual knowledge1. If we use this strategy we need to deal with differences between a fast learner2 and one that got stuck in a concept for longer. Here’s how the paths would look like.\n\n\n\n\n\n\n\n\n\nWe can see that one student mastered the content before the other. There’s no problem with that. In fact, speed differences already happen in the current system and faster students disengage because there’s no further development to go after. The big advantage of teaching for mastery is that we have options, at the moment marked with an asterisk * we could:\n\nPromote the student to the next level.\nPromote the student to be peer mentor.\nBoth options at the same time.\n\nThe first option is the more individualistic. If one made progress at a faster pace, one should be allowed to move forward with more content. Here we have a first difference with the current system. Currently, students are put together by age, as if that were the only variable that determined their capabilities. Teachers have to go a long way outside the comfort zone to give extra assignments to these students. Faster students become a burden instead of a bliss. When the content is already being delivered individually by design, there is no extra effort to pump more advanced material to fast learners.\nThe second option allows the student to engage in fulfilling activities and enhance even more their confidence with the topic. To be able to mentor a student or teach a topic, you have to have mastered the content and be able to communicate it in a way the other mind can understand. Fostering empathetic relationships brings a social element to the classroom, priming students to a real-life experience in collaborative environment. Additionally, students that teach interact with the topic in different ways than traditional students. Peer mentors gain a complete new dimension by having to exercise different approaches to solve a problem or convey an argument.\nThere are good arguments for having classrooms with small age differences. Sociability among students is heavily influenced by age and there are particular moments in development where two years of difference means a lot. Thus, exploring both options could be a good trade off for students. Given a student was able to master the material, now she has the opportunity to continue forward. But she also has the responsibility to help fellow students who are struggling. It sounds like a win-win situation to me."
  },
  {
    "objectID": "posts/2018-01-05-teaching-for-mastery/index.html#footnotes",
    "href": "posts/2018-01-05-teaching-for-mastery/index.html#footnotes",
    "title": "Teaching for Mastery",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nVisit Khan Academy↩︎\nWhen I say fast I do not mean to generalize, it’s just for the sake of argument. Fast as in “a student that took less time to master that particular material”.↩︎"
  },
  {
    "objectID": "posts/2018-08-30-writing-challenge-day-1/index.html",
    "href": "posts/2018-08-30-writing-challenge-day-1/index.html",
    "title": "Writing challenge Day 1",
    "section": "",
    "text": "It’s been a while since I have several things to write in the back of my mind. I finally got the strength to commit myself to one of those goal oriented challenges: Writing 2000 words a day.\nIt might sound like a bit much, now that I think about it, but today is Day 1 so I might as well get to do it. I have not limited the type of content I will write, my aim is to get as fluent as possible and to have writing become habitual. That means I will increase the number of posts/short stories/book chapters/papers/review papers/twitter posts.\nDoes coding count? Is it a line by line instead of word count? I have not started writing and I am already thinking how to cheat about it…I am not quite confident about fortune cookie writer, but, since I foresee writing to be paramount for my future, I’m 100% down to master it.\nI’m a bit obsessed about measuring stuff. Thus, I will also be implementing a suggested technique to keep a record of the data that such writing endeavor produces1. It goes like this:\nMake a table with\nIt sounds fairly easy to keep up with recording (I record so many other stuff) and there are some pieces of information that sound like really interesting to have. I have always considered myself better at night. Is it really the case? Do I write better at night or in the morning before work? Do I write better at home or somewhere else? Should I go to the international space station? Maybe that would give enough inspiration to put those works together into text like it’s no big deal.\nRecording means a lot of plots (oh yes it does) and I have played a bit with calendar/github like heatmap plots before (see here), so I will be happy to show the trends in future posts. I will also be commenting about my impressions with the technique and if I actually manage to make it a habit of mine.\nSo…having written 390 words and finished my first slotted time, I should go back to my duties. Every long journey starts with a small step, writing will continue in the next scheduled period."
  },
  {
    "objectID": "posts/2018-08-30-writing-challenge-day-1/index.html#footnotes",
    "href": "posts/2018-08-30-writing-challenge-day-1/index.html#footnotes",
    "title": "Writing challenge Day 1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSuggested here↩︎"
  },
  {
    "objectID": "posts/2018-10-02-skype-a-scientist-1/index.html",
    "href": "posts/2018-10-02-skype-a-scientist-1/index.html",
    "title": "Skype a Scientist part 1",
    "section": "",
    "text": "I have been lucky to participate in the project lead by Sarah McAnulty called Skype a scientist. So far, I have done calls with two grades:\nI should say it was great to be able to talk with students, tell them about science in general and my research. It was really fun to answer their questions. I felt they were surprised about talking with a normal human being that also happens to do science. I feel really grateful to Sonia and Cindy, they are wonderful teachers, making efforts to connect their students with science and those who make it possible. We also did a Q&A session, which I compiled below."
  },
  {
    "objectID": "posts/2018-10-02-skype-a-scientist-1/index.html#rd-grade",
    "href": "posts/2018-10-02-skype-a-scientist-1/index.html#rd-grade",
    "title": "Skype a Scientist part 1",
    "section": "3rd Grade",
    "text": "3rd Grade\nHow old are you?\nI am 26 years old.\nDo you dissect rats?\nYes, I dissect rats.\nWhat’s your favorite animal?\nMy favorite animal is the Lion.\nWhat’s your favorite color?\nMy favorite color is green!\nWhy do you like to learn about rats?\nRats are fantastic animals. They are super smart and very similar to humans.\nHow and why did you become a scientist?\nI have always been into questions, I am super curious! Scientists work to ask interesting questions and that’s the best job ever for curious people. Scientists also like to fix stuff. If we find ways to do it, we can help people in need.\nIs being a scientist fun?\nIt’s super fun! You get to see crazy stuff nobody else has ever seen. You are always learning and discovering new things.\nDo you like being a scientist?\nYes, I love it!\nDo you want to be a scientist forever?\nI am 26 years old…I think I have plenty time, but forever is too long\nIs your mom or dad a scientist?\nNo, my mom is an accountant and my dad is a salesman. I am the only scientist in my whole family!\nDo you have any kids?\nNot (yet)!\nDo you have friends?\nYes, I do!\nDo you have any pets?\nNope. But I had a dog before coming to the US and I’m looking into having a cat!\nWhat do you do on your breaks?\nI like to go hiking, I have seen BEAUTIFUL places in nature!\nWhen is your birthday?\nAugust the 12th (Candy is welcome)\nWhat do you like to eat?\nMeat, and Pasta, and Ice-Cream, and Candy, and Chocolate, and Pizza, I eat a lot !\nDo you play sports? Do you like sports?\nYes!\nDo you like anything else besides science?\nYes! I play soccer and I run, I cook, I dance, and I am a drummer!\nDo you invent things like robots, mouse traps…?\nI invent things that look like a robot to trap rats. Does it count?\nDo you have a Youtube channel?\nI had a YouTube channel (in Spanish). I teach different things (Math, Science, …) there! My channel is Profesores Gama.\nDid you like science since you were a child?\nActually, I liked Math the most. But math is most beautiful in Nature, it’s everywhere!\nIs rats the only animal you like to observe and study?\nNope, I like most animals! I worked with crabs before working with rats. Hopefully I will be able to work with mice, flies, and fish.\nHow many rats do you have in your lab?\nIt depends on the number of babies! We can have hundreds!\nWhat do rats like to eat?\nThey eat a lot of things, they go crazy about M&Ms and Cheerios.\nWhat kind of rats do you observe?\nI observe a particular type of albino rat called Sprague Dawley.\nDo you name your rats?\nYes! My favorite one is called SD7A4B1E3F3G1M2 but I prefer Amelia\nDo you observe boy rats too?\nSometimes. I mostly observe mother rats.\nWhat do you do with the baby rats?\nI put the baby rats with mothers and see how they take care of them.\nIs your lab big or small?\nWe have several rooms in two floors, fairly big.\nHow do you observe your rats?\nSometimes just looking, sometimes filming, sometimes with the microscope.\nWhere do you find your rats?\nWe breed them.\nDo your rats wear clothes?\nNope, any ideas in mind?\nWhere do you keep the rats so they will not escape?\nThey leave in comfortable cages with rat friends.\nWhat clothes do you use inside your lab? Uniform?\nLab coat, gloves, shoe protection, and eye protection. Full package."
  },
  {
    "objectID": "posts/2018-10-02-skype-a-scientist-1/index.html#th-grade",
    "href": "posts/2018-10-02-skype-a-scientist-1/index.html#th-grade",
    "title": "Skype a Scientist part 1",
    "section": "7th Grade",
    "text": "7th Grade\nWhy did you want to become a scientist?\nI don’t know…I just felt really curious about how the natural world works. I could see beauty in Math and later in Chemistry, and wanted to learn more. Living things are by far the most complex systems we know of, so I was totally down for the challenge of figuring out how they work.\nWhen did you become a scientist?\nThis question is really challenging. The correct answer is I don’t know. I am not sure there was a date. I could choose a random date when I was around 10 or I could say I’m working to be a scientist. As it happens with everything, there are some days when I feel more scientist than others.\nHow many years did you have to go to school before you became a scientist?\nI can tell you how many years I’ve been in school. Elementary school was 7, middle/high-school was 5, then university added another 5-6 years, 2 years for my Master’s of Science degree. School is just the beginning.\nIs science different in Argentina than in the US?\nScience in Argentina is different. To start, we speak in Spanish and drink mate instead of coffee or tea during breaks. One thing that the US has is people from many different places, I’ve met people from all the continents, all of them working here, together, to make the best science they can. In Argentina, things are a bit complicated because of the rough economic situation in the country.\nWhat fascinates you about neuroscience?\nEvolution has given animals a biological machine that is far more advanced and efficient than anything we’ve ever built or known. We have the most complex machine between our ears and, curiously, we can only understand the world using that machine. We only perceive what our brain interprets, we can’t be sure about things being out there. For all we know, there could be nothing out there, everything could be an illusion recreated by our minds, playing inside our heads. How do you know the color I call red is the color you call red? The first time I heard that question I was 12 years old, and it continues to blow my mind.\nWhat is the thing that surprised you the most about studying the rodents?\nRodents are extremely robust. Also, I can’t help but to have the feeling that their life is so much faster than ours. Their metabolism is higher, their lifespan is shorter. Five minutes is a lot of time for a mouse, much more for the tiny neurons in mice brains. Still, five minutes is not enough for long term events that influence 99.999% of the biology of animals.\nAre rodent brains different sizes?\nYes, different species have different brain sizes. This is true for almost all living things. Here’s a cool plot about brain size1!\n\nIt is also impressive that brain sizes grow predictably with body mass23.\n\nThe brain size question is tricky because we try to jump into conclusions about intelligence or cognitive abilities (also see below). The truth is, most of what a bigger brain is there for has nothing to do with what we humans call intelligence. For example, if you are a bigger animal, you need to receive signals from more places and you have larger muscles to control (aka, there’s a lot of room to tickle an elephant and moving an elephant needs a lot of neurons). Hence, bigger animals will tend to have a bigger brain.\nAre brains smarter if they are bigger?\nThis is a frequently asked question. The key thing to consider, as previously stated, is the size of the animal that carries that brain. A mosquito can’t carry a whale’s brain, simply because it’s own size is smaller than the whale’s brain. However, mosquitoes have a nervous system that allows them to solve complex tasks and have successful reproductive endeavors.\nBrains are there to sense, process the information we receive from the outside world and make a decision (aka move towards/away). We also have to consider what smart means. But that’s a discussion for another day. Briefly, we tend to think about being smart as having higher cognitive abilities. For example, a higher cognitive abilities might usually depend on integration of several sensory stimuli and abstraction, like appreciating the regular patterns of a particular instrument in a symphony. Those activities tend to correlate well with the size of a region of the brain called the cortex, which is pretty big in primates.\nCan you show us a rat? Can you show us its brain?\nNo, but I can show you pictures of brains.\nThese are sections of a mouse brain.\n\n\nBegin again pic.twitter.com/kfflg6R8XE\n\n— Matias Andina (@NeuroMLA) September 13, 2018\n\n\nThis is a picture of a special staining showing different kinds of neurons in a rat brain.\n\n\nPaint…? Nah, Antibodies :) #sciart pic.twitter.com/LlxBOPlzs5\n\n— Matias Andina (@NeuroMLA) August 24, 2018\n\n\nCan rats swim underwater? Where do they live?\nYes they can, check this cool picture of rats swimming. We actually use the fact that rats are good swimmers to challenge them in a maze. It’s called the water maze (also known as Morris maze).\nRats are highly adapted to many different ecosystems. They can also do pretty well near human populations, scavenging city waste for a living. The map below shows an estimate of where rats live (aka everywhere):\n\nWhat is the difference between a rodent brain and a human brain?\nThey are pretty similar, both contain massive amount of cells called neurons and glia. A key distinction is the size of the olfactory bulbs of a rodent and a human. While rodents have huge amount of brain devoted to the task of smelling the outside world, human bulbs are tiny. We are visual animals and hence, much of our brain is devoted to seeing things.\nHow is it possible to learn from a rodent’s brain how our brain works?\nHumans and rodents are pretty related. We have a huge amount of DNA in common. We both have spinal cords and a similar brain organization. Many of the things that keep us alive have been functioning since many many years ago and are shared in the evolution of both animals.\nStudying the brain of a rodent can tell us many ways in which human brains function, and can help us build models of human diseases to try potential treatment before risking human lives. If it works in mice, there’s a good chance that it will work in humans. At least it’s worth a shot. Moreover, if it doesn’t work in mice, especially if mice show detrimental effects, there are good reasons not to try it in humans.\nWhat equipment do you use in your lab to study rodents?\nLots. Most commons are test tubes, pipettes, and syringes. The microscopes are the fancy equipment we use to take great pictures.\nCan rodents learn math?\nThis is a pretty challenging question. I don’t want to get into the What is math question. People tried to teach horses how to do math, which was a commercial success, but turned out that horse was not actually doing it for real, it was responding to the signals of its owner (see Clever Hans here). Crows can solve puzzles that need a pretty deep understanding of the physical world, like fluid displacement, compare object lengths/weights. In my opinion, that comes super close to our math. Youtube is full of awesome crow videos!\n\n\n\nCrow solving a puzzle that involves fluid displacement\n\n\nWe know mathematical abilities are not fully related to language, cultures that don’t have the words for specific numbers can distinguish different amounts and do simple math without any formal structure. Many animals can definitely learn simple tasks that are related to number of events or a certain amount of time. They can distinguish amounts (e.g., 10 M&Ms is more food than 2 M&Ms), and they can keep a count. However, we often face difficulties to be certain of what is going on in the rat’s head. We can’t ask, we can’t be rats. Thus, the best question would be whether we can test unequivocally that rats are performing mathematical operations, counting, or have any representation of numbers.\nFor those of you really tempted to dig further into this question:\n\nhttps://en.wikipedia.org/wiki/Number_sense_in_animals\nhttps://www.ncbi.nlm.nih.gov/pubmed/26463617"
  },
  {
    "objectID": "posts/2018-10-02-skype-a-scientist-1/index.html#footnotes",
    "href": "posts/2018-10-02-skype-a-scientist-1/index.html#footnotes",
    "title": "Skype a Scientist part 1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe source of the brain pictures can be found here↩︎\nThis is called brain::body-mass ratio and it is a famous law of scaling. More info here https://en.wikipedia.org/wiki/Brain-to-body_mass_ratio.↩︎\nThe source of the graph can be found here. Many other places will have similiar graphs. Also, I recommend a book called Scale by Geoffrey West.↩︎"
  },
  {
    "objectID": "posts/2019-04-06-email-analysis/index.html",
    "href": "posts/2019-04-06-email-analysis/index.html",
    "title": "Email analysis",
    "section": "",
    "text": "I am keen on tracking things. Some people call this personal analytics, I call it fun. In this post, I will explore how to extract your email information using python and R. My goal is to do some analysis on the data and, hopefully, explore different visualizations that can inform future behavior."
  },
  {
    "objectID": "posts/2019-04-06-email-analysis/index.html#emails-from-gmail",
    "href": "posts/2019-04-06-email-analysis/index.html#emails-from-gmail",
    "title": "Email analysis",
    "section": "Emails from Gmail",
    "text": "Emails from Gmail\nReading emails in python is quite simple, we need to import the mailbox library. My file is called correo and comes from downloading my Gmail stuff out of https://takeout.google.com/settings/takeout. It can take a while so be patient.\nThis should return something that looks like:\nX-GM-THRID\nX-Gmail-Labels\nReceived\nMIME-Version\nDate\nTo\nFrom\nSubject\nContent-Type\nContent-Disposition\nMessage-Id\nLet’s save the info we care about into a new file. We select subject, from, date, to, and some variables that allow us to keep track of things (i.e, labels and threads)."
  },
  {
    "objectID": "posts/2019-04-06-email-analysis/index.html#emails-from-thunderbird",
    "href": "posts/2019-04-06-email-analysis/index.html#emails-from-thunderbird",
    "title": "Email analysis",
    "section": "Emails from Thunderbird",
    "text": "Emails from Thunderbird\nI keep 3 accounts in Thunderbird. Using the ImportExportTools Add-on, I exported things into .mbox format. Following a similar procedure to the one depicted above, I got the other three accounts exported to .csv files. Just be sure you select the correct keys (see example below, this might change for other email clients)."
  },
  {
    "objectID": "posts/2019-04-06-email-analysis/index.html#data-cleaning",
    "href": "posts/2019-04-06-email-analysis/index.html#data-cleaning",
    "title": "Email analysis",
    "section": "Data cleaning",
    "text": "Data cleaning\nLet’s switch from python to R1.\nUnfortunately, emails come tagged (things like \"\\\\?=\" and other nasty stuff) and you might have to deal with different encodings (the perks of speaking multiple languages). As an example, let’s see what Quora sends me.\n\n\n                                                             from\n1                         Quora Digest &lt;digest-noreply@quora.com&gt;\n2                         Quora Digest &lt;digest-noreply@quora.com&gt;\n3                         Quora Digest &lt;digest-noreply@quora.com&gt;\n4  =?utf-8?q?Selecci=C3=B3n_de_Quora?= &lt;digest-noreply@quora.com&gt;\n5                         Quora Digest &lt;digest-noreply@quora.com&gt;\n6                         Quora Digest &lt;digest-noreply@quora.com&gt;\n7                         Quora Digest &lt;digest-noreply@quora.com&gt;\n8  =?utf-8?q?Selecci=C3=B3n_de_Quora?= &lt;digest-noreply@quora.com&gt;\n9  =?utf-8?q?Selecci=C3=B3n_de_Quora?= &lt;digest-noreply@quora.com&gt;\n10 =?utf-8?q?Selecci=C3=B3n_de_Quora?= &lt;digest-noreply@quora.com&gt;\n\n\nThat’s nasty…Let’s do some cleaning. This function comes really handy for text replacement.\nWe are going to modify the function a bit, we add x as the string we pass for cleaning and we will remove the tags progressively.\nWe are ready to use our super cool function and clean the text! Not perfect, but gets us 90% of the way.\nLet’s see how emails from Quora changed with this new encoding:\n\n\n                                            from\n1        Quora Digest &lt;digest-noreply@quora.com&gt;\n2        Quora Digest &lt;digest-noreply@quora.com&gt;\n3        Quora Digest &lt;digest-noreply@quora.com&gt;\n4  Selección_de_Quora &lt;digest-noreply@quora.com&gt;\n5        Quora Digest &lt;digest-noreply@quora.com&gt;\n6        Quora Digest &lt;digest-noreply@quora.com&gt;\n7        Quora Digest &lt;digest-noreply@quora.com&gt;\n8  Selección_de_Quora &lt;digest-noreply@quora.com&gt;\n9  Selección_de_Quora &lt;digest-noreply@quora.com&gt;\n10 Selección_de_Quora &lt;digest-noreply@quora.com&gt;\n\n\nLet’s filter those from “Received” or “Sent” (in Spanish, “Recibidos” or “Enviado”).\nTo save you from reading a considerable amount of code, I will load the other accounts and modify them accordingly in the background. I will finally merge everything together. Just enjoy the kitten while the code is running in my machine."
  },
  {
    "objectID": "posts/2019-04-06-email-analysis/index.html#analysis",
    "href": "posts/2019-04-06-email-analysis/index.html#analysis",
    "title": "Email analysis",
    "section": "Analysis",
    "text": "Analysis\nThere’s still some stuff to clean, but I’d rather go into the analysis. So, let’s get some questions to guide our purpose:\n\nWho sends me the most emails? Who receives emails from me?\nWhen do I get emails (mostly)?\nWhen should I do something about it (aka, reply)?\n\n\nWarning: We have to dance with parsing dates and times. I highly recommend being familiar with lubridate (for example, see https://rdrr.io/cran/lubridate/man/parse_date_time.html).\n\n\nMost frequent senders\nJust because I’m curious, let’s take a look at who are the all time senders!\n\n\n                    from   n\n1                  Quora 393\n2                 Maggie 316\n3                   Yair 216\n4                Luciano 173\n5                  Sarah 167\n6                  \"Bank 161\n7           \"Amazon.com\" 139\n8                Mariana 138\n9  pubchase@zappylab.com 131\n10            \"Mendeley\" 126\n\n\nIt’s cool to know that my lingering feeling (“wow…Quora just spams the hell out of me”) is supported by data. Other big spammers are, of course, the Bank and Amazon. People I work with and friends come high up too. Funny to see Mendeley and Pubchase on the top ten, it’s been a long journey of them sending me papers, thank you for that2.\n\n\nFrom me to you\nLet’s try to find the people I directly send the most emails to. I tend to send a lot of automatic reminders via email to myself so I removed me from the destination.\n\n\n       to   n\n1 Mariana 192\n2 Mariana 126\n3    Yair  86\n4 Mélanie  64\n5   Beata  59\n\n\nLooks like both my former advisers get most of my output (yes, same name first name, not related)."
  },
  {
    "objectID": "posts/2019-04-06-email-analysis/index.html#working-with-dates-and-times",
    "href": "posts/2019-04-06-email-analysis/index.html#working-with-dates-and-times",
    "title": "Email analysis",
    "section": "Working with dates and times",
    "text": "Working with dates and times\nEvery time I have to deal with dates, I have a miniature panic attack. As a general rule, you have to have all the variables that you want to use as separate columns (i.e, year, month, day, week_day, time, …). The lubridate package helps a lot, but it’s still quite an effort.\nWorking only with times of the day, regardless of date itself is problematic. Working with periods is difficult, so as.numeric(x, \"hour\") is a friend.\nHere’s a hint of how the date column in the original data actually looks like. This may or might not look the same way for you, it depends on your date settings.\n\n\n                             date\n1 Sat, 23 Mar 2019 08:57:48 -0700\n2 Sat, 23 Mar 2019 08:57:32 -0700\n3 Sat, 23 Mar 2019 20:25:31 -0400\n4 Sat, 23 Mar 2019 08:57:46 -0700\n5 Sat, 23 Mar 2019 08:57:35 -0700\n\n\nLet’s create all the variables we need. It seems like a lot because it should work out of the box and it doesn’t, but it’s actually somewhat straight-forward to get most of what we want."
  },
  {
    "objectID": "posts/2019-04-06-email-analysis/index.html#birdseye",
    "href": "posts/2019-04-06-email-analysis/index.html#birdseye",
    "title": "Email analysis",
    "section": "Birdseye",
    "text": "Birdseye\nLet’s look at how the whole email movement looks like. In the last couple of years, I clearly felt the load rising and rising. The lack of data in the early years is mostly due to me not downloading everything from the Hotmail account (it’s too late, too far in the past to fix :P). Besides, the trend likely holds quite well.\n\n\n\n\n\n\n\n\n\nIf we split by input and output, we can easily see that the input-output ratio went nuts when I moved to the US.\n\n\n\n\n\n\n\n\n\nThis is not really surprising, given the amount of unsolicited advertising I started getting since the move. Yes, I’m talking to you again Quora/Amazon/people trying to sell me stuff3. Of course, University related chains likely take a big chunk of the pie.\nI don’t feel like parsing out each sender out of the sheer amount. I have had the Gmail and Hotmail accounts for more than 10 years, but the University email is something relatively recent. All in all, considering the time I’ve had each account, the input rate coming from universities worries me. Here are the total email for each account:\n\n\n\n  Gmail hotmail     MIT   umass \n   7481   10331    1297    7122 \n\n\n\nWhen\nLet’s add the time of the day to the equation. This plot was made using ggbeeswarm package, I highly recommend checking it, it’s pure power. I got help to put the labels in the y axis from ‘00:00’ to ‘24:00’. You can find a toy example in this StackOverflow question.\n\n\nDaily news\nWhat’s the average number of emails per day? I’m including all the emails in from 2015 to 2019, including those that go directly to trash.\n\n\n\n\n\n\n\n\n\nFor those looking for some tabulated info, here it is:\n\n\n\nNumber of emails per day received in all accounts\n\n\nYear\nMean\nMin\nMax\nMode\n\n\n\n\n2015\n6.8\n1\n30\n3\n\n\n2016\n10.2\n1\n45\n2\n\n\n2017\n14.5\n1\n68\n2\n\n\n2018\n13.0\n1\n66\n6\n\n\n2019\n18.9\n1\n132\n7\n\n\n\n\n\nI am more inclined to graphics, the following figure shows not only an increasing mean, but, surprisingly, a widening range.\n\n\n\n\n\n\n\n\n\n\n\n\nAll days were not created equal\nOf course, the number of emails somewhat depends on the day of the week. We can easily see a decreasing trend.\n\n\n\n Mon  Tue  Wed  Thu  Fri  Sat  Sun \n4720 4748 4469 4326 3928 1938 1970 \n\n\nAlthough the day of the week has influence on the amount of emails received, the time of the day seems to have a stronger, more permanent effect.\n\n\n\n\n\n\n\n\n\n\n\nEverything together\nIf we pool all the data together, it seems that I receive/send emails at all times, although there is more movement in the accounts around 10:00 and 16:30. Overall, the distributions are quite similar4.\n\n\n\n\n\n\n\n\n\n\nJust for fun\nJust for the fun of data visualization. Here’s the same plot but adding coord_polar to it. I believe it creates a very weird but good looking thing. It’s not really a clock but there’s something about it I can’t stop looking at5.\n\n\n\n\n\n\n\n\n\n\n\n\nSplit in two\nAs you can see from the figures above, the emails in the received bucket have two humps (wink, Bactrian camel, little prince), but I send emails at almost all times (except maybe between 2 AM and 5 AM). This is a bad habit, I should not be sending emails all the time, I should batch to diminish the costs associated with shifting tasks. I could just put a rule of thumb and check emails only once a day (e.g, 12:00:00). However, this might not be the best decision, because it chunks the response time in two very broad categories (either I get back to you somewhat quick, within 2 hours, or I take almost a full day to reply).\n\n\n\n\n\n\n\n\n\nAdditionally, checking emails only once might make me miss something somewhat fleeting. In general, I want to read things during the time they are relevant (did anybody say free pizza?).\nThe primary goal, then, is to minimize the times I check/send emails without 1) impacting my perceived response rate and 2) missing out too much info during the day. But that optimization problem is hard to solve and likely a waste of time (trust me, I tried and I’m not that smart).\nI believe we can solve it with a rule of thumb anyway. Let’s say, I would check emails twice a day and respond immediately, unless I need to harness some brain power to create an elaborate response6.\nI just wrote a “cost function” and calculated the cost for several combinations of times.\n\n\nShow the code\nvalues &lt;- emails %&gt;%\n  filter(simple_label==\"Received\") %&gt;%\n  mutate(val = as.numeric(seconds_to_period(my_time))) %&gt;%\n  pull(val)\n\n# calculate linear distance to minimize \n\ndist_to_min &lt;- function(values, possible_times){\n \n  min_time &lt;- min(possible_times)\n  max_time &lt;- max(possible_times)\n  # do nothing to first batch\n  corrected_values &lt;- ifelse(values &lt; max_time,\n                           values,\n  # shift the ones answered on next day, this already gives positive distance\n                          86400 - values + min_time)\n\n  \n  to_second &lt;- between(corrected_values, min_time, max_time)\n  second_batch &lt;- corrected_values[to_second]\n  first_batch &lt;- corrected_values[!to_second]  \n  \n  # Calculate distance (should be all positive)\n  dist_second &lt;- max_time - second_batch \n  \n  dist_first &lt;- ifelse(first_batch &lt; min_time,\n                       min_time - first_batch,\n                       corrected_values)\n\n  total_dist &lt;- sum(c(dist_first, dist_second))\n  \n  return(total_dist)\n}\n\n\nNow we can use our dist_to_min function in a loop. We’ll calculate from the first second of the day, to the last (86400) every half hour (1800 sec).\n\n\nShow the code\n# Create the data to iterate over\nval &lt;- seq(1, 86400, 1800)\nval &lt;- data.frame(t(combn(val,2)))\nnames(val) &lt;- c(\"Var1\", \"Var2\")\ndistance &lt;- numeric(length=nrow(val))\n\n# For loop...\nfor (i in 1:nrow(val)){\n  possible_times &lt;- val[i, ]\n  \n  distance[i] &lt;- dist_to_min(values, possible_times)\n  \n}\n\n\nThe function calculates the distance we want to minimize. The output looks like this.\n\n\n\n\n\n\n\n\n\nSounds like the combinations we care about are those below 2.5e+8.\n\n\n\n\n\n\n\n\n\nAll this long post is to say that, from now on, I will be answering my emails in either one of these combinations.\n\n\n  first_batch second_batch\n1   12H 0M 1S    18H 0M 1S\n2   11H 0M 1S   17H 30M 1S\n\n\n\n\nA finer grain\nJust for the fun of it, let’s take a closer look, a second by second analysis. It seems like machine programmed emails peak at 2 and 3 seconds past midnight.\n\n\n       time simple_label Freq\n1  00:00:02     Received   77\n2  00:00:03     Received   43\n3  15:32:51     Received    6\n4  10:17:11     Received    5\n5  12:05:40     Received    5\n6  12:09:11     Received    5\n7  12:42:03     Received    5\n8  15:30:29     Received    5\n9  17:17:06     Received    5\n10 09:07:15     Received    4\n\n\nWho are these emails coming from anyway?\n\n\n# A tibble: 10 × 2\n   from         n\n   &lt;chr&gt;    &lt;int&gt;\n 1 @mit.edu    33\n 2 @mit.edu    24\n 3 @mit.edu    23\n 4 @mit.edu    10\n 5 @mit.edu     6\n 6 @mit.edu     6\n 7 @mit.edu     4\n 8 @mit.edu     3\n 9 @mit.edu     3\n10 @mit.edu     2\n\n\nLooks like people at MIT programmed news to be sent seconds after midnight."
  },
  {
    "objectID": "posts/2019-04-06-email-analysis/index.html#summary",
    "href": "posts/2019-04-06-email-analysis/index.html#summary",
    "title": "Email analysis",
    "section": "Summary",
    "text": "Summary\nI have had a lot of fun doing this project. I also experienced an enormous amount of frustration with dates. Moreover, every time I thought this project was over, a new little idea for a not so little graph came into my mind. Of course, I went after it. I hope this info helps other people take a look at their own personal analytics and make some decisions. I am somewhat happy I have almost all notifications turned off (hence, no Facebook/Twitter/Slack/whatever appearing as top senders). In fact, turning email notifications off is the first thing I do when I sign up for a service/site, I encourage you to do the same.\nBatching is something I will start testing. I can’t control my input but, hopefully, the distributions of my sent email will start matching the times I designated. More importantly, people will not notice, even if the email input keeps increasing.\n\nSome people requested me to do the following scatter-plot. I went with the ggbeeswarm version on the text because I find it more appealing.\n\n\n\n\n\n\n\n\n\n\nI excluded parts of the code because it was too much. I am happy to share if requested!\n\nSources:\nhttps://jellis18.github.io/post/2018-01-17-mail-analysis/\nhttps://blog.stephenwolfram.com/2012/03/the-personal-analytics-of-my-life/\nhttps://uc-r.github.io/kmeans_clustering"
  },
  {
    "objectID": "posts/2019-04-06-email-analysis/index.html#footnotes",
    "href": "posts/2019-04-06-email-analysis/index.html#footnotes",
    "title": "Email analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou could actually stay in python (follow https://jellis18.github.io/post/2018-01-17-mail-analysis/). I’m way more comfortable with R for analysis and I only wanted python because I had the copy-paste version of getting my .mbox file to .csv fast.↩︎\nI had signed up for Mendeley before Elsevier bought it…I’m not quite happy about it now, but at least I still get paper recommendations.↩︎\nI know I could just unsubscribe to these kind of things, just the way I do with 99% of all other aggressive garbage. I just didn’t do it for these senders.↩︎\nDo they look like an elephant inside a boa or a hat?↩︎\nPlease, if you know how to make the 0:00 or 24:00 appear on the center, reach out! I couldn’t figure it out.↩︎\nIf you have as much free time as me, you can run a kmeans(...). My emails actually turned out to be around 2 clusters.↩︎"
  },
  {
    "objectID": "posts/2020-04-30-complex-fun/index.html",
    "href": "posts/2020-04-30-complex-fun/index.html",
    "title": "Complex Fun",
    "section": "",
    "text": "People say that if you have to explain a joke, it loses value. Probably true. I guess I just wanted to partially comment on how different disciplines analyze different levels of complexity.\nAs molecular biologist, I think we were trained to seldom think about the whole system, we were trained to just give up complexity and focus on a couple of fancy names (yes, YFG fits pretty well as the center of this Universe). Genes are awesome, weak protein-protein interactions and conformational changes rock. Are you missing the whole world by focusing too much?\nAs a biologist, I think we were trained by association (i.e., “If this, then that. Now repeat logic for a million different processes”). This means that our view of every phenomenon tends to be overly complicated. We do this at every scale we analyze Life, not always with the study complexity in mind, but just as a reflex. This is not an intrinsic fault of the biologist, it might be just a result of how data is collected and the empirical nature of the field.\nI think physicists get it often right, as simple as it should be, not less. If the whole thing is a tennis ball, and your system of interest is ruled by macro laws, just approximate and do \\(m\\vec{g}\\).\nAll in all, this is the same dataset, 3 worlds apart (or 3 convention centers apart). I think the joke is on us :smile:.\n\nFor those of you who want to recreate, you can find the R code below:\n\n\nCode\nlibrary(tidyverse)\nlibrary(xkcd)\nlibrary(patchwork)\n\nset.seed(2)\ndf &lt;- tibble(\n  x = rnorm(100, 0, 2),\n  y = rnorm(100, 0, 2)\n) %&gt;%\n  mutate(time = 1:n())\ndf &lt;- tweenr::tween_along(df,'cubic-in-out', 1000, along = time)\n\n\nmy_theme &lt;-\n  theme_xkcd()+\n  theme(panel.border = element_rect(fill=NA, color=\"black\", size=1),\n        axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        plot.title = element_text(hjust = 0.5))\n\n\np0 &lt;- ggplot(df %&gt;% slice(1:100), aes(x,y))+\n  geom_jitter()+\n  geom_line(position=\"jitter\")+\n  my_theme+\n  labs(title=\"MOLECULAR\\nBIOLOGIST\", x=\"\", y=\"\")\n\np1 &lt;- ggplot(df %&gt;% sample_n(1000), aes(x,y))+\n  geom_path(lwd=0.5, alpha=0.8)+\n  my_theme+\n  labs(x=\"\",y=\"\",title=\"BIOLOGIST\")\n\np2 &lt;- ggplot()+\n  ggforce::geom_circle(aes(x0=0,y0=0,r=1), fill=\"gray30\", color=NA)+\n  geom_segment(aes(x=0, xend=0, y=0,yend=-0.5),\n               arrow = arrow(length = unit(0.2, \"cm\"),\n                             type = \"closed\"),\n               arrow.fill = \"black\", size=0.5, color=\"black\")+\n  geom_text(aes(x=0.2,y=-0.5), label=\"mg\", color=\"black\")+\n  my_theme +\n  labs(x=\"\", y=\"\", title=\"PHYSICIST\")\n  \np0 + p1 + p2\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{andina2020,\n  author = {Andina, Matias},\n  title = {Complex {Fun}},\n  date = {2020-04-30},\n  url = {https://matiasandina.netlify.app/posts/2020-04-30-complex-fun/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndina, Matias. 2020. “Complex Fun.” April 30, 2020. https://matiasandina.netlify.app/posts/2020-04-30-complex-fun/."
  },
  {
    "objectID": "posts/2017-11-30-about-good-discussions/index.html",
    "href": "posts/2017-11-30-about-good-discussions/index.html",
    "title": "About Good Discussions",
    "section": "",
    "text": "Arguing may likely be an intrinsic feature of our species, maybe our competitive edge, or even a paramount example of our Human Nature1. Part of my decision to work in Science is because of the continuous exchange of ideas generated through formal argumentation. By formal argumentation I mean the translation of the basic principles that govern how the Universe functions into a language we can understand and share (hopefully agree upon?). But I also enjoy argumentation outside the scientific environment. For example, I deeply enjoy books on negotiation2 and having hypothetical conversations in agreement/disagreement with Sam Harris or Tim Ferris.\nI devote some time to think about discussions and debate myself about the existence of general principles that can be extracted. Such principles should work regardless of the context in which the discussion is happening (e.g, professional, teaching, personal).\nI have been baking some ideas, which I try to apply to the way I work with people and teach. Some of my thinking is below:"
  },
  {
    "objectID": "posts/2017-11-30-about-good-discussions/index.html#what-makes-a-good-discussion",
    "href": "posts/2017-11-30-about-good-discussions/index.html#what-makes-a-good-discussion",
    "title": "About Good Discussions",
    "section": "What makes a good discussion?",
    "text": "What makes a good discussion?\n\nExchange of ideas. This is more than a fundatamental principle of arguments, it is mandatory for any communication that has more than one direction of flow. Matt Ridley has made an interesting point about what are the best contexts in which this exchange happens for scientific endeavors.\nDefining the space of the discussion. I detest the philosophical debate about definitions, particularly when it prevents from getting to the actual discussion. Burrying a debate in semantics is one of my greatest fears. But Philosophers understand that there is no point in trying to debate about something if we do not agree about what that something is. Additionally, for a discussion to be meaningful, we should agree on the terms in which we evaluate evidence and thus are allowed to make valid claims. If and only if we agree on the axioms that serve as building blocks for argumentation we might have a proper space to elaborate reasons and viewpoints (from Marvel vs DC to the Meaning of Life)."
  },
  {
    "objectID": "posts/2017-11-30-about-good-discussions/index.html#what-makes-a-bad-discussion",
    "href": "posts/2017-11-30-about-good-discussions/index.html#what-makes-a-bad-discussion",
    "title": "About Good Discussions",
    "section": "What makes a bad discussion?",
    "text": "What makes a bad discussion?\n\nThe agree to disagree mentality. This exit is somewhat primitive. It shuts conversation, it stalls argumentation at a basic level. The goal of coming to an agreement is always secondary to the fact that we are exchanging ideas. The danger of this phrase is that it hides what the real problem is (the subject matter of discussion) and shifts it towards the person you are disagreeing with (e.g., because you are not smart/patient/calm/whatever enough to get this, I am no longer talking with you). Passive aggressiveness is not a constructive way to get to a disagreement. It is true that, given genuine disagreement, the real conclusion of a discussion will sound much like this phrase. However, taking this exit prematurely makes both parts end up losing, and feeling as if it the endeavour was a waste of time, while getting at the genuine point in which we disagree and understanding why we do so can be enlightening.\nThis is not an exchange, it’s a competition. The goal of a discussion should not be to win the argument. It is not a battle, it is an exchange of ideas. Given 1) I convince you that my way of thinking makes sense and 2) If and only if my rationale is correct; Then we both win. Otherwise, it’s a zero-sum game, or even worse, we all lose.\nWinner takes all mentality. This one is very related to the previous one, but it is more dangerous. Even with a winning mentality, whoever wins does not need to be crowned master and owner of all. We need not argue against an opponent nor we need to completely destroy our interlocutor.\nYou can talk but I’m not listening. Listening does not mean that you have to agree with what it is being said. But it does mean that you are open to considering whatever argument is offered to you as valid, or even true, as long as it is sound. You need a certain mindset to be listening and actively evaluating the arguments presented. You need to be somehow outside of yourself. If any or both parts are so self-centered they cannot listen, there is no discussion. Why? Because it is not possible to have a proper exchange of ideas by taking turns in vomiting a set of words into each another. At that point, it is no longer a discussion, it shifts towards being personal, no longer the idea itself."
  },
  {
    "objectID": "posts/2017-11-30-about-good-discussions/index.html#footnotes",
    "href": "posts/2017-11-30-about-good-discussions/index.html#footnotes",
    "title": "About Good Discussions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nComplex and highly recommended http://www.hup.harvard.edu/catalog.php?isbn=9780674368309↩︎\nNever Split the Difference by Chris Voss https://www.goodreads.com/book/show/26156469-never-split-the-difference↩︎"
  },
  {
    "objectID": "posts/2020-11-02-Russian-roulette/index.html",
    "href": "posts/2020-11-02-Russian-roulette/index.html",
    "title": "Russian roulette",
    "section": "",
    "text": "There are many traders and most of them blow up at some point in their careers1. Maybe that’s why we worship those who don’t. We find rational explanations for their success, which basically boil down to saying they are smarter/faster/bolder/better than everyone else. We may even go as far as justifying success at all costs.\nI’m sure there’s a lot of skill involved in playing dice with such great results over such a long period of time. And I’m not writing this to indicate there’s no such thing as ability involved in long-term successful investing. However, we know that things are never 100% skill and I am curious about the role of luck in performing a series of successful trades. In other words, what is the portion of luck involved in the process? There’s a ton of traders out there [citation needed], shouldn’t we expect the emergence of at least one Warren Buffet by chance?"
  },
  {
    "objectID": "posts/2020-11-02-Russian-roulette/index.html#warren-buffet-and-the-russian-roulette",
    "href": "posts/2020-11-02-Russian-roulette/index.html#warren-buffet-and-the-russian-roulette",
    "title": "Russian roulette",
    "section": "Warren Buffet and the Russian Roulette",
    "text": "Warren Buffet and the Russian Roulette\nWarren Buffet is quite often considered unique in the pool of people who operate on the markets. But, what does unique mean? In other words, how likely is that we find a Warren Buffet?"
  },
  {
    "objectID": "posts/2020-11-02-Russian-roulette/index.html#russian-roulette-quite-good-odds.",
    "href": "posts/2020-11-02-Russian-roulette/index.html#russian-roulette-quite-good-odds.",
    "title": "Russian roulette",
    "section": "Russian Roulette: quite good odds.",
    "text": "Russian Roulette: quite good odds.\nLet’s play with some odds here. The Russian Roulette is that game in which you put a bullet on a six chamber revolver, you spin the barrel and pull the trigger, hoping that your attempt does not blow your brains. Not quite like taking a position on a stock, but not much different.\nI will focus on the first event, the first trigger after the spin. If it turns out that the bullet is fired in the first trigger, the outcome is terrible. In other words, failure in this game equals total unrecoverable loss. However, the odds of survival are not so bad. Assuming the spin of the barrel is mostly governed by random processes, the odds of the bullet firing are \\(\\frac{1}{6}\\) , meaning that the survival chances are \\(\\frac{5}{6}\\). I’m quite sure the current financial system allows, and maybe encourages, traders to take higher risks of blowing up. And my guess is they sleep comfortably at night."
  },
  {
    "objectID": "posts/2020-11-02-Russian-roulette/index.html#one-million-roulettes",
    "href": "posts/2020-11-02-Russian-roulette/index.html#one-million-roulettes",
    "title": "Russian roulette",
    "section": "One million roulettes",
    "text": "One million roulettes\nLet’s imagine we have 1 million people playing (aka, 1 million traders, comfortably spinning barrels and triggering over and over). Let’s assume that the events are independent, so traders don’t affect each other and each time traders spin the barrel we have full randomness in place. The equation giving the number of survivors (S) over time is:\n\\(S(t) = 10^6 * (\\frac{5}{6})^t\\)\nWe can ask ourselves: how long does it take until there’s only one standing? This last trader, the survivor of all trades, will be our Warren Buffet.\n\\(S(t = t_{x}) = 10^0 = 10^6 * (\\frac{5}{6})^{t_{x}}\\)\n\\(10^{-6}= (\\frac{5}{6})^{t_x}\\)\n\\(\\frac{-6}{log_{10}(5/6)}= t_x\\)\n\\(75.9 \\approx t_x\\)\nNow we see that after 76 rounds (or trades) we are expected to have one survivor who was lucky enough to keep the bullet on the revolver and therefore their brain intact. This level of lucky is extremely profitable in a leveraged, compounding environment. It pays well to be the last one standing on the graph below.\n\n\n\n\n\n\n\n\n\n\nDependency\nI have assumed, just for illustration purposes, that events are random and independent. That is, traders don’t influence each other and the guns spinning and are identically dominated by chance. But we know that’s not a realistic picture of society.\nWe live and experience events in sequence. Our reason operates to explain the present using the previous conditions, even if the only real explanation is luck. Moreover, luck is nonlinear in the sense that it can create a qualitative difference. Once labeled a winner, it’s awful hard to lose that label, even if actively trying. People assume that you know something, you see something they don’t. You can profit from the hype, it increases your chances of survival. Luck begets more luck.\nLuck begets help. The winners of just a few rounds can enjoy special treatment. For example, they would be allowed to borrow at better conditions, the equivalent of playing Russian roulette with somebody else’s brain on the line. They would receive hoards of people willing to put their brains in front of the bullet in the hopes of sharing part of the proceeds. And they would have all the incentives to keep happily playing even more riskier versions of Russian roulette."
  },
  {
    "objectID": "posts/2020-11-02-Russian-roulette/index.html#footnotes",
    "href": "posts/2020-11-02-Russian-roulette/index.html#footnotes",
    "title": "Russian roulette",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA less pernicious version are those fund managers that loose money in comparison to an index fund such as the SP500 and charge a handsome fee to do so.↩︎"
  },
  {
    "objectID": "posts/2022-10-10-Inequality/index.html",
    "href": "posts/2022-10-10-Inequality/index.html",
    "title": "Inequality",
    "section": "",
    "text": "I love Future Crunch. The idea of receiving positive news that I wouldn’t otherwise get is fantastic and I appreciate the work they do. However, during one of the latest editions I got a pyramid plot coming from Credit Suisse which was really hard to read. Moreover, the people who made the plot were trying to make the case that the world is a better place, because there is less inequality.\nYou are welcome to try to interpret the plot on its source, here’s the link again. Because I think the representation is lacking, I do want to produce my own version using their data (aka, the values provided in their plot). Here’s a basic table for their data:\nBracket\nBracket2\nYear\nPeople (M)\n\n\n\n\n&lt;10K\n&lt;10K\n2011\n3054.0\n\n\n&lt;10K\n&lt;10K\n2021\n2818.0\n\n\n10K-100K\n10K+\n2011\n1066.0\n\n\n10K-100K\n10K+\n2021\n1791.0\n\n\n100K-1M\n10K+\n2011\n369.0\n\n\n100K-1M\n10K+\n2021\n627.0\n\n\n1M+\n10K+\n2011\n29.7\n\n\n1M+\n10K+\n2021\n62.5"
  },
  {
    "objectID": "posts/2022-10-10-Inequality/index.html#why-i-dont-like-the-pyramid-plot",
    "href": "posts/2022-10-10-Inequality/index.html#why-i-dont-like-the-pyramid-plot",
    "title": "Inequality",
    "section": "Why I don’t like the pyramid plot",
    "text": "Why I don’t like the pyramid plot\n\nPyramids have angles, I strive to stay away from angles.\nThe x axis is reversed.\nAll quantities are changing but there are horizontal levels trying to guide us. I think they do more harm than good.\n\nThese horizontal levels play the role of a y axis, only to add confusion because the scale is logarithmic."
  },
  {
    "objectID": "posts/2022-10-10-Inequality/index.html#enter-waffle",
    "href": "posts/2022-10-10-Inequality/index.html#enter-waffle",
    "title": "Inequality",
    "section": "Enter Waffle",
    "text": "Enter Waffle\nI think Waffle plots are a fantastic alternative to all the nasty pie and pyramid plots. The representation of fractions is clear and explicit, and there are no angles that mess things up. I kept their colors to make it as similar as possible. Note that the world’s population has increased, so 1 square (or 1% of the population) is different on the 2011 and 2021 plots."
  },
  {
    "objectID": "posts/2022-10-10-Inequality/index.html#the-case",
    "href": "posts/2022-10-10-Inequality/index.html#the-case",
    "title": "Inequality",
    "section": "The case",
    "text": "The case\nThe case that people are trying to make is: “Good News! There’s more people in the wealthier categories!”. For sure, we can say that these numbers indeed point towards the right direction. Is it fast enough? That depends on how much expectations you had for the global change we can achieve in one decade. If anything, things are better, but we have much room for improvement.\nIn the following plot, I tried to focus attention on the bottom bracket. If we manage to get people out of the bottom bracket, that would be a huge triumph for global development.\n\nWe are making progress. Progress is slow, but the share of people living in the bottom bracket is decreasing. However, looking at these plots, I cannot help but notice the immense task we have ahead: we still have to lift half of the planet out of this bracket. This challenge is something I wanted to convey, and I think it’s more evident in these waffle than the pyramid plots. I think Hans, put it better than anyone:\n\nThings can be bad, and getting better. – Hans Rosling"
  },
  {
    "objectID": "posts/2022-10-10-Inequality/index.html#the-downside",
    "href": "posts/2022-10-10-Inequality/index.html#the-downside",
    "title": "Inequality",
    "section": "The downside",
    "text": "The downside\nThere are downsides of using Waffle Plots. I’m quoting from a nice person who gave me feedback directly, they said: “One disadvantage of the waffle plot is a lack of precision. For instance, your version doesn’t show the change at the top from .5% to 1.2% (more than double!)”.\nI was somewhat saving this idea for another plot. What idea? The idea that all wealthier brackets increased by a lot (~2X!). I decided to go with a bar plot showing the absolute number of people. This is because I want to keep numbers in terms of people, real human beings that belong to each bracket. Each person that we move is a lot, and we have a lot to do!\n\n\n\n\n\n\n\n\n\nThis is not the only issue with waffle plots. As I said before, the wealth brackets here are logarithmic, and the highest bracket is virtually infinite. Counting methods (aka, counting how many people belong to each bracket), can go far to give a sense of the overall distribution, but cannot bring a picture of the massive differences in wealth between the higher and the lower brackets when the axis is discretized to condense a logarithmic scale. Looking further the actual wealth range exceeds my intentions for this quick makeover, but I couldn’t help to mention it."
  },
  {
    "objectID": "posts/2018-05-27-github-style-calendar-heatmap/index.html",
    "href": "posts/2018-05-27-github-style-calendar-heatmap/index.html",
    "title": "Github style calendar heatmaps",
    "section": "",
    "text": "I like how the commit heatmap looks in Github. I wanted to play with something that could be plotted that way. I’ve seen some beautiful things done in d3 and javascript. But, of course, I wanted to make it in R. Turns out a bunch of other people have great ideas for how to go about it. Thus, I’m borrowing heavily from them1."
  },
  {
    "objectID": "posts/2018-05-27-github-style-calendar-heatmap/index.html#the-commit-heatmap",
    "href": "posts/2018-05-27-github-style-calendar-heatmap/index.html#the-commit-heatmap",
    "title": "Github style calendar heatmaps",
    "section": "",
    "text": "I like how the commit heatmap looks in Github. I wanted to play with something that could be plotted that way. I’ve seen some beautiful things done in d3 and javascript. But, of course, I wanted to make it in R. Turns out a bunch of other people have great ideas for how to go about it. Thus, I’m borrowing heavily from them1."
  },
  {
    "objectID": "posts/2018-05-27-github-style-calendar-heatmap/index.html#loading-packages",
    "href": "posts/2018-05-27-github-style-calendar-heatmap/index.html#loading-packages",
    "title": "Github style calendar heatmaps",
    "section": "Loading packages",
    "text": "Loading packages\nWe will need a few packages to generate this plot.\n\n\nShow the code\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(viridis)  # Color palette\nlibrary(ggthemes) # theme tufte"
  },
  {
    "objectID": "posts/2018-05-27-github-style-calendar-heatmap/index.html#the-data",
    "href": "posts/2018-05-27-github-style-calendar-heatmap/index.html#the-data",
    "title": "Github style calendar heatmaps",
    "section": "The data",
    "text": "The data\nLet’s generate a data.frame for May 2018. We want the date as datetime and we also want to extract values from that date (month, year, week, …).\n\n\nShow the code\n# choose dates\nstart_date &lt;- ymd(\"2018-05-01\")\nend_date &lt;- ymd(\"2018-05-31\")\n\nd &lt;- tibble::tibble(\n    date = seq(start_date, end_date, by = \"days\"),\n    month = month(date),\n    year = format(date, \"%Y\"),\n    week = as.integer(format(date, \"%W\")) + 1,  # Week starts at 1\n    day = factor(weekdays(date, T), \n                 levels = rev(c(\"Mon\", \"Tue\", \"Wed\", \"Thu\",\n                                \"Fri\", \"Sat\", \"Sun\"))))\n\n\nThis is how the data we generated looks like:\n\n\nShow the code\nhead(d)\n\n\n# A tibble: 6 × 5\n  date       month year   week day  \n  &lt;date&gt;     &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt;\n1 2018-05-01     5 2018     19 Tue  \n2 2018-05-02     5 2018     19 Wed  \n3 2018-05-03     5 2018     19 Thu  \n4 2018-05-04     5 2018     19 Fri  \n5 2018-05-05     5 2018     19 Sat  \n6 2018-05-06     5 2018     19 Sun  \n\n\nNow, let’s assume I registered some events in my life and that my data looks something like: A date column date, and the number of events that happened on a particular date (n).\nAgain, here’s how the data looks like.\n\n\nShow the code\ndf\n\n\n# A tibble: 13 × 2\n   date           n\n   &lt;date&gt;     &lt;int&gt;\n 1 2018-05-15     1\n 2 2018-05-16     1\n 3 2018-05-17     1\n 4 2018-05-18     2\n 5 2018-05-19     4\n 6 2018-05-20     2\n 7 2018-05-21     2\n 8 2018-05-22     2\n 9 2018-05-23     2\n10 2018-05-24     5\n11 2018-05-25     2\n12 2018-05-26     1\n13 2018-05-27     4\n\n\nI can join both data.frames and visualize!\n\n\nShow the code\ndf_plot &lt;- d %&gt;% left_join(df, by = \"date\") \n\ndf_plot %&gt;%\n  mutate(n=ifelse(is.na(n), 0, n)) %&gt;% ## Fill the NAs with zeros\n  ggplot(aes(date, n)) +\n  geom_line(lwd=0.7)+\n  geom_point(size=2, shape=21, fill=\"black\", colour=\"white\", stroke=2)+\n    theme_classic() +\n  theme(panel.background = element_rect(colour = \"black\"))+\n  ylab(\"Number of events\")\n\n\n\n\n\n\n\n\n\nMy goal is not to analyze long term trends like seasonality. Thus, this plot is rather unremarkable. Not only because it is a small toy-like dataset, but because it fails to inform calendar information. Let’s try to make it better!"
  },
  {
    "objectID": "posts/2018-05-27-github-style-calendar-heatmap/index.html#abstracting-into-functions",
    "href": "posts/2018-05-27-github-style-calendar-heatmap/index.html#abstracting-into-functions",
    "title": "Github style calendar heatmaps",
    "section": "Abstracting into functions",
    "text": "Abstracting into functions\nA good way of improving the procedure is to abstract things into a function we can call calendar_plot().\n\n\nShow the code\ncalendar_plot &lt;- function(data, color.scale = \"viridis\",\n                          viridis.pal = \"D\", dir = 1){\n  \n  p &lt;- ggplot(data, aes(x = week, y = day, fill = n)) +\n    geom_tile(color = \"white\", size = 0.8) +\n    facet_wrap(\"year\", ncol = 1) +\n    theme_tufte() +\n    theme(axis.ticks = element_blank(),\n          legend.position = \"bottom\",\n          legend.key.width = unit(1, \"cm\"),\n          strip.text = element_text(hjust = 0.01,\n                                    face = \"bold\", size = 12),\n          text = element_text(size=16)) + \n    ylab(\"\")\n  \n  \n  \n  # Let's add more than one possible pallete. Default keeps being viridis\n  # Add case switch? or add 'none' for user to define their own ?\n  \n  if(color.scale==\"viridis\"){\n    \n    \n    p &lt;- p + scale_fill_viridis(name=\"Number of Events\", \n                                # Variable color palette\n                                option = viridis.pal,  \n                                # Variable color direction\n                                direction = dir,  \n                                na.value = \"grey93\",\n                                limits = c(1, max(data$n)))\n    \n  } else if(color.scale == 'greens') {\n    \n    p &lt;- p + \n      scale_fill_gradient(name=\"Number of Events\",\n                          low=\"lightyellow2\", \n                          high=\"darkgreen\", \n                          na.value = \"grey93\")\n  } else{\n    \n    error(\"Accepted color.scale are 'viridis' and 'greens'\")\n    \n  }\n  \n  \n  ## x axis control of labels\n  \n  \n  num_months &lt;- length(unique(data$month))\n  \n  if(num_months &gt; 1){\n    \n    p &lt;- p + scale_x_continuous(\n      expand = c(0, 0),\n      breaks = seq(1, 52, length = 12),\n      labels = c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\",\n                 \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"))\n    \n  } else {\n    \n    # do nothing\n    \n    p &lt;- p + xlab(\"Week Number.\")\n    \n  }\n  \n  \n  \n  print(p)\n}\n\n\nWe can use calendar_plot() function now to make a plot in calendar-like shape. It is easier to see the data, even with such as small dataset. Below there are two color scale representations of the same data.\n\n\nShow the code\ncalendar_plot(df_plot, 'greens')\n\n\n\n\n\n\n\n\n\nShow the code\ncalendar_plot(df_plot, viridis.pal = \"B\")"
  },
  {
    "objectID": "posts/2018-05-27-github-style-calendar-heatmap/index.html#update",
    "href": "posts/2018-05-27-github-style-calendar-heatmap/index.html#update",
    "title": "Github style calendar heatmaps",
    "section": "Update",
    "text": "Update\nI was curious about how data would look like for a longer span. Here’s the data for a longer time interval.\n\n\nShow the code\ncalendar_plot(df_plot)\n\n\n\n\n\n\n\n\n\nBy no means this is a perfect function and is far from tested. For example, when I did this update, I realized that my calendar_plot() function should handle internally the creation of the data.frame named d that serves as a placeholder. I guess that will happen in a following update :)."
  },
  {
    "objectID": "posts/2018-05-27-github-style-calendar-heatmap/index.html#footnotes",
    "href": "posts/2018-05-27-github-style-calendar-heatmap/index.html#footnotes",
    "title": "Github style calendar heatmaps",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGreat resources here, and here↩︎"
  },
  {
    "objectID": "posts/2018-09-05-writing-challenge-day-7/index.html",
    "href": "posts/2018-09-05-writing-challenge-day-7/index.html",
    "title": "Writing challenge Day 7",
    "section": "",
    "text": "This week, I did a couple of new things:\nOut of these three things, writing has proven to be the most challenging.\nHowever, it’s increasingly building into a good habit. I can feel somewhere in my brain a voice that tells me “we have to write today”. Even though I was not planning to write the article today, I ended up going for it anyways. As it turns out, it has been a week since I started, so I should just go to report it.\nHere’s how the first week looks like:\ndate\ntime_start\ntime_end\nproject_name\ntype_of_writing\nword_count\n\n\n\n\n2018-08-30\n09:35\n09:55\nblog post\nfirst draft\n390\n\n\n2018-08-30\n21:22\n22:47\nshort story\nfirst draft\n897\n\n\n2018-09-01\n08:35\n10:00\nshort story\noutlines\n700\n\n\n2018-09-02\n22:16\n23:59\nnovel\nfirst draft\n800\n\n\n2018-09-03\n00:00\n00:32\nnovel\nfirst draft\n300\n\n\n2018-09-03\n19:50\n21:03\nshort story\nfirst draft\n1323\n\n\n2018-09-04\n22:00\n23:05\nshort story\nfirst draft\n1038"
  },
  {
    "objectID": "posts/2018-09-05-writing-challenge-day-7/index.html#thoughts",
    "href": "posts/2018-09-05-writing-challenge-day-7/index.html#thoughts",
    "title": "Writing challenge Day 7",
    "section": "Thoughts",
    "text": "Thoughts\nSome early but general comments/thoughts about the challenge:\n\nI appear to be more willing to write after dinner and late into the night. This might not be because I am naturally better at this moment, it’s just better for my schedule to be expedite in the morning (the time between wake-up and the moment I leave is normally less than an hour). This decision leaves no room for writing in the mornings except for weekends.\nWriting in the morning can feel great too! Indeed, on September 1st I wrote during the morning and it was great. I knew I was going to be literally dead after the move, so I allowed myself to be happy with my small writing after breakfast. It didn’t feel weird to write in the morning at all, and I was full of energy after a pretty charged breakfast.\nI am always below the word count goal. My current writing reaches somewhere between 1000-1300 a day, which gets considerably easier if I know what I’m gonna be writing about, or if I have a somewhat detailed outline. On the contrary, being tired (like right now when I’m writing this first draft) makes it really tough to find creative paths. Maybe that’s why I chose a report-like structure for this post. I am not super concerned because I already feel some improvements and because 2000 words a day is a lot. It’s a nice goal to have and I can probably do it sooner than later. I can probably also run 5K a day too, and bake a cake after that, and cook dinner, and never sleep?"
  },
  {
    "objectID": "posts/2018-09-05-writing-challenge-day-7/index.html#the-setup",
    "href": "posts/2018-09-05-writing-challenge-day-7/index.html#the-setup",
    "title": "Writing challenge Day 7",
    "section": "The setup",
    "text": "The setup\nI have a fair amount of ongoing projects. The ones that are especially out of control are the fiction stories/novels I write for fun. I have been thinking about closing all those loops for a long time, having the unfinished folder uncluttered would be beautiful. These texts are partially written but in a disorganized manner that makes it hard to keep track of. Hopefully with time and the habit of writing words into text consistently they will become finished."
  },
  {
    "objectID": "posts/2018-09-05-writing-challenge-day-7/index.html#the-good",
    "href": "posts/2018-09-05-writing-challenge-day-7/index.html#the-good",
    "title": "Writing challenge Day 7",
    "section": "The good",
    "text": "The good\nThis challenge set me up in a good mood regarding my writing. I regained encouragement towards writing and confidence towards being able to reduce the number of unfinished pieces. I also feel words coming easily (and it works for Spanish too!).\nShort stories are easier to write. These take less time to outline and develop into scenes, stakes are lower and I feel them flow easier through my brain. The relatively low amount of characters makes it easy to keep track of. The stats book and novels are harder. These need more research and planning, pruning of the content. For example, triple checking the code makes sense and graphs show the way you want or that after x amount of words.\nI decided to go full Markdown from now on. Webpage and stats book are already .Rmd, some other note-taking things are .md too. I changed my .docx short stories into .md. I discovered an awesome text editor (Sublime text 3). It’s everything I wanted a text editor to be. I am pretty happy with Rstudio’s editor for .Rmd files but I find Sublime to be awesome1."
  },
  {
    "objectID": "posts/2018-09-05-writing-challenge-day-7/index.html#the-bad",
    "href": "posts/2018-09-05-writing-challenge-day-7/index.html#the-bad",
    "title": "Writing challenge Day 7",
    "section": "The bad",
    "text": "The bad\nI am still not organized when writing. I don’t stick to the correct stages. I jump from outline to completion of the idea. Investing more into thinking about and writing proper outlines can save tremendous amount of energy/time and propel writing during the first draft.\nI took the first project at random and I did not finish that one project before continuing with another (and yet another one). It was a two-fold scenario: I felt blocked after writing a bit of the first project and all of a sudden better ideas appeared for the other projects. I did stop at 3 (out of the 30). I guess I still need to work on the completion part before jumping into new."
  },
  {
    "objectID": "posts/2018-09-05-writing-challenge-day-7/index.html#footnotes",
    "href": "posts/2018-09-05-writing-challenge-day-7/index.html#footnotes",
    "title": "Writing challenge Day 7",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI actually went into it because I really wanted something that could handle .csv files without changing them (Excel would do whatever it wants but keep the text as is). I had the “somebody must have solved this issue” feeling for so long, and now I have Sublime, pretty happy about it :).↩︎"
  },
  {
    "objectID": "posts/2024-01-12-recoding-america/index.html",
    "href": "posts/2024-01-12-recoding-america/index.html",
    "title": "Recoding America",
    "section": "",
    "text": "In order to continue my 2023 challenge, I hoarded several books written by women. Recoding America, by Jennifer Pahlka, was sitting in my queue waiting for the right moment, and I felt January 2024 was as good as it gets to dive into it. My hopes were high, and the author delivered above and beyond.\nRecoding America is jammed with examples of the good, the bad, and the ugly when it comes to doing technology work in the Government. The prose is clear and the chosen examples illustrate the point the author is trying to make. For a non-fiction book, it’s a page turner and kept me reading wide awake at night. Even when the events narrated imply terrible things happening to terrible people, there is an optimistic tone carrying you forward and giving you hope.\nThere’s just too much to unpack and I feel I wouldn’t be able to do a good job at it (i.e., go read the book, it’s fantastic!). But I wanted to keep some reflections/notes from my highlights with a few major points.\n\n\n\nImage Generated with GPT"
  },
  {
    "objectID": "posts/2024-01-12-recoding-america/index.html#a-strong-start",
    "href": "posts/2024-01-12-recoding-america/index.html#a-strong-start",
    "title": "Recoding America",
    "section": "",
    "text": "In order to continue my 2023 challenge, I hoarded several books written by women. Recoding America, by Jennifer Pahlka, was sitting in my queue waiting for the right moment, and I felt January 2024 was as good as it gets to dive into it. My hopes were high, and the author delivered above and beyond.\nRecoding America is jammed with examples of the good, the bad, and the ugly when it comes to doing technology work in the Government. The prose is clear and the chosen examples illustrate the point the author is trying to make. For a non-fiction book, it’s a page turner and kept me reading wide awake at night. Even when the events narrated imply terrible things happening to terrible people, there is an optimistic tone carrying you forward and giving you hope.\nThere’s just too much to unpack and I feel I wouldn’t be able to do a good job at it (i.e., go read the book, it’s fantastic!). But I wanted to keep some reflections/notes from my highlights with a few major points.\n\n\n\nImage Generated with GPT"
  },
  {
    "objectID": "posts/2024-01-12-recoding-america/index.html#implementation",
    "href": "posts/2024-01-12-recoding-america/index.html#implementation",
    "title": "Recoding America",
    "section": "Implementation",
    "text": "Implementation\nIf there was one take home message from this book, is that we should stop arguing about policies, and only argue about implementation1. Even when specific laws pass, what is the actual effect of these laws? Who ends up being served by them? What conflicts do they generate? It’s about real people dealing with real systems, not some abstract agents in a game theoretic scenario. The author states that this is an ever present reality in the policy-making world.\nIt’s funny to see the “Thinkers vs Doers” dilemma playing in Government. But maybe I shouldn’t be surprised since I have experienced it elsewhere. I find it strange that we want to build these hierarchies everywhere we go. Maybe they arise as a natural result of specialization of labor, with domain experts being really good at one thing and not the other? It’s hard to imagine we can do anything anywhere without a fluid dialogue between these two camps. It’s sad to see Government entangled in these infantile quests.\nMost times that there’s a contentious debate at home/work/whatever, I have tried to have discussions about implementation with people. I find that people often don’t want to discuss implementation. We are stuck on whether we should do an idealized version of something or not. Or whether any hot topic should be a right provided by Government or not.\nIn my mind, we should make the debate more practical: what’s the current state of affairs and how our implementation modifies that state of affairs? I’ve seen it split into these bullet points before:\n\nWhere are we now? People have facts and alternative facts.\nWhere are we going? There’s as many opinions as people alive.\nHow do we get from here to there? Nobody wants to talk about this\n\nI feel we are stuck on what’s the true nature of the world and where do we want to take it. We spend far less time talking about the path to get there. In the meantime we implement the policy best known as no policy or the famous old policy that got us here in the first place. Even when we try to implement new policy, we forget to measure and engage with that implementation, or to adjust it when it fails.\nRecoding America is filled with examples of people working to improve this situation. I see fertile ground here. I feel optimistic for change.\n\nOther Core Concepts\nThe author makes a very nice bite-sized presentation of the core principles of this book. You can read them in the original format here. But I cannot stop mentioning two that I find true for any organization.\n\nCulture eats policy: whatever policy you try to implement,\nIt has to make sense to a person: stop it with the legalese, talk to humans."
  },
  {
    "objectID": "posts/2024-01-12-recoding-america/index.html#destroying-the-myth",
    "href": "posts/2024-01-12-recoding-america/index.html#destroying-the-myth",
    "title": "Recoding America",
    "section": "Destroying the myth",
    "text": "Destroying the myth\nOne myth about the people working in Government is that they are useless sloths who don’t know better. These creatures have only one pastime: to make everyone’s lives more difficult. Jennifer Pahlka’s book serves as proof that, if anything, people working in the Government are actually super qualified and doing their jobs diligently. In fact, they are held to such a degree of accountability, that this eagerness to comply is what gets them into trouble.\n\n\n\nThe Simpsons S10 E7 “Lisa Gets an ‘A’”\n\n\nWhen coupled with “Big Government” fears, the sloth myth supercharges the demands to make Government small. Note that I said small, and not efficient. It seems to me that campaigns are always looking to get Government out of places, instead of improving how it works and how we relate to it. Recoding America brings a compelling case against these myths. Here’s the gist of it:\n\nWhen implementing policy, any divergence from the law will be met with endless lawsuits.\nTherefore, government employees want to comply with the law and all the requirements for each project. It’s not about exercising judgement, it’s about extreme compliance with all possible scenarios. It’s about being evaluated, promoted, and fired for how well you do what you are told to do. Even if it is stupid or doesn’t help anybody.\nSolving for all possible cases implies a level of complexity that doesn’t serve anyone and creates maintenance challenges. Moreover, it increases the costs for both the citizens, who need to pay to interact with Government in a sane way2, and the Government itself, who has to request contractors fulfill huge systems for very simple things. Most importantly, it alienates those citizens in need, who cannot afford to pay for others to deal with the Government in their place.\n\nThat is the essence of “Big Useless Government” right there. Because all projects must be massive, and “Small Government” demands keep pressuring Government to outsource, the Government doesn’t have the in-house talent to deal with any projects. As a result, Government ends up spending way more than it should.\n\nSmall Government demands beget Big Government Spending"
  },
  {
    "objectID": "posts/2024-01-12-recoding-america/index.html#write-less-do-more",
    "href": "posts/2024-01-12-recoding-america/index.html#write-less-do-more",
    "title": "Recoding America",
    "section": "Write Less do More",
    "text": "Write Less do More\nWe humans have this tendency to always be on the look out to adding things. We are much more accustomed to trying to solve problems by adding than subtracting. In politics, that translates to legislating the hell out of things, which leads to a law base with many regulatory requirements.\nBecause we have been writing laws for a while, but it’s not trivial to keep count of which regulations conflict with each other. It’s also not the case that people in Government remove outdated stuff (i.e., the task is far from trivial and not mandated). I find the argument regarding vast simplification of laws by reducing quite compelling."
  },
  {
    "objectID": "posts/2024-01-12-recoding-america/index.html#wrap-up",
    "href": "posts/2024-01-12-recoding-america/index.html#wrap-up",
    "title": "Recoding America",
    "section": "Wrap-up",
    "text": "Wrap-up\nAlthough I had previously read Hack Your Bureaucracy, by Marina Nitze and Nick Sinai, I consider myself quite ignorant in all things government. Both books are filled with concrete examples, practical solutions, and real experiences with people. Both books highlight the importance of culture over tech, which is quite refreshing coming from technologists. Something that Recoding America leaves you with is the feeling that we cannot bail out. Regardless of the current situation, we have to play our part. If we belong to those who know technology, we should consider serving directly in Goverment. As citizens, we are the ones who can serve as agents of change. It is our collective responsibility.\nI see fertile ground here. I feel optimistic for change."
  },
  {
    "objectID": "posts/2024-01-12-recoding-america/index.html#footnotes",
    "href": "posts/2024-01-12-recoding-america/index.html#footnotes",
    "title": "Recoding America",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nExaggeration is mine. It’s more like switch from 99.9% policy/law/legalese 0.01% implementation to something substantially more balanced↩︎\nFor example, when filing taxes↩︎"
  },
  {
    "objectID": "posts/2018-09-20-writing-challenge-day-21/index.html",
    "href": "posts/2018-09-20-writing-challenge-day-21/index.html",
    "title": "Writing challenge Day 21",
    "section": "",
    "text": "Third week has had ups and downs. I finished another short story. But I keep losing the word count battle. Yes, again, work."
  },
  {
    "objectID": "posts/2018-09-20-writing-challenge-day-21/index.html#the-data",
    "href": "posts/2018-09-20-writing-challenge-day-21/index.html#the-data",
    "title": "Writing challenge Day 21",
    "section": "The data",
    "text": "The data\nThe word count data is somewhat similar to what I’ve seen for last week. Definitely writing bouts are going down, just being able to write a few words before bedtime knocks me over.\n\n\n\n\n\n\n\n\n\nLooking at the aggregate, the downward slope of the third week is a little bit disappointing. We will see if I recover my energies for this following week and start going up again.\n\n\n\n\n\n\n\n\n\nActually, while Tuesday and Wednesday are my weakest days, it seems that my output ramps up from Thursday into the weekends, which are definitely highest. Surprisingly, Mondays are quite good!1"
  },
  {
    "objectID": "posts/2018-09-20-writing-challenge-day-21/index.html#the-good",
    "href": "posts/2018-09-20-writing-challenge-day-21/index.html#the-good",
    "title": "Writing challenge Day 21",
    "section": "The good",
    "text": "The good\nI finished another short story and I am quite close to finishing two more. The words are still there, the desires are still there, I begin to imagine actually compiling the work into something that is published.\nI started exploiting the the power of the outline. It happened with a blog post I’m writing for my experiences with Skype a Scientist (stay tuned). And it also happened with my short stories. Markdown to the rescue, making sub-headers with main ideas, developing paragraphs and erasing later (during a second draft).\nI tried writing on the bus on the way home. It’s not impossible, it is actually somewhat OK for outlines, sketching things around in a 20 minute trip can go a long way. I would definitely not recommend that for big time writing, I find I need to be still and focused to write on a computer. Paradoxically, I have a lot of good ideas on the move."
  },
  {
    "objectID": "posts/2018-09-20-writing-challenge-day-21/index.html#the-bad",
    "href": "posts/2018-09-20-writing-challenge-day-21/index.html#the-bad",
    "title": "Writing challenge Day 21",
    "section": "The bad",
    "text": "The bad\nThe downward trend reflects me collapsing at night. I also skipped a couple of days, so technically I’m close to cheating. It’s really difficult to write when the energy level is zero. This week has also had very long work days and writing during the night is quite difficult. The solution could be sleeping less, but I’m aiming for a solution that is compatible with life."
  },
  {
    "objectID": "posts/2018-09-20-writing-challenge-day-21/index.html#footnotes",
    "href": "posts/2018-09-20-writing-challenge-day-21/index.html#footnotes",
    "title": "Writing challenge Day 21",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYes, a bar plot. I am really really sorry about that.↩︎"
  },
  {
    "objectID": "posts/2021-01-03-rotating-perspectives/index.html",
    "href": "posts/2021-01-03-rotating-perspectives/index.html",
    "title": "Rotating perspectives",
    "section": "",
    "text": "I have been asked to unveil a bit of what’s under the hood on this post. I decided to make a new post to share how my creative process took place and maybe inspire others to play along.\nSomething interesting about all of this is how well it plays into common sense. Looking at things from a different perspective, in this case adding just a rotation, can yield outstanding results.\n\nThe shapes\nFirst of all, we are going to use the shapes provided by ggforce::geom_regon().\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggforce)\n\ndf &lt;- data.frame(\n  x0=3:8,\n  y0=1,\n  r=0.2\n)\n\nggplot(df)+\n  geom_regon(aes(x0 = x0, y0 = y0,\n                 r = r, sides= x0, angle = 0),\n             fill=\"gray50\", color=\"black\")+\n  coord_equal()+\n  labs(title=\"Regular polygons using ggforce\", \n       x=\"\", y=\"\",\n       caption=\"@NeuroMLA\")+\n  theme(panel.background = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        plot.title = element_text(hjust=0.5))\n\n\n\n\n\n\n\n\n\n\n\nThe twist\nNow we can implement a rotation to each figure. We will use the previous df and expand_grid() to add an angle rotation to the regular polygon. The greater the number of sides, the closer we get to a circular shape illusion when we rotate and overlap the polygons. For n&gt;6 it didn’t generate the type of look I was looking after during my experimentation.\n\n\nCode\ndf &lt;- expand_grid(df, angle = seq(0, 0.5, 0.1))\n\n \nggplot(df)+\n  # notice we use angle = angle now\n  geom_regon(aes(x0=x0,y0=y0,r=r, sides=x0, angle=angle),\n             fill=\"gray50\", color=\"black\")+\n  coord_equal()+\n  labs(title=\"Rotated regular polygons using ggforce\", \n       x=\"\", y=\"\",\n       caption=\"@NeuroMLA\")+\n  theme(panel.background = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(), \n        plot.title = element_text(hjust=0.5))\n\n\n\n\n\n\n\n\n\nWe can tinker with the alpha and fill to make some nice looking shapes. I’m not going to modify color but it’s also a possibility.\n\n\nCode\nggplot(df)+\n  # notice we use angle = angle now and fil=factor(x0)\n  geom_regon(aes(x0=x0,y0=y0,r=r, sides=x0, angle=angle,\n                 fill=factor(x0)), \n             alpha=0.1, color=\"black\")+\n  coord_equal()+\n  labs(title=\"Rotated regular polygons using ggforce\", \n       x=\"\", y=\"\",\n       caption=\"@NeuroMLA\")+\n  theme(panel.background = element_blank(),\n        legend.position = \"none\",\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        plot.title = element_text(hjust=0.5)) -&gt; p\n\nprint(p)\n\n\n\n\n\n\n\n\n\n\n\nMake it pallette\nWe can have unlimited color combinations. Just as a start, two places I like to use when dealing with color pallettes in R:\n\nR Color Brewer Pallettes\nAdobe Color Picker\n\nWe will use scale_fill_*() functions of ggplot. I normally use scale_fill_manual() if I want to handpick the values, but scale_fill_viridis() and scale_fill_brewer() often do a nice job too!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{andina2021,\n  author = {Andina, Matias},\n  title = {Rotating Perspectives},\n  date = {2021-01-03},\n  url = {https://matiasandina.netlify.app/posts/2021-01-03-rotating-perspectives/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndina, Matias. 2021. “Rotating Perspectives.” January 3,\n2021. https://matiasandina.netlify.app/posts/2021-01-03-rotating-perspectives/."
  },
  {
    "objectID": "posts/2018-11-23-query-pubmed-in-r/index.html",
    "href": "posts/2018-11-23-query-pubmed-in-r/index.html",
    "title": "Query Pubmed in R",
    "section": "",
    "text": "I have seen a couple of presentations that use plots of PubMed query results. I have even seen some of them in papers. I just think it’s really cool, so I wanted to play with something that could provide the data.\nA couple of google searches lead me into two nice options to do this in R.\nI went with the custom approach, decided to borrow heavily from Kristoffer’s repo, and did a few modifications here and there. Mainly, I updated libraries, included some dplyr output to make it cleaner, and separated functions into several files.\nYou can find the updated code in the following repo:\nhttps://github.com/matiasandina/pubmed_query\nThe logic of the code is to loop over the search terms and the years, performing queries to PubMed each time. To make things more friendly we wrap everything into a main function that performs some checks and handles the multiple calls to the working functions. This main function, query_pubmed(), expects a query (character vector), and 2 years for the time interval (yrStart and yrMax).\nThe function is somewhat self contained, if it can’t find things on the local computer it will source from GitHub1. Here’s a little demo of the main function query_pubmed(). Since we are using internet to get the data, I assume the user will be able to source from GitHub (these calls are often performed via devtools::source_url)."
  },
  {
    "objectID": "posts/2018-11-23-query-pubmed-in-r/index.html#little-demo",
    "href": "posts/2018-11-23-query-pubmed-in-r/index.html#little-demo",
    "title": "Query Pubmed in R",
    "section": "Little demo",
    "text": "Little demo\nLet’s look for the term hiv in publications from the 1970 until today. PubMed requests us to limit the traffic to ~3 queries per second. Thus, queries will take a while because the function has Sys.sleep(0.5) in between iterations. You will see a progress bar for each term (not shown here for simplicity).\n\n\n\n\n\nPubMed publications containing the term HIV relative to the total number of publications.\n\n\n\n\nI chose to keep the graphic output as simple as possible (aka use ggplot2 defaults) and return a data.frame that can be fed into a custom ggplot2 call later, if the users feel like it. Here’s a glimpse of the returned object.\n\n\n  query_term year count total_count         freq\n1        hiv 1970     1      219426 0.0004557345\n2        hiv 1971     0      223658 0.0000000000\n3        hiv 1972     0      227949 0.0000000000\n4        hiv 1973     0      231159 0.0000000000\n5        hiv 1974     0      235136 0.0000000000\n6        hiv 1975     1      249241 0.0004012181"
  },
  {
    "objectID": "posts/2018-11-23-query-pubmed-in-r/index.html#making-things-faster",
    "href": "posts/2018-11-23-query-pubmed-in-r/index.html#making-things-faster",
    "title": "Query Pubmed in R",
    "section": "Making things faster",
    "text": "Making things faster\nTotal publication numbers should not change2. Thus, if we don’t want to waste time grabbing the total number of publications over and over, we can either:\n\nUse get_totals()\nGet it from GitHub\n\nI will do my best, but I can’t be certain I will keep running the function and pushing once a year to GitHub (as in forever)3. I don’t feel like waiting, I already have a recent version in the repo.\nHaving this object around will speed the main function (it will not query PubMed every year for the totals). Here’s a graph of the number of publications by year:\n\n\n\n\n\nTotal publications in PubMed by year. Science is growing :)"
  },
  {
    "objectID": "posts/2018-11-23-query-pubmed-in-r/index.html#multiple-terms",
    "href": "posts/2018-11-23-query-pubmed-in-r/index.html#multiple-terms",
    "title": "Query Pubmed in R",
    "section": "Multiple terms",
    "text": "Multiple terms\nWe can use multiple terms to query, just make a character vector. For example, let’s add aids and hepatitis b:\n\n\n\n\n\n\n\n\n\nBecause we saved the previous object in the environment, we don’t have to query again, we can merge the data and plot all together.\n\n\n\n\n\nFrequency of query terms. Relative to total number of PubMed entries per year.\n\n\n\n\nWe see that the term aids came first in the literature, before the virus was identified, in the early 1980s. Although strongly correlated with aids, hiv is a term with higher frequency. Research for hepatitis b seems to have kept a constant relative level, growing as much as the total body of research."
  },
  {
    "objectID": "posts/2018-11-23-query-pubmed-in-r/index.html#footnotes",
    "href": "posts/2018-11-23-query-pubmed-in-r/index.html#footnotes",
    "title": "Query Pubmed in R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGranted, several packages are needed to run the code. I assume users will know how to install.packages(...) if in need.↩︎\nThere are some variations in the recent years as the data base updates, but shouldn’t be significant for these purposes.↩︎\nYes, I guess I could automate it but reaching diminishing returns here…↩︎"
  },
  {
    "objectID": "posts/2020-07-05-Unique/index.html",
    "href": "posts/2020-07-05-Unique/index.html",
    "title": "Unique",
    "section": "",
    "text": "I have experienced a few explosions here and there. Yes, it’s me not reading the docs. But it’s also me, assuming that verbs mean one thing and not another. Let’s do some unique operations in different languages.\n\nThe MATLAB way\n&gt;&gt; unique([3, 3, 3, 90, 10])\n\nans =\n\n3 10 90\n\n\nThe Python way\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; np.unique([3, 3, 3, 90, 10])\narray([ 3, 10, 90])\n\n\nThe R way\n\n\n[1]  3 90 10\n\n\nThis is what I would expect. I want the unique values, without the sorting. If I wanted to sort, I would do:\n\n\n[1]  3 10 90\n\n\nMaybe this is just me and R is the odd (read intuitive) ball here, but 🤷…\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{andina2020,\n  author = {Andina, Matias},\n  title = {Unique},\n  date = {2020-07-05},\n  url = {https://matiasandina.netlify.app/posts/2020-07-05-Unique/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndina, Matias. 2020. “Unique.” July 5, 2020. https://matiasandina.netlify.app/posts/2020-07-05-Unique/."
  },
  {
    "objectID": "posts/2017-11-14-how-to-make-a-cake/index.html",
    "href": "posts/2017-11-14-how-to-make-a-cake/index.html",
    "title": "How do you make a cake?",
    "section": "",
    "text": "This post is inspired by my brief experience with the education system in the United States (US) and the way some students approach learning. I do not know what it is. Maybe it has to do with the tsunami of multiple-choice exams they have to go through, or the endless preparation for another standardized test on the horizon, or the glorification of the take-home message culture.\nI wrote a question for an assignment that was pointing towards mechanisms at the level of the synapse that could result in drug tolerance. The question went something like\nAll students had one week to write answers and submit. One student came to talk right after class finished and asked for clarification on the question. The student was confused about what would the expected answer be. As you can see, it is not possible to answer the question by guessing between quite wrong and less wrong options; you have to know what XYZ does and some bits and pieces that make up synapses. Finally, you have to write something coherent.\nIf I was given this question, my first impulse is to go to Google. The original question was a bit more general, so I will not search for a particular drug. Let’s google:\nThis experience gets you towards a good number of resources, some of my favorite are:\nBoth of which have enough material to answer the question. I also want to make a honorable mention to Google Images, there’s plenty cartoon synapses and cascades there.\nBut my experience went a little different. The student was looking for a magic word that would solve the problem. She was looking for naming nodes and not for explaining mechanisms. Actually, it didn’t take long to realize that the student was actually not sure about what a mechanism was."
  },
  {
    "objectID": "posts/2017-11-14-how-to-make-a-cake/index.html#how-do-you-make-a-cake",
    "href": "posts/2017-11-14-how-to-make-a-cake/index.html#how-do-you-make-a-cake",
    "title": "How do you make a cake?",
    "section": "How do you make a cake?",
    "text": "How do you make a cake?\n\nMe: How do you make a cake?\nStudent: You take the list of ingredients and put them in the oven.\n\nThat was not the answer I was looking for. At that moment, I realized how important was for me to receive the kind of education I received. I was taught to look for processes, not to rush for the answer. But the underlying idea of a sequence of steps had to be there, so I looked for it and it appeared:\n\nMe: So…you take the piece of paper, with the list of ingredients and you bake it?\nStudent: No…(rolls eyes, explains better)\n\nHow could it be possible? The student knew what we were talking about and had understanding about the fact that there is a the sequence of events causally interconnected. However, making the same connection for the assignment question did not come as easy. This was a good student, my concern started growing. In other words, I had failed in writing the exercise in terms that were easy to relate, the question was vague and did not point towards a clear path to answer it."
  },
  {
    "objectID": "posts/2017-11-14-how-to-make-a-cake/index.html#a-wave-of-uncooked-cakes",
    "href": "posts/2017-11-14-how-to-make-a-cake/index.html#a-wave-of-uncooked-cakes",
    "title": "How do you make a cake?",
    "section": "A wave of uncooked cakes",
    "text": "A wave of uncooked cakes\nThe truth is that, when the assignments came back, most of the students failed to recognize what the question was aiming towards. Many used phrases like “Dopamine is a mechanism” or “Dopamine receptor is a mechanism”. So we went along and tried to cook the best chocoloate cake {ever}. I believe this exercise was good for working from the concrete up, into more abstract, general ideas of what a mechanism is.\nI assume most of the fault about the wording, but I cannot disregard completely students’ responsibility. My guess is that part of this issue had to do with the way some students approach the exercises. They are looking for the quickest way to solve it, normally a noun to name a node of the network. Flour is an ingredient of cakes, but yelling it to my face does not help me understand how to make one. The question is about how flour gets modified into cake. The question needs you to tell me the story of the ingredients and how they become related to one another.\nThese things don’t come naturally, they are acquired through a lifetime of thinking and rethinking. What concerns me the most are the general education they received; what are the relevant predictors of success in learning? what are the ways to acquire and consolidate knowledge they have been taught? Maybe that’s why there was a mismatch after all."
  },
  {
    "objectID": "posts/2019-05-09-Birthday-Meritocracy/index.html",
    "href": "posts/2019-05-09-Birthday-Meritocracy/index.html",
    "title": "Birthday Meritocracy",
    "section": "",
    "text": "We believe in meritocracy as one of the cornerstones of Western civilization. This idea is so embedded in our culture that we seldom question it. We praise those driven individuals who combine effort and talent to make it to the top. They smile in the cover of the magazines, the dreams of children and the nightmares of adults. Sports might be the activity where the blood, sweat and tears feel the most real and therefore are most notoriously broadcasted to the world.\nThe beautiful game is no exception. But what if we were wrong about the meritocracy assumption? What if there were random events that sneak in the player selection process? What if such events were not negligible, but heavily influenced what players that make it to the top?"
  },
  {
    "objectID": "posts/2019-05-09-Birthday-Meritocracy/index.html#one-trimester",
    "href": "posts/2019-05-09-Birthday-Meritocracy/index.html#one-trimester",
    "title": "Birthday Meritocracy",
    "section": "One trimester",
    "text": "One trimester\nThree or four months of difference at 25 year old is nothing1. Moreover, for most purposes in adult life, such a small difference in age is considered zero. But elite athletes are not ordinary adults and we don’t care about most of their purposes in life. We want them to do one thing extraordinarily well.\nWhat could a trimester difference do to a 25 year old, prime time, football player? You might still find yourself asking this question and this question might sound reasonable. But the data looks quite different. Let’s take a look."
  },
  {
    "objectID": "posts/2019-05-09-Birthday-Meritocracy/index.html#the-database",
    "href": "posts/2019-05-09-Birthday-Meritocracy/index.html#the-database",
    "title": "Birthday Meritocracy",
    "section": "The database",
    "text": "The database\nI compiled a small database of football players from www.soccerwiki.com. This database is not perfectly complete, but contains enough players to provide support to the general idea. Here’s a table of the number of players by country:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCountry\nFootball Players (n)\n\n\n\n\n\n\n\nBrazil\n\n\n\n\nBrazil\n4606\n\n\n\n\n\nUnited Kingdom\n\n\n\n\nUnited Kingdom1\n3967\n\n\n\n\n\nSpain\n\n\n\n\nSpain\n3322\n\n\n\n\n\nItaly\n\n\n\n\nItaly\n3297\n\n\n\n\n\nArgentina\n\n\n\n\nArgentina\n2995\n\n\n\n\n\nFrance\n\n\n\n\nFrance\n2453\n\n\n\n\n\nGermany\n\n\n\n\nGermany\n2038\n\n\n\n\n\nColombia\n\n\n\n\nColombia\n1716\n\n\n\n\n\nUruguay\n\n\n\n\nUruguay\n1316\n\n\n\n1 The data for the United Kingdom only comes from England.\n\n\n\n\n\n\nTotal number of players per country.\n\n\nHere’s a month by month representation of the number of players born in each country.\n\n\n\n\n\n\n\n\n\nThis looks quite strange, there’s a clear trend, with most players being born in the first months of the year. Maybe it’s a seasonal thing?\nWell…It’s quite difficult to support the idea that players born on any season are better. For example, January in Brazil means blazing hot summer while it’s freezing cold winter in Germany. Still, both countries seem to produce the most number of players in the first month of the year. In fact, the data show that being born on the first trimester after the cutoff date (e.g, January to March if cutoff is January 1st) increases the chances of being a top player by a huge margin."
  },
  {
    "objectID": "posts/2019-05-09-Birthday-Meritocracy/index.html#analysis-by-position",
    "href": "posts/2019-05-09-Birthday-Meritocracy/index.html#analysis-by-position",
    "title": "Birthday Meritocracy",
    "section": "Analysis by position",
    "text": "Analysis by position\nThere might be some logic behind this phenomenon. Maybe the birth effect is big for positions most strongly associated with bare physical strength. Thus, we could expect to find a stronger effect for those in defensive positions (i.e, Goalkeeper up to Defensive Midfielders) and find a broader distribution of birth dates for those positions that are reserved for the creatives and magicians of the ball, the offensive players.\n\n\n\n\n\n\n\n\n\nAgain, the evidence for a strong bias favoring players born on the first trimester is compelling. What’s going on here?"
  },
  {
    "objectID": "posts/2019-05-09-Birthday-Meritocracy/index.html#the-one-percent-difference",
    "href": "posts/2019-05-09-Birthday-Meritocracy/index.html#the-one-percent-difference",
    "title": "Birthday Meritocracy",
    "section": "The one percent difference",
    "text": "The one percent difference\nThis is not a post about how the top athletes distinguish themselves from the rest because of their talent and effort. This post is about something completely out of their control: their birth dates. Yes, it sounds crazy, but the month of birth plays a huge role in the success of a football player.\nFootball schools around the world are normally organized around the calendar year. It makes sense, if you are going to make a tournament for kids, instead of having kids of different ages competing together, you grab all the kids born on a same year and make them play against each other (5 year-old kids vs other 5 year-old kids). For example, when I was playing these tournaments, I played in the 1991 category.\nScouts and coaches might decide to promote a strong/talented kid. In my case, I could play for the 1990 or 1989 categories. But we all agree that it would be unfair to downgrade me to play for the 1995 category. I would have a huge developmental advantage. So far so good, but there’s something that escapes from everyone: one year is a lot of time. The developmental difference between a 5 year old kid born at the beginning of the year (e.g, January 1st) vs one at the very end (e.g, December 31st) is huge.\nLet’s take a less extreme case, kids born with a 180 days difference. That amount of days corresponds to an extra fraction of life lived (aka experience, aka time kicking the ball) of 0.1. It might sound like very little but that 10% advantage will definitely compound over the competitive career."
  },
  {
    "objectID": "posts/2019-05-09-Birthday-Meritocracy/index.html#logic-behind",
    "href": "posts/2019-05-09-Birthday-Meritocracy/index.html#logic-behind",
    "title": "Birthday Meritocracy",
    "section": "Logic behind",
    "text": "Logic behind\nIt turns out to be that scouts and coaches think they are selecting the best, but they are selecting the oldest. The January 1st cutoff is arbitrary and favors those born close to the date, who will be slightly stronger/faster/better only because they have had more time to play around in the world and, basically, grow. If you look closely, the only country that does not follow the pattern is the UK. But, yes, you guessed it, the cutoff date for the UK is August 31st.\nPlayers that are selected as the best normally play in the starting team and stay in the field more minutes each game [Experience based, Citation needed]. They get better training and more opportunities to play, either because they play more or because coaches pay more attention to them. Each extra minute reinforces the small initial difference, creating a real one. Of course, talent and effort play a role. But it consistently turns out to be that players born earlier on the calendar year end up being better than the other players. This phenomenon is known as relative age effect 2.\nWhenever we see skewed age distributions we should suspect it is likely due to arbitrary cuts. The magic ingredients go like this:\n\nMake an arbitrary cut.\nSelect those that are a tad ahead, those closest to the cut (older people relative to cut).\nDifferentiate training experience by giving more opportunities to those already ahead.\nHarvest the best players (older people relative to cut).\n\nThis is difficult to detect because it’s a self-fulfilled profecy for scouts (after all, they want their picks to end up being good). Basketball doesn’t have this kind of problem, mostly because availability of opportunities to train and the absence of strict cutoff dates. But in other sports, this effect is even more marked. In ice hockey, you are mostly constrained to the winter season to play and you need a rink, which puts more barriers on training3."
  },
  {
    "objectID": "posts/2019-05-09-Birthday-Meritocracy/index.html#market-value",
    "href": "posts/2019-05-09-Birthday-Meritocracy/index.html#market-value",
    "title": "Birthday Meritocracy",
    "section": "Market Value",
    "text": "Market Value\nI couldn’t scrape enough to make a case about market value. But a quick Google search lead me to a paper suggesting that, although month of birth has a big effect in the frequency of players, once the pros made it, their salaries are not related to such factor 4."
  },
  {
    "objectID": "posts/2019-05-09-Birthday-Meritocracy/index.html#the-bad-news",
    "href": "posts/2019-05-09-Birthday-Meritocracy/index.html#the-bad-news",
    "title": "Birthday Meritocracy",
    "section": "The bad news",
    "text": "The bad news\nThis effect transcends beyond sports. Turns out that arbitrary cutoffs during development affect academic performance. Kids born near the cutoff perform better at school and go to better universities. I cannot stop thinking that I was somewhat fortunate in this account. Overall, school never felt like too difficult. But maybe it’s because I had had enough days on earth and was mature enough to grasp concepts. Maybe it’s not only that “Math and Science came easy to me and I liked them”. Maybe I was born at the correct moment and I wouldn’t be pursuing academic endeavors if I hadn’t been born around the cutoff date in my country (July 1st).\nDividing school kids by age is retrograde. I would like to see a continuous progress with no boundaries on dates. I would like to see no kid be made to wait because he isn’t old enough yet and, more importantly, I would like to see no hurrying kids through content because their age says so. I would like to see people moving through school at their own pace5. Maybe we should start looking at performance adjusted by age, it might be the antidote we are looking for."
  },
  {
    "objectID": "posts/2019-05-09-Birthday-Meritocracy/index.html#the-silverlining",
    "href": "posts/2019-05-09-Birthday-Meritocracy/index.html#the-silverlining",
    "title": "Birthday Meritocracy",
    "section": "The silverlining",
    "text": "The silverlining\nWe can account for this effect and attempt to correct it! Some countries have started to make adjustments to the birth effect in football. An easy way of doing it is using two cutoff dates and training coaches (and scouts) against their bias.\nSolving this issue might require clubs to have two teams per year. This measure doesn’t have to go all the way until they are pro, but at least until kids are old enough, so that we keep below the 6-8 month difference. This will possibly increase the costs for everyone and makes logistics more difficult. However, we would see a huge increase in the production of players (and/or their quality) if we just change this arbitrary dates."
  },
  {
    "objectID": "posts/2019-05-09-Birthday-Meritocracy/index.html#wait-a-second",
    "href": "posts/2019-05-09-Birthday-Meritocracy/index.html#wait-a-second",
    "title": "Birthday Meritocracy",
    "section": "Wait a second",
    "text": "Wait a second\nThere’s a possible hypothesis I’ve been evading. The reasoning will be like this:\n\nThe frequency of Football players is a proportion of the people born in a country.\nThe frequency of births in the whole country is higher in the months that the Football Associations of each country use as cutoffs.\n\nIf both things were true, then that explains why we see a peak of players in January (or whatever the specific month of cutoff date is). I will use data from my country to show that births do not follow Football."
  },
  {
    "objectID": "posts/2019-05-09-Birthday-Meritocracy/index.html#argentina",
    "href": "posts/2019-05-09-Birthday-Meritocracy/index.html#argentina",
    "title": "Birthday Meritocracy",
    "section": "Argentina",
    "text": "Argentina\nMy country is well known for the production and exportation of football players. I had access to data on births in Buenos Aires. I later gained access to the data for the whole country, which showed mostly the same trend (you can see it at the very bottom of the article). The number of births in Argentina are somewhat constant. If anything, March seems to be the month with most consistent high amount of births.\nBirths in Argentina"
  },
  {
    "objectID": "posts/2019-05-09-Birthday-Meritocracy/index.html#footnotes",
    "href": "posts/2019-05-09-Birthday-Meritocracy/index.html#footnotes",
    "title": "Birthday Meritocracy",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt’s actually 1.32 % of life.↩︎\nGreat general info here https://en.wikipedia.org/wiki/Relative_age_effect↩︎\nI cannot help but relate this with Science. Limited resources, too many hands trying to pull a piece to themselves… add on some age effect, differential training/opportunities (quite often bought with money and good connections). We are probably loosing so many great minds.↩︎\nYou can find the paper here↩︎\nSalman Kahn expresses it way better than I could ever do in his TED talk.↩︎"
  },
  {
    "objectID": "posts/2023-08-19-the-power-to-normalize/index.html",
    "href": "posts/2023-08-19-the-power-to-normalize/index.html",
    "title": "The Power to Normalize",
    "section": "",
    "text": "I started participating in the Tidytuesday project to practice my visualization skills, while using datasets that come from sources that I’m not used to. In addition, I enjoy checking what other people do with the same dataset. I find that others are way more creative than I am, and I borrow heavily!\nThe challenge for Week 33 of 2023 was to perform viz on the spam dataset."
  },
  {
    "objectID": "posts/2023-08-19-the-power-to-normalize/index.html#when-pca-fails",
    "href": "posts/2023-08-19-the-power-to-normalize/index.html#when-pca-fails",
    "title": "The Power to Normalize",
    "section": "When PCA fails",
    "text": "When PCA fails\nThe spam dataset presents heavily skewed distributions for variables that serve as predictors of spam email. Because it was a dataset with 6 numeric variables and a single binary predictor, my initial idea was to perform a quick and dirty PCA.\n\n\nCode\nlibrary(tidyverse, warn.conflicts = FALSE)\nlibrary(tidytuesdayR)\nlibrary(paletteer)\nlibrary(FactoMineR)\nlibrary(factoextra)\nlibrary(scales, warn.conflicts = FALSE)\n\n# load the data\nspam &lt;- tt_load(2023, week=33)$spam\n\n\n\n    Downloading file 1 of 1: `spam.csv`\n\n\nCode\nspam$yesno &lt;- dplyr::if_else(spam$yesno == \"y\", \"spam\", \"email\")\npc &lt;- prcomp(spam[, 1:6], center = TRUE, scale. = TRUE)\n# make it a tibble for ggplot plotting\npc_data &lt;- pc$x[, 1:2] %&gt;% as_tibble()\npc_data$yesno &lt;- spam$yesno\n\npc_ori_plot &lt;- ggplot(pc_data, \n       aes(PC1, PC2, color = yesno)) +\n  geom_point() +\n  coord_equal() +\n  scale_color_paletteer_d(\"awtools::a_palette\") +\n  ggthemes::theme_base()+\n  theme(legend.position = \"bottom\",\n        plot.background =  element_rect(color = NA),\n        legend.background = element_rect(fill = \"gray90\"),\n        legend.key = element_rect(fill = \"gray90\"),\n        panel.background = element_rect(fill=\"#81AE5C\")) +\n  labs(color = element_blank())\npc_ori_plot\n\n\n\n\n\n\n\n\n\nIf you are inclined to do so, you can check that fviz_screeplot(pc) gives you a horrible scree plot with very little variance explained and use fviz_pca_contrib(pc, choice = 'var') to check that the contributions of the different variables are also close to random."
  },
  {
    "objectID": "posts/2023-08-19-the-power-to-normalize/index.html#skewed-data-distributions",
    "href": "posts/2023-08-19-the-power-to-normalize/index.html#skewed-data-distributions",
    "title": "The Power to Normalize",
    "section": "Skewed Data Distributions",
    "text": "Skewed Data Distributions\nThe vanilla PCA does nothing to help us visualize a separation between the. Why is that the case?\nUpon a closer inspection of the regular variables, which I should have done before diving into the PCA, we see that we are dealing with heavily skewed distributions\n\n\nCode\nspam %&gt;% \n  pivot_longer(-yesno) %&gt;% \n  ggplot(aes(yesno, value, fill = yesno)) +\n  geom_violin() +\n  facet_wrap(~name, scales = \"free\", nrow=3) +\n  scale_y_continuous(labels = label_number(scale_cut = cut_short_scale()))+\n  scale_fill_paletteer_d(\"awtools::a_palette\") +\n  ggthemes::theme_base() +\n  theme(legend.position = \"bottom\",\n        plot.background =  element_rect(color = NA),\n        legend.background = element_rect(fill = \"gray90\"),\n        legend.key = element_rect(fill = \"gray90\"),\n        panel.background = element_rect(fill=\"#81AE5C\")) +\n  labs(fill = element_blank(), x = element_blank(), y = element_blank())\n\n\n\n\n\n\n\n\n\nThe distributions are so skewed we can barely see them."
  },
  {
    "objectID": "posts/2023-08-19-the-power-to-normalize/index.html#transform",
    "href": "posts/2023-08-19-the-power-to-normalize/index.html#transform",
    "title": "The Power to Normalize",
    "section": "Transform",
    "text": "Transform\nEnter the Yeo–Johnson transformation, a type of Power Transform1 that will come handy to normalize the data.\nAs a side note, I had a bit of trouble running this using the more conventional caret or recipes packages, you can read my StackOverflow question here and the nice answer about estimating parameters properly. For this post, I will be using bestNormalize::yeojohnson to normalize all columns in the dataset.\n\n\nCode\n# quickly apply transformation to the data itself\ndf_transformed &lt;- select(spam, where(is.numeric)) %&gt;% \n  mutate_all(.funs = function(x) predict(bestNormalize::yeojohnson(x), newdata = x))\n# check the new distributions\ndf_transformed$yesno &lt;- spam$yesno\ndf_transformed %&gt;% \n  pivot_longer(-yesno) %&gt;% \n  ggplot(aes(yesno, value, fill = yesno)) +\n  geom_violin() +\n  facet_wrap(~name, scales = \"free\", nrow=3) +\n  scale_y_continuous(labels = label_number(scale_cut = cut_short_scale()))+\n  scale_fill_paletteer_d(\"awtools::a_palette\") +\n  ggthemes::theme_base() +\n  theme(legend.position = \"bottom\",\n        plot.background =  element_rect(color = NA),\n        legend.background = element_rect(fill = \"gray90\"),\n        legend.key = element_rect(fill = \"gray90\"),\n        panel.background = element_rect(fill=\"#81AE5C\")) +\n  labs(fill = element_blank(), x = element_blank(), y = element_blank())\n\n\n\n\n\n\n\n\n\nI am not a huge fan of data transformations, but that is a very nice transformation. We often deal with skewed data, which produces difficulties when visualizing and performing data analysis. Having a tool like this power transform comes really handy2."
  },
  {
    "objectID": "posts/2023-08-19-the-power-to-normalize/index.html#second-pca",
    "href": "posts/2023-08-19-the-power-to-normalize/index.html#second-pca",
    "title": "The Power to Normalize",
    "section": "Second PCA",
    "text": "Second PCA\nWe can now check how the second PCA looks like. It’s not a panacea, but we have made large improvements. Check the side by side comparisons:\n\n\nCode\npc &lt;- prcomp(df_transformed[, 1:6])\npc_data &lt;- pc$x[, 1:2] %&gt;% as_tibble()\npc_data$yesno &lt;- spam$yesno\n\npc_second_plot &lt;- ggplot(pc_data, \n       aes(PC1, PC2, color = yesno)) +\n  geom_point() +\n  coord_equal() +\n  scale_color_paletteer_d(\"awtools::a_palette\") +\n  ggthemes::theme_base()+\n  theme(legend.position = \"bottom\",\n        plot.background =  element_rect(color = NA),\n        legend.background = element_rect(fill = \"gray90\"),\n        legend.key = element_rect(fill = \"gray90\"),\n        panel.background = element_rect(fill=\"#81AE5C\")) +\n  labs(color = element_blank())\nlibrary(patchwork)\npc_ori_plot + pc_second_plot\n\n\n\n\n\n\n\n\n\nIn terms of separating data, the second PCA is not the best PCA in the world. We can still see that there is a bunch of points all clustered together:\n\n\nCode\np1 &lt;- ggplot(pc_data, \n       aes(PC1, PC2, color = yesno)) +\n  geom_point(color = 'gray50', alpha = 0.5)  + \n  labs(title = \"All Data\") + \n  coord_equal()+\n  ggthemes::theme_few(base_family = \"Ubuntu\")\nspam_color &lt;- paletteer::paletteer_d(\"awtools::a_palette\")[2]\nemail_color &lt;- paletteer::paletteer_d(\"awtools::a_palette\")[1]\np2 &lt;- ggplot(pc_data, \n       aes(PC1, PC2, color = yesno)) +\n  geom_point(color = 'gray50', alpha = 0.5)  + \n  geom_point(data=filter(pc_data, yesno==\"spam\"),\n             color = spam_color, alpha = 0.5)  + \n  labs(title = \"Spam\") + \n  coord_equal()+\n  ggthemes::theme_few(base_family = \"Ubuntu\")\np3 &lt;- ggplot(pc_data, \n       aes(PC1, PC2, color = yesno)) +\n  geom_point(color = 'gray50', alpha = 0.5)  + \n  geom_point(data=filter(pc_data, yesno==\"email\"),\n             color = email_color, alpha = 0.5)  + \n  labs(title = \"Emails\") + \n  coord_equal() +\n  ggthemes::theme_few(base_family = \"Ubuntu\")\np1 + p2 + p3\n\n\n\n\n\n\n\n\n\nHowever, I encourage you to check fviz_screeplot(pc) to see how dramatically better this second PCA is."
  },
  {
    "objectID": "posts/2023-08-19-the-power-to-normalize/index.html#ending-remarks",
    "href": "posts/2023-08-19-the-power-to-normalize/index.html#ending-remarks",
    "title": "The Power to Normalize",
    "section": "Ending remarks",
    "text": "Ending remarks\nRegardless of the final separation that we could achieve in this particular dataset, the normalization performed using Yeo–Johnson transform was quite powerful. You have been given the Power to Normalize, I hope you try it on your own skewed datasets!"
  },
  {
    "objectID": "posts/2023-08-19-the-power-to-normalize/index.html#footnotes",
    "href": "posts/2023-08-19-the-power-to-normalize/index.html#footnotes",
    "title": "The Power to Normalize",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYes, the title of this post is 100% pun intended.↩︎\nThe devil is on the details. Always check the parameters and be careful on data interpretation when transforming your data!↩︎"
  },
  {
    "objectID": "posts/2020-09-06-calories/index.html",
    "href": "posts/2020-09-06-calories/index.html",
    "title": "A calorie is a calorie",
    "section": "",
    "text": "The world was a different place around 1960. But humans were humans, roughly similar to those that were alive thousands of years ago and likely, as most humans do, they needed to eat.\nThe good news is that we have quite some data of humans eating and we have an interminable debate over how much should we eat.\nIt all boils down to one number: Total Calories. They would have us believe that “a calorie is a calorie” and it doesn’t matter where it comes from. Well, I’m not going to challenge the assumption on this blog, I’m saving that for later. But I was curious to see the evolution of this magical number across the world since around 1960.\nBefore we begin, I want to make some brief remarks.\nData from FAO can be found here.\n\nNOTE: In this analysis, calorie data contains all what’s available for consumption, including waste, which can unfortunatelly be significant. Also note that “Calorie” is easier to write about than kiloCalorie, which is the unit we are actually referring to.\n\n\nSpagetthi Trend\n\n\n\n\n\n\n\n\n\nThis looks mostly like an upward trend. In fact, just for fun, I am going to simplify this whole situation into the before/after scenario. The trend is so clear that I’m not going to lose too much information.\n\n\n\n\n\n\n\n\n\nIn the plot above, we see that most countries increase their food supply as measured by the calories available for consumption since 1960. We can more easily observe each country’s travel in the calorie spectrum in the dot plot below. An interesting thing to focus on this plot is the little amount of countries that have stayed around the dietary guidelines (1900 to 2400 kCal/day/person).\n\n\n\n\n\n\n\n\n\n\n\nHow much of an increase\nWhen we look at the calorie increase as a proportion of the original available, we see most countries increasing calories between 5% and 25%, with a good number of countries increasing heavily, even doubling their calories.\n\n\n\n\n\n\n\n\n\nCan you make it move? Yes! To nail the point home, I want to add some motion to the plots, to see the World’s constant move into higher and higher calorie values. Surprisingly, we don’t see the world moving above 2000 calories and narrowing the distribution into an efficent “Enough food for everyone without waste” stage. We see this terrifying amoeba-like distribution creep into higher and higher caloric values.\n\n\nFeel the excess calories creeping in? It's not just you it's #worldwide pic.twitter.com/NILcKc9vDg\n\n— Matias Andina (@NeuroMLA) September 6, 2020\n\n\nI found this waterfall (?) plot to be also quite telling about the overall movement by decade.\n\n\nThe good and the Bad\nGood news is we lifted an incredible amount of people out of poverty. With that, we see the daily available calories in all countries go up above a somewhat arbitrary 2000 Calories/day/person. The bad news, is that\nCalories are just the tip of the iceberg. If you want to see analysis at the macro levels, check my post about cereals.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{andina2020,\n  author = {Andina, Matias},\n  title = {A Calorie Is a Calorie},\n  date = {2020-09-06},\n  url = {https://matiasandina.netlify.app/posts/2020-09-06-calories/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndina, Matias. 2020. “A Calorie Is a Calorie.” September\n6, 2020. https://matiasandina.netlify.app/posts/2020-09-06-calories/."
  },
  {
    "objectID": "posts/2019-11-22-fitbit-analysis/index.html",
    "href": "posts/2019-11-22-fitbit-analysis/index.html",
    "title": "Fitbit Analysis",
    "section": "",
    "text": "It’s been a bit more than a year since I got a fitbit and I have been pretty excited about tracking my activity and heart rate. I should say I’m quite surprised about the sleep data. Tracking sleep has been, in fact, the most exciting feature, and I now strive to get at least 7 hours of sleep per night.\nLet’s first see a glimpse of the data, just to know what type of data we are dealing with.\n# A tibble: 5 × 8\n  date_time           dateTime   dataset_time variable value total_value\n  &lt;dttm&gt;              &lt;date&gt;     &lt;time&gt;       &lt;chr&gt;    &lt;dbl&gt;       &lt;dbl&gt;\n1 2018-06-10 00:00:00 2018-06-10 00'00\"       steps        0        7256\n2 2018-06-10 00:01:00 2018-06-10 01'00\"       steps        8        7256\n3 2018-06-10 00:02:00 2018-06-10 02'00\"       steps        0        7256\n4 2018-06-10 00:03:00 2018-06-10 03'00\"       steps        0        7256\n5 2018-06-10 00:04:00 2018-06-10 04'00\"       steps        0        7256\n# ℹ 2 more variables: date &lt;date&gt;, time &lt;time&gt;"
  },
  {
    "objectID": "posts/2019-11-22-fitbit-analysis/index.html#density-plots",
    "href": "posts/2019-11-22-fitbit-analysis/index.html#density-plots",
    "title": "Fitbit Analysis",
    "section": "Density plots",
    "text": "Density plots\nLet’s now inspect the overall distribution for heart rate and step values."
  },
  {
    "objectID": "posts/2019-11-22-fitbit-analysis/index.html#when-do-i-move",
    "href": "posts/2019-11-22-fitbit-analysis/index.html#when-do-i-move",
    "title": "Fitbit Analysis",
    "section": "When do I move?",
    "text": "When do I move?\nI will start by focusing on the data for steps.\nI’m curious to see what times of the day have the most activity. Because I have a quite large amount of data points (~751 K) I will use geom_hex() to count for me and simplify rendering1.\n\n\n\n\n\n\n\n\n\nWell, I should have remembered that for the vast majority of minutes (regardless of the hour of the day), the count is exactly zero. Let’s only look at the positive counts:\n\n\n\n\n\n\n\n\n\nWe see now some patches that have high activity (&gt; 100 steps), particularly around 9:00, 12:00 and 18:00. These mostly correspond to “going to work”, “activity around lunch time (?)”, and “going home / physical activity”. For all other cases, it looks like I move around 10-20 steps per minute, regardless of the minute within the hour.\n\nLast 10 minutes\nThe result above is interesting because I usually have to be reminded by Fitbit to “move up to 250 steps in the hour”. I receive this notification during the last 10 minutes of the hour and I would think that during those 10 minutes I put more steps than during the first 50. The data show I’m wrong:\n\n\n\n\n\n\n\n\n\nThat being said, I want to keep my reminder on. I feel like having it turned on definitely adds ~ 1000-2000 steps per day.\n\n\nDaily average\nLet’s get one level above and aggregate each day as a unit. This plot shows a nice trend, with months from May to August showing an increase on the number of steps. Keep in mind that November will show little average steps because for that month we have incomplete data (last day in database is 2019-11-13),\n\n\n\n\n\n\n\n\n\nAt this part of the analysis, I should make clear that I took vacations from 2019-06-27 to 2019-07-11. We will use this information in the analysis to make some things clear."
  },
  {
    "objectID": "posts/2019-11-22-fitbit-analysis/index.html#distribution",
    "href": "posts/2019-11-22-fitbit-analysis/index.html#distribution",
    "title": "Fitbit Analysis",
    "section": "Distribution",
    "text": "Distribution\nWe looked at the average daily steps for each month, how about the distribution of daily steps? We see that most days I come quite close to the default target of 10K steps. There are some days with very little steps (see below) and, obviously, some days with extreme number of steps."
  },
  {
    "objectID": "posts/2019-11-22-fitbit-analysis/index.html#extreme-events",
    "href": "posts/2019-11-22-fitbit-analysis/index.html#extreme-events",
    "title": "Fitbit Analysis",
    "section": "Extreme events",
    "text": "Extreme events\nUsing the boxplot below, we can define extreme events as instances where I walked more than 20K steps. I chose to plot this by day of the week, to get an insight about periodicity of events.\n\n\n\n\n\n\n\n\n\nBecause I walked a lot during the vacations, I highlighted the days on top of the previous boxplot. Most of the extreme events are definitely during the vacations. Moreover, none of the days I walked less than 10K steps, pretty amazing!\n\n\n\n\n\n\n\n\n\nThere are some extreme low events, these are quite likely the days I just don’t wear the fitbit (or days I forget to wear it for most of the day). Just because I can order the data and make another plot, I went ahead and did it!\n\n\n\n\n\n\n\n\n\nWe usually go for walks on Saturdays and/or Sundays. Knowing this little piece of data, it’s quite expected to see Saturdays being the days with higher number of steps (and hence higher success rate on the 10K target)."
  },
  {
    "objectID": "posts/2019-11-22-fitbit-analysis/index.html#season",
    "href": "posts/2019-11-22-fitbit-analysis/index.html#season",
    "title": "Fitbit Analysis",
    "section": "Season",
    "text": "Season\nI want to turn the focus now to the seasonality of the data. I will use a helper function getSeason() that I took from StackOverflow.\nWe can inspect the effect of season on my walking.\n\n\n\n\n\n\n\n\n\nThe plot above is not good, it fails to communicate. I think this is a better way to show the data."
  },
  {
    "objectID": "posts/2019-11-22-fitbit-analysis/index.html#a-years-heart-rate-in-one-plot",
    "href": "posts/2019-11-22-fitbit-analysis/index.html#a-years-heart-rate-in-one-plot",
    "title": "Fitbit Analysis",
    "section": "A year’s heart rate in one plot",
    "text": "A year’s heart rate in one plot\nI’m borrowing heavily from Nick here. But I thought it was a brilliant plot, so I took it for a ride with my data. I actually changed a few things, I decided to keep the native sampling rate and use geom_line() instead of down-sampling and using geom_tile(). The overall trend is clear, movements during the morning and the afternoon that correlate well with going and coming back from work. Somewhere around July 2019 you can see the change in timezone when I took vacations. There are a couple of days in late May with continuously high or lacking values, I take this as one of the days I forgot the fitbit at home, likely spurious measures."
  },
  {
    "objectID": "posts/2019-11-22-fitbit-analysis/index.html#code",
    "href": "posts/2019-11-22-fitbit-analysis/index.html#code",
    "title": "Fitbit Analysis",
    "section": "Code",
    "text": "Code\nThe code for this post is quite long and I thought it would get in the way. I am happy to share upon request, hit me up on Twitter or in the comments below."
  },
  {
    "objectID": "posts/2019-11-22-fitbit-analysis/index.html#sources",
    "href": "posts/2019-11-22-fitbit-analysis/index.html#sources",
    "title": "Fitbit Analysis",
    "section": "Sources",
    "text": "Sources\n\nhttps://livefreeordichotomize.com/2017/12/27/a-year-as-told-by-fitbit/"
  },
  {
    "objectID": "posts/2019-11-22-fitbit-analysis/index.html#footnotes",
    "href": "posts/2019-11-22-fitbit-analysis/index.html#footnotes",
    "title": "Fitbit Analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI had been searching for excuses to use this function for a while. Check it out here↩︎"
  },
  {
    "objectID": "posts/2018-01-07-instead-of-research/index.html",
    "href": "posts/2018-01-07-instead-of-research/index.html",
    "title": "Instead of Research",
    "section": "",
    "text": "Here’s to the list things you are doing instead of research. I wasted some time by organizing the categories into logical clusters of endless misery. I wasted some more time finding people who agree with my complains.\n\nBack up.\nHard drive space is not enough.\nFree space in local or personal network is not enough.\nUniversity sponsored sync program does not sync.\nNew computer, sync between your own files.\nRe-install life, yes you are going to use custom preferences.\n\n\n\nDeciding whether OS A or OS B is better.\n\n\n\nDeciding you should use a double boot.\nBeing delusional and wishing to have just one copy per file between multiple OS.\nCompatibility between different OS running on different people’s devices.\nCompatibility between people.\n\n\n\nProprietary software.\nExcel.\nTrack changes didn’t work.\nLikely that co-worker didn’t track changes.\nFilenames are not useful.\nn copies of the same file. Everywhere.\nMYfile_Version_3.044_final_seriously_I_promise_OKFINE.something\n\n\n\nReference managers\nWYSIWYG vs LATEX\nLATEX fonts and templates.\nSpecial characters (as in anything outside the English Language).\nPDF does not compile for X reason.\nUniversity mail is not compatible with gmail.\nR working directory.\nMATLAB PATH.\nWorking directories, paths and beyond.\nPackages, libraries, modules, snippets of codes that updated and are thus incompatible.\n\n\n\nMaking meaningless lists like this one.\nMaking meaningful lists of things you will never do.\n\n\n\nPeople writing dates in notoriously diverging formats.\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{andina2018,\n  author = {Andina, Matias},\n  title = {Instead of {Research}},\n  date = {2018-01-07},\n  url = {https://matiasandina.netlify.app/posts/2018-01-07-instead-of-research/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndina, Matias. 2018. “Instead of Research.” January 7,\n2018. https://matiasandina.netlify.app/posts/2018-01-07-instead-of-research/."
  },
  {
    "objectID": "posts/2021-01-16-lifeviz/index.html",
    "href": "posts/2021-01-16-lifeviz/index.html",
    "title": "Lifeviz",
    "section": "",
    "text": "I have not made new year resolutions, I made art. I also finally got some time to explore one of the little Shiny projects that were dormant in my /Project folder during most of 2020: lifeviz.\nThe goal of lifeviz is to visualize life events. This has been inspired by the post ‘The Tail End’ by WaitButWhy. I have taken the visualizations and added a few degrees of freedom, so that the users can visualize the past/present. On the activities tab, users can display the remaining activity counts for things they could represent using emojis.\nThis is an example of what it can do:\nYou can find lifeviz here."
  },
  {
    "objectID": "posts/2021-01-16-lifeviz/index.html#footnotes",
    "href": "posts/2021-01-16-lifeviz/index.html#footnotes",
    "title": "Lifeviz",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt’s an aspirational goal, maybe someday I will get there.↩︎"
  },
  {
    "objectID": "posts/2017-12-01-segregation-of-beliefs/index.html",
    "href": "posts/2017-12-01-segregation-of-beliefs/index.html",
    "title": "Segregation of Beliefs",
    "section": "",
    "text": "I am writing because I believe there is a powerful message in the ideas of Simon Sinek. Simon is the author of interesting material such as Start with why and Leaders eat Last. I have had a great time reading his books and watching his talks. The call to surround ourselves by people who believe what we believe1 had a powerful effect on the way I approach life.\n\n\nFinding people who believe what we believe is reinforcing. When we find such people, trust emerges, we belong, and we experience a sense of purpose unlike any other. If I like you and you like me back, we are likely to hang out sooner than later. Humans hanging out together makes great things for humanity.\nYou might feel great alone. You might find other people difficult to deal with or distracting in a way that negatively impacts whatever you want to do. You might believe that being a lone wolf and going your own way is the best path for you. First, there’s a great deal of admiration thrown towards the renegade/rebel/visionary. Second, people are quite often humans (see Cognitive Dissonance).\nIn reality, the “lone wolf” is a less productive way2. An aggregation of like minded people creates a positive feedback loop that brings more like minded people to the group, generation after generation. Things cannot escalate to infinity (the world’s population is limited), but they can get pretty big, pretty fast3. We can easily model this positive feedback loop, which should look logistic:\n\n\n\nAnother reason we aggregate with like minded people is to reduce cognitive dissonance. We experience an extreme load when dealing with opinions that are not aligned with what we think is true. Continuous accumulation of such a negative pressure ends up being unbearable. Thus, the relief we get from finding a group we belong to reinforces our behavior of moving towards/with them."
  },
  {
    "objectID": "posts/2017-12-01-segregation-of-beliefs/index.html#start-with-why",
    "href": "posts/2017-12-01-segregation-of-beliefs/index.html#start-with-why",
    "title": "Segregation of Beliefs",
    "section": "",
    "text": "I am writing because I believe there is a powerful message in the ideas of Simon Sinek. Simon is the author of interesting material such as Start with why and Leaders eat Last. I have had a great time reading his books and watching his talks. The call to surround ourselves by people who believe what we believe1 had a powerful effect on the way I approach life.\n\n\nFinding people who believe what we believe is reinforcing. When we find such people, trust emerges, we belong, and we experience a sense of purpose unlike any other. If I like you and you like me back, we are likely to hang out sooner than later. Humans hanging out together makes great things for humanity.\nYou might feel great alone. You might find other people difficult to deal with or distracting in a way that negatively impacts whatever you want to do. You might believe that being a lone wolf and going your own way is the best path for you. First, there’s a great deal of admiration thrown towards the renegade/rebel/visionary. Second, people are quite often humans (see Cognitive Dissonance).\nIn reality, the “lone wolf” is a less productive way2. An aggregation of like minded people creates a positive feedback loop that brings more like minded people to the group, generation after generation. Things cannot escalate to infinity (the world’s population is limited), but they can get pretty big, pretty fast3. We can easily model this positive feedback loop, which should look logistic:\n\n\n\nAnother reason we aggregate with like minded people is to reduce cognitive dissonance. We experience an extreme load when dealing with opinions that are not aligned with what we think is true. Continuous accumulation of such a negative pressure ends up being unbearable. Thus, the relief we get from finding a group we belong to reinforces our behavior of moving towards/with them."
  },
  {
    "objectID": "posts/2017-12-01-segregation-of-beliefs/index.html#the-segregation-problem",
    "href": "posts/2017-12-01-segregation-of-beliefs/index.html#the-segregation-problem",
    "title": "Segregation of Beliefs",
    "section": "The Segregation Problem",
    "text": "The Segregation Problem\n\nA simple desire\nLet’s be optimistic for a minute and assume that the vast majority of people have no problem living with people that are different (remember: I am using “different” to signify “people who believe differently from what they believe”). But let’s add a simple desire, they want to live surrounded by a majority of people who do believe what they believe (say, 60% of my neighbors are like me). Well, the bad news is that the simple desire evolves into high degrees of segregation. Check the following simulation here.\n\n\n\nFrom random to segregated using 60% as threshold\n\n\nWe start to see how easy rules can generate complex behaviors that are difficult to predict."
  },
  {
    "objectID": "posts/2017-12-01-segregation-of-beliefs/index.html#complexity",
    "href": "posts/2017-12-01-segregation-of-beliefs/index.html#complexity",
    "title": "Segregation of Beliefs",
    "section": "Complexity",
    "text": "Complexity\nIf we are talking about complexity emerging from simple rules, I can’t help but to mention The Game of Life.\n\nThe Game of Life\nThe Game of Life is a creation by the mathematician John Horton Conway4. Basic rules apply to set life free and watch it evolve:\n\nAny live cell with fewer than two live neighbors dies, as if caused by underpopulation.\nAny live cell with two or three live neighbors lives on to the next generation.\nAny live cell with more than three live neighbors dies, as if by overpopulation.\nAny dead cell with exactly three live neighbors becomes a live cell, as if by reproduction.\n\nEasy at it seems, these little cells can behave in spectacular ways. They can thrive and perish but they can also oscillate, establishing a pattern forever. Below I present 2 cases of many:\n\n\n\n\n\n\n\nCircle of Fire\nFrog 2\n\n\n\n\n\n\n\n\n\nThese oscillators or organisms function as independent entities from day 1. But not all of them function like that. Some oscillators have a phase in which they grow or diversify.\n\n\n\nGrowing and Diversifying\n\n\nOnce self-assembled, they become stable and perpetuate.\n\n\n\nReady to oscillate forever\n\n\nIn the previous case, the critical mass was quite limited but oscillators need not to be small, check this monster called 258P3!\n\n\n\n258P3\n\n\nI have digressed into this little cult game to show how very simple rules produce remarkably complex behavior that can scale from a few cells (read: small groups of people) to large ensambles (read: very large groups of people) and perpetuate in time (read: all History)."
  },
  {
    "objectID": "posts/2017-12-01-segregation-of-beliefs/index.html#people-change",
    "href": "posts/2017-12-01-segregation-of-beliefs/index.html#people-change",
    "title": "Segregation of Beliefs",
    "section": "People Change",
    "text": "People Change\nWait a minute, you might say. People change, they don’t stay fixed believing any particular thing. Maybe. There might be evidence that, over the long run, some societal beliefs evolve 5. However, it’s also true that individuals are remarkably stubborn and will hold on to their core beliefs even when proven wrong. Moreover, the whole point of finding our tribe is to be able to keep the things that we believe intact (often holy).\n\nConstant Beliefs\nThere are some reasons to assume that individual beliefs are constant:\n\nThe fact that one chooses to affiliate/adhere with a group based on a subset of fixed beliefs (e.g, Holy Trinity, Fundamental Theorem of Algebra, …). Thus, one expects such beliefs, or at least the most fundamental ones, to be held constant for our lifetime or even longer than that.\nThe fact that information within such group is segregated and presented in a way that clearly distinguishes us from them. Tainted information becomes increasingly difficult to recognize when it’s all the information there is.\nThe fact that it requires an infinite amount of effort to think outside the margins of such reality, or diversify beliefs when stuck with a bunch of people that are riding the same boat, specially because all agreed on the first place that it was the correct boat to be in.\n\nWe might as well assume beliefs (B) to be held constant or change little over long periods of time (\\(\\frac{dB}{dt} \\approx 0\\) )."
  },
  {
    "objectID": "posts/2017-12-01-segregation-of-beliefs/index.html#the-republic-lost",
    "href": "posts/2017-12-01-segregation-of-beliefs/index.html#the-republic-lost",
    "title": "Segregation of Beliefs",
    "section": "The republic, Lost",
    "text": "The republic, Lost\nSegregation allows two stories to live happily in two different worlds, worlds that never need to talk to each other. Groups of people that only receive a portion information will grow up to embody the beliefs they grew up with. We are all guilty of this. Moreover, we will happily fund the maintenance of institutions that allow us to hold and propagate our beliefs, regardless of how harmful they might be to them. Lawrence Lessig develops it further and more clearly than myself6. Andrew Yang wrote similar ideas in [The War on Normal People](https://en.wikipedia.org/wiki/The_War_on_Normal_People)."
  },
  {
    "objectID": "posts/2017-12-01-segregation-of-beliefs/index.html#all-in-all",
    "href": "posts/2017-12-01-segregation-of-beliefs/index.html#all-in-all",
    "title": "Segregation of Beliefs",
    "section": "All in all",
    "text": "All in all\nSegregation and self-sufficient organizations entice us in the private sphere. We seek to belong to them. That’s why we come up with religions, tribes, clubs, groups, neighborhood associations, and all other aggregates. But how do we manage situations in which that logic gets taken into spheres of public policy or politics?\nWhat happens if the things that we believe, the things that segregate us, are disputes over facts?\nWhat happens when we no longer agree on the definition of truth? Sticking our heads into our oasis of overlapping beliefs makes for a society unable to dialogue, willing to dangerously escalate into intolerance and violence."
  },
  {
    "objectID": "posts/2017-12-01-segregation-of-beliefs/index.html#update-from-the-future",
    "href": "posts/2017-12-01-segregation-of-beliefs/index.html#update-from-the-future",
    "title": "Segregation of Beliefs",
    "section": "Update From the Future",
    "text": "Update From the Future\nI originally wrote this article in 2017. I am editing this in 2023, having lived through this very scenario during a pandemic, and I am sure we all have very fresh answers to the closing questions 7."
  },
  {
    "objectID": "posts/2017-12-01-segregation-of-beliefs/index.html#footnotes",
    "href": "posts/2017-12-01-segregation-of-beliefs/index.html#footnotes",
    "title": "Segregation of Beliefs",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSinekVideo↩︎\nBy the way, wolves are gregarious, social animals. No such thing as the lone wolf.↩︎\nWe can assume a logistic growth with the change in people over time has the shape \\(\\frac{dN}{dt} = rN \\left( 1 - \\frac{N}{K} \\right)\\)↩︎\nGame of Life Wiki↩︎\nhttps://ourworldindata.org/lgbt-rights↩︎\nhttp://republic.lessig.org/↩︎\nOf course, segregated answers. Echo chambers. You know the drill.↩︎"
  },
  {
    "objectID": "posts/2020-01-02-writing-checklist/index.html",
    "href": "posts/2020-01-02-writing-checklist/index.html",
    "title": "Writing Checklist",
    "section": "",
    "text": "I spent a good portion of the end of 2019 and the beginning of 2020 reading Paul Graham’s essays. Many caught my attention (and my fingers to take notes). I want to comment on an essay about advice for good writing. Because it was written in a way that allowed for easy reformatting (ideas separated by ;), I thought it would be cool to convert the prose into a checklist)."
  },
  {
    "objectID": "posts/2020-01-02-writing-checklist/index.html#creating-the-checklist",
    "href": "posts/2020-01-02-writing-checklist/index.html#creating-the-checklist",
    "title": "Writing Checklist",
    "section": "Creating the checklist",
    "text": "Creating the checklist\nThe main text comes from here. For simplicity, I have only used the main paragraph (aka, copy and paste the second paragraph). I omitted it here because it would be long and you can read it in the native format from the source.\nIn the code chunk below I comment the steps I took.\n\n# text is copy-paste of the second paragraph\n# text &lt;- c(\"...\") ... == copy-paste\n\n# split using \"; \"\nsp &lt;- stringr::str_split(text, \"; \")\n\n# To create a checklist\n# DASH SPACE [ ] SPACE SPACE\nsp &lt;- paste0(\"- [ ]  \", unlist(sp))\n# append markdown title\nsp &lt;- c(\"# Paul Graham's Writing Checklist\", sp)\n\n# Capitalize first letter of every sentence\nsp &lt;- stringr::str_to_sentence(sp)\n\nIf you want to write it to file, you can use writeLines()\n\n# write\nfileConn &lt;- file(\"paul_graham_writing_checklist.md\")\nwriteLines(sp, fileConn)\nclose(fileConn)\n\n# If you want to knit to html you can do this\n# knitr::knit2html(\"paul_graham_writing_checklist.md\")\n\nLet’s take a look at the product:\n\n# printing for web post\ncat(sp, sep = \"\\n\")\n\n# Paul graham's writing checklist\n- [ ]  Write a bad version 1 as fast as you can\n- [ ]  Rewrite it over and over\n- [ ]  Cut out everything unnecessary\n- [ ]  Write in a conversational tone\n- [ ]  Develop a nose for bad writing, so you can see and fix it in yours\n- [ ]  Imitate writers you like\n- [ ]  If you can't get started, tell someone what you plan to write about, then write down what you said\n- [ ]  Expect 80% of the ideas in an essay to happen after you start writing it, and 50% of those you start with to be wrong\n- [ ]  Be confident enough to cut\n- [ ]  Have friends you trust read your stuff and tell you which bits are confusing or drag\n- [ ]  Don't (always) make detailed outlines\n- [ ]  Mull ideas over for a few days before writing\n- [ ]  Carry a small notebook or scrap paper with you\n- [ ]  Start writing when you think of the first sentence\n- [ ]  If a deadline forces you to start before that, just say the most important sentence first\n- [ ]  Write about stuff you like\n- [ ]  Don't try to sound impressive\n- [ ]  Don't hesitate to change the topic on the fly\n- [ ]  Use footnotes to contain digressions\n- [ ]  Use anaphora to knit sentences together\n- [ ]  Read your essays out loud to see (a) where you stumble over awkward phrases and (b) which bits are boring (the paragraphs you dread reading)\n- [ ]  Try to tell the reader something new and useful\n- [ ]  Work in fairly big quanta of time\n- [ ]  When you restart, begin by rereading what you have so far\n- [ ]  When you finish, leave yourself something easy to start with\n- [ ]  Accumulate notes for topics you plan to cover at the bottom of the file\n- [ ]  Don't feel obliged to cover any of them\n- [ ]  Write for a reader who won't read the essay as carefully as you do, just as pop songs are designed to sound ok on crappy car radios\n- [ ]  If you say anything mistaken, fix it immediately\n- [ ]  Ask friends which sentence you'll regret most\n- [ ]  Go back and tone down harsh remarks\n- [ ]  Publish stuff online, because an audience makes you write more, and thus generate more ideas\n- [ ]  Print out drafts instead of just looking at them on the screen\n- [ ]  Use simple, germanic words\n- [ ]  Learn to distinguish surprises from digressions\n- [ ]  Learn to recognize the approach of an ending, and when one appears, grab it.\n\n\nThe checklist is not in strict chronological order, so I might try to reshape it later into something that would make more sense as a timeline. It will render as a checklist in something that is GitHub flavored (e.g., on GitHub). However, I will probably just use it as text file or print it as is.\nI also noticed that the link for the Spanish translation is broken/outdated, so I tried my best to translate it. Here it is"
  },
  {
    "objectID": "posts/2020-01-02-writing-checklist/index.html#summary",
    "href": "posts/2020-01-02-writing-checklist/index.html#summary",
    "title": "Writing Checklist",
    "section": "Summary",
    "text": "Summary\nI think this checklist is an awesome learning opportunity and a clear path to improving one’s writing. Quite happy to have found it, looking forward to improving my skills.\nI got to learn a few cool R things like, 1) I can use writeLines() to write a .md file and 2) stringr::str_to_sentence() is an awesome function."
  },
  {
    "objectID": "posts/2017-12-16-about-student-motivation/index.html",
    "href": "posts/2017-12-16-about-student-motivation/index.html",
    "title": "About student motivation",
    "section": "",
    "text": "I’m writing this post to document my own learning on reading from Teaching Tips and Teaching at its Best.\nI have found chapter 11 of McKeachie’s to be of great value, it contains lots of starting points for discussion and further analysis."
  },
  {
    "objectID": "posts/2017-12-16-about-student-motivation/index.html#choice",
    "href": "posts/2017-12-16-about-student-motivation/index.html#choice",
    "title": "About student motivation",
    "section": "Choice",
    "text": "Choice\nHumans are into the free will thing. Access to choosing their own topic, exam questions and due dates might facilitate the learning experience and boost motivation through a feeling of agency. However, too much choice is normally a bad thing. Assignments like “write about anything” are not a good idea.\n\nI plead guilty\nI have incurred in the “write about anything” practice for some of my assignments. The way I try to convince myself that the strategy works, is by saying that “I keep the range of acceptable answers broad and do not constrain individual variability”. I would sometimes say that to my students, which is received with mixed feelings. There is a sweet spot of “individual variability” within a constrained playground that I should look for. Letting them on the loose ends up hindering their production and diminishing motivation."
  },
  {
    "objectID": "posts/2017-12-16-about-student-motivation/index.html#value",
    "href": "posts/2017-12-16-about-student-motivation/index.html#value",
    "title": "About student motivation",
    "section": "Value",
    "text": "Value\nI have previously written about value and purpose here. I am really glad that many concerns I had before reading McKeachie’s have also appeared as concerns for the author. It is explicitly advised to teachers: Teach with a sense of purpose. Moreover, one should:\n\nMake the value of the course explicit and take time to help students understand why what they are learning matters."
  },
  {
    "objectID": "posts/2017-12-16-about-student-motivation/index.html#teach-for-mastery",
    "href": "posts/2017-12-16-about-student-motivation/index.html#teach-for-mastery",
    "title": "About student motivation",
    "section": "Teach for Mastery",
    "text": "Teach for Mastery\nWhat if we taught for mastery instead of grades? What if we set concrete non relative standards? Students will greater benefit from this approach. I have my own set of thoughts about teaching for mastery. For now, I will relay information better put together by people who believe what I believe, like Salman Khan. On a related note, I highly recommend this book on Mastery.\n\nThis is not a nice to have, it’s a social imperative. - Salman Khan"
  },
  {
    "objectID": "posts/2017-12-16-about-student-motivation/index.html#praise",
    "href": "posts/2017-12-16-about-student-motivation/index.html#praise",
    "title": "About student motivation",
    "section": "Praise",
    "text": "Praise\nReading a beautiful piece written by a student is a superb experience. Sometimes they come up with insightful ways of producing content, powered by creative thinking, showing angles you have not seen before, citing sources you totally missed or read through a different mindset. Those moments are wonderful and I thank this profession for that. Good work should be recognized and encouraged.\nBut there is a potential pitfall on praising. We tend to appreciate talent over hard work. Hence we might fall on the idea that some of the students are just naturals, they just have it. Whatever that it is, it’s extremely hard to define and completely subjective. There are interesting pieces written on appreciation of talent over working ethic like this one or any of the following Google Search. Whenever encountering outstanding work, praise the work, the distilled action is the following:\n\nDo not praise for talent. Praise working ethic.\n\nOn a day to day basis, strive to make the correct associations. Instead of “Good job! You must be really talented” go for “Good job! You must have worked really hard”."
  },
  {
    "objectID": "note.html",
    "href": "note.html",
    "title": "Preview",
    "section": "",
    "text": "Preview\nquarto preview index.qmd\n\n\nPublish\nquarto render git add docs git commit -m “Publish site to docs/” git push"
  },
  {
    "objectID": "blogroll/index.html",
    "href": "blogroll/index.html",
    "title": "Blogroll",
    "section": "",
    "text": "I use RSS feeds to share my content in different pages. Here’s a list of the sites I use:\n\nhttps://www.r-bloggers.com/"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yair Litman",
    "section": "",
    "text": "Computational Material Scientist - Theoretical Chemist"
  },
  {
    "objectID": "codes/index.html",
    "href": "codes/index.html",
    "title": "Contribution to Community Codes",
    "section": "",
    "text": "I am a main developer of i-PI and contributor to FHI-aims\n\n i-PI: a universal force engine \n\n FHI-aims: The ab initio materials simulation package"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Yair Litman",
    "section": "",
    "text": "Below is a list of all posts in this site.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nRecoding America\n\n\n\n\n\n\nlearning\n\n\npolicy\n\n\nbooks\n\n\n\nA great dive into the inner workings of digital technology in the US government\n\n\n\n\n\nJan 12, 2024\n\n\n7 min\n\n\n\n\n\n\n\nOriginal Text\n\n\n\n\n\n\nR\n\n\n\nChecking how the use of verbatim quotes in books\n\n\n\n\n\nNov 7, 2023\n\n\n9 min\n\n\n\n\n\n\n\nThe Power to Normalize\n\n\n\n\n\n\nR\n\n\ndataviz\n\n\n\nA tidytuesday inspired foray into the YeoJohnson transformation\n\n\n\n\n\nAug 19, 2023\n\n\n7 min\n\n\n\n\n\n\n\ngoodreads\n\n\n\n\n\n\nR\n\n\nbooks\n\n\n\nMy 2023 challenge to read more women authors\n\n\n\n\n\nAug 6, 2023\n\n\n13 min\n\n\n\n\n\n\n\nInequality\n\n\nMakeover of Pyramid Plot\n\n\n\n\n\n\n\n\nOct 10, 2022\n\n\n8 min\n\n\n\n\n\n\n\nIcebreaker\n\n\n\n\n\n\n\n\n\n\n\nApr 25, 2021\n\n\n3 min\n\n\n\n\n\n\n\nQueen’s Gambit\n\n\nA trip down the chess rabbit hole\n\n\n\nchess\n\n\nfun\n\n\ndesign\n\n\n\n\n\n\n\n\n\nFeb 7, 2021\n\n\n26 min\n\n\n\n\n\n\n\nLifeviz\n\n\n\n\n\n\nart\n\n\nR\n\n\n\n\n\n\n\n\n\nJan 16, 2021\n\n\n2 min\n\n\n\n\n\n\n\nRotating perspectives\n\n\n\n\n\n\ngenerative-art\n\n\nR\n\n\n\n\n\n\n\n\n\nJan 3, 2021\n\n\n4 min\n\n\n\n\n\n\n\nArt in a New Year\n\n\n\n\n\n\ngenerative-art\n\n\nR\n\n\n\n\n\n\n\n\n\nJan 2, 2021\n\n\n2 min\n\n\n\n\n\n\n\nRussian roulette\n\n\n\n\n\n\nrandom\n\n\ninvesting\n\n\n\n\n\n\n\n\n\nNov 2, 2020\n\n\n4 min\n\n\n\n\n\n\n\nA calorie is a calorie\n\n\n\n\n\n\ndataviz\n\n\n\n\n\n\n\n\n\nSep 6, 2020\n\n\n10 min\n\n\n\n\n\n\n\nUnique\n\n\n\n\n\n\nfun\n\n\npython\n\n\nR\n\n\n\n\n\n\n\n\n\nJul 5, 2020\n\n\n1 min\n\n\n\n\n\n\n\nBirthday Problem\n\n\n\n\n\n\nfun\n\n\npython\n\n\nR\n\n\n\n\n\n\n\n\n\nJun 13, 2020\n\n\n1 min\n\n\n\n\n\n\n\nPython Keylogger\n\n\n\n\n\n\nfun\n\n\npython\n\n\n\n\n\n\n\n\n\nJun 13, 2020\n\n\n3 min\n\n\n\n\n\n\n\nComplex Fun\n\n\n\n\n\n\nart\n\n\nfun\n\n\nR\n\n\n\n\n\n\n\n\n\nApr 30, 2020\n\n\n4 min\n\n\n\n\n\n\n\nThe power of compounding\n\n\n\n\n\n\n\n\n\n\n\nJan 20, 2020\n\n\n4 min\n\n\n\n\n\n\n\nEscribiendo en pocas palabras\n\n\n\n\n\n\n\n\n\n\n\nJan 2, 2020\n\n\n3 min\n\n\n\n\n\n\n\nWriting Checklist\n\n\n\n\n\n\nwriting\n\n\nlearning\n\n\nDocumentingLearning\n\n\nR\n\n\n\n\n\n\n\n\n\nJan 2, 2020\n\n\n4 min\n\n\n\n\n\n\n\nFitbit Analysis\n\n\n\n\n\n\ntracking\n\n\nlearning\n\n\nR\n\n\n\n\n\n\n\n\n\nNov 22, 2019\n\n\n17 min\n\n\n\n\n\n\n\nOn pipelines\n\n\n\n\n\n\nR\n\n\ndata\n\n\nreproducibility\n\n\n\n\n\n\n\n\n\nNov 14, 2019\n\n\n7 min\n\n\n\n\n\n\n\nSkype a Scientist part 2\n\n\n\n\n\n\nresearch\n\n\noutreach\n\n\n\n\n\n\n\n\n\nSep 13, 2019\n\n\n15 min\n\n\n\n\n\n\n\nLos mismos de siempre\n\n\n\n\n\n\n\n\n\n\n\nSep 10, 2019\n\n\n35 min\n\n\n\n\n\n\n\nBirthday Meritocracy\n\n\n\n\n\n\nideas\n\n\nRandomness\n\n\n\n\n\n\n\n\n\nMay 9, 2019\n\n\n19 min\n\n\n\n\n\n\n\nData Visualization Challenge 2\n\n\n\n\n\n\nR\n\n\nlearning\n\n\n\n\n\n\n\n\n\nApr 14, 2019\n\n\n7 min\n\n\n\n\n\n\n\nEmail analysis\n\n\n\n\n\n\nR\n\n\nlearning\n\n\n\n\n\n\n\n\n\nApr 6, 2019\n\n\n27 min\n\n\n\n\n\n\n\nR pipeline – It’s a trap!\n\n\n\n\n\n\nR\n\n\nlearning\n\n\n\n\n\n\n\n\n\nJan 2, 2019\n\n\n10 min\n\n\n\n\n\n\n\nQuery Pubmed in R\n\n\n\n\n\n\nR\n\n\nPhD\n\n\n\n\n\n\n\n\n\nNov 23, 2018\n\n\n5 min\n\n\n\n\n\n\n\nSkype a Scientist part 1\n\n\n\n\n\n\nresearch\n\n\noutreach\n\n\n\n\n\n\n\n\n\nOct 2, 2018\n\n\n11 min\n\n\n\n\n\n\n\nWriting challenge day 28\n\n\n\n\n\n\nresearch\n\n\nwriting\n\n\nlearning\n\n\nDocumentingLearning\n\n\n\n\n\n\n\n\n\nSep 27, 2018\n\n\n5 min\n\n\n\n\n\n\n\nWriting challenge Day 21\n\n\n\n\n\n\nresearch\n\n\nwriting\n\n\nlearning\n\n\nDocumentingLearning\n\n\n\n\n\n\n\n\n\nSep 20, 2018\n\n\n3 min\n\n\n\n\n\n\n\nWriting challenge Day 14\n\n\n\n\n\n\nresearch\n\n\nwriting\n\n\nlearning\n\n\nDocumentingLearning\n\n\n\n\n\n\n\n\n\nSep 13, 2018\n\n\n4 min\n\n\n\n\n\n\n\nWriting challenge Day 7\n\n\n\n\n\n\nwriting\n\n\nDocumentingLearning\n\n\n\n\n\n\n\n\n\nSep 5, 2018\n\n\n5 min\n\n\n\n\n\n\n\nWriting challenge Day 1\n\n\n\n\n\n\n\n\n\n\n\nAug 30, 2018\n\n\n2 min\n\n\n\n\n\n\n\nXY-density-maps\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nJul 13, 2018\n\n\n4 min\n\n\n\n\n\n\n\nGithub style calendar heatmaps\n\n\n\n\n\n\nresearch\n\n\ndesign\n\n\nR\n\n\n\n\n\n\n\n\n\nMay 27, 2018\n\n\n8 min\n\n\n\n\n\n\n\nMaking atlas images\n\n\n\n\n\n\nresearch\n\n\ndesign\n\n\n\n\n\n\n\n\n\nMay 19, 2018\n\n\n2 min\n\n\n\n\n\n\n\nMatlab list_files\n\n\n\n\n\n\n\n\n\n\n\nApr 24, 2018\n\n\n5 min\n\n\n\n\n\n\n\nt-tests for correlation\n\n\n\n\n\n\nteaching\n\n\n\n\n\n\n\n\n\nApr 20, 2018\n\n\n4 min\n\n\n\n\n\n\n\nInstead of Research\n\n\n\n\n\n\nPhD\n\n\nideas\n\n\n\n\n\n\n\n\n\nJan 7, 2018\n\n\n2 min\n\n\n\n\n\n\n\nTeaching for Mastery\n\n\n\n\n\n\nteaching\n\n\nlearning\n\n\nideas\n\n\n\n\n\n\n\n\n\nJan 5, 2018\n\n\n13 min\n\n\n\n\n\n\n\nRules of Composition\n\n\n\n\n\n\nteaching\n\n\nlearning\n\n\nDocumentingLearning\n\n\nwriting\n\n\n\n\n\n\n\n\n\nDec 28, 2017\n\n\n3 min\n\n\n\n\n\n\n\nAbout student motivation\n\n\n\n\n\n\nteaching\n\n\nlearning\n\n\nDocumentingLearning\n\n\n\n\n\n\n\n\n\nDec 26, 2017\n\n\n3 min\n\n\n\n\n\n\n\nList of Common Errors\n\n\n\n\n\n\nteaching\n\n\nlearning\n\n\nDocumentingLearning\n\n\n\n\n\n\n\n\n\nDec 19, 2017\n\n\n1 min\n\n\n\n\n\n\n\nAbout grades\n\n\n\n\n\n\nteaching\n\n\nlearning\n\n\nDocumentingLearning\n\n\n\n\n\n\n\n\n\nDec 19, 2017\n\n\n15 min\n\n\n\n\n\n\n\nAny Questions?\n\n\n\n\n\n\nteaching\n\n\nlearning\n\n\nDocumentingLearning\n\n\n\n\n\n\n\n\n\nDec 17, 2017\n\n\n2 min\n\n\n\n\n\n\n\nSegregation of Beliefs\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2017\n\n\n8 min\n\n\n\n\n\n\n\nAbout Good Discussions\n\n\n\n\n\n\nteaching\n\n\nlearning\n\n\nideas\n\n\n\n\n\n\n\n\n\nNov 30, 2017\n\n\n4 min\n\n\n\n\n\n\n\nAbout Teaching Nonsense\n\n\n\n\n\n\nteaching\n\n\nlearning\n\n\n\n\n\n\n\n\n\nNov 16, 2017\n\n\n5 min\n\n\n\n\n\n\n\nHow do you make a cake?\n\n\n\n\n\n\nteaching\n\n\nlearning\n\n\n\n\n\n\n\n\n\nNov 14, 2017\n\n\n4 min\n\n\n\n\n\n\nNo matching items\n\n\n  \n\nReuseCC BY 4.0CitationBibTeX citation:@online{andina,\n  author = {Andina, Matias},\n  url = {https://matiasandina.netlify.app/posts/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndina, Matias. n.d. https://matiasandina.netlify.app/posts/."
  },
  {
    "objectID": "posts/2017-11-16-about-teaching-nonsense/index.html",
    "href": "posts/2017-11-16-about-teaching-nonsense/index.html",
    "title": "About Teaching Nonsense",
    "section": "",
    "text": "Living without a purpose is something I am unable to do. I am into the rules of the game, the idea that there’s a goal, and we are trying to achieve something, do something, create an object or service that was not there before and now fulfills a function. There are few games that I play for the sake of keep playing like the learning game. But, more often than not, I stand on the side of finite games. Still, when I compare myself with the average student in the United States (US), I feel I know nothing about finite games. Their whole Universe cycles around winning, knowing how to follow the rules, hack the multiple choice, get the job done, keep the eyes on the prize, [fill here yet another catchy phrase].\nThese young individuals come to University being highly driven and stumble upon a wall of requirements and unclear bureaucratic norms. They are expected to sit quietly and go through several courses that have little purpose. They shuttle between irrelevant numbing experiences and crucial content, delivered in mini-lectures (attention spans are shortening), that only happen in the classes that actually matter (supposedly those that offer 3 or 4 credits). Everything else is hiding under a mystic cloud. Most of my students complain about classes being vague, unstructured, pointless. I thought they would also complain about the amount of money that goes into acquiring these credits but they are still willing to allow the system to borrow from their pocket. It seems to me that they have been indoctrinated, well done high-school!\nWhat am I talking about? Maybe a couple of examples could help. Some of the things that are currently being taught in US higher education.\n\nHow do you make a friend?\nHow do you detect and respond to a microagression?\nWhere is the Campus Center?\nHow to watch television?\nHow to climb a tree?\n\nWhen I say being taught, I mean this is actual class content and students are graded on their answers to these questions. Don’t get me wrong, I am not making any judgement on the value about how important this information is. I would suggest that knowing to make friends is one of the most fundamental traits any Homo Sapiens should have. My question lives at another level. I wonder about higher education requirements and whether this content is something that needs to be taught in formal education or not. I wonder about the training the next generation of builders, writers, and creatives receives. We ask them to ‘Answer with an opinion’, ‘React to a class’ or ‘React to a reaction’. We give them points for breathing, as long as it is inside the classroom. If they want to breathe outside, they better have a piece of paper signed by an authority for the next class.\n\n\n\nMaking Friends by xkcd"
  },
  {
    "objectID": "posts/2017-11-16-about-teaching-nonsense/index.html#life-without-purpose",
    "href": "posts/2017-11-16-about-teaching-nonsense/index.html#life-without-purpose",
    "title": "About Teaching Nonsense",
    "section": "",
    "text": "Living without a purpose is something I am unable to do. I am into the rules of the game, the idea that there’s a goal, and we are trying to achieve something, do something, create an object or service that was not there before and now fulfills a function. There are few games that I play for the sake of keep playing like the learning game. But, more often than not, I stand on the side of finite games. Still, when I compare myself with the average student in the United States (US), I feel I know nothing about finite games. Their whole Universe cycles around winning, knowing how to follow the rules, hack the multiple choice, get the job done, keep the eyes on the prize, [fill here yet another catchy phrase].\nThese young individuals come to University being highly driven and stumble upon a wall of requirements and unclear bureaucratic norms. They are expected to sit quietly and go through several courses that have little purpose. They shuttle between irrelevant numbing experiences and crucial content, delivered in mini-lectures (attention spans are shortening), that only happen in the classes that actually matter (supposedly those that offer 3 or 4 credits). Everything else is hiding under a mystic cloud. Most of my students complain about classes being vague, unstructured, pointless. I thought they would also complain about the amount of money that goes into acquiring these credits but they are still willing to allow the system to borrow from their pocket. It seems to me that they have been indoctrinated, well done high-school!\nWhat am I talking about? Maybe a couple of examples could help. Some of the things that are currently being taught in US higher education.\n\nHow do you make a friend?\nHow do you detect and respond to a microagression?\nWhere is the Campus Center?\nHow to watch television?\nHow to climb a tree?\n\nWhen I say being taught, I mean this is actual class content and students are graded on their answers to these questions. Don’t get me wrong, I am not making any judgement on the value about how important this information is. I would suggest that knowing to make friends is one of the most fundamental traits any Homo Sapiens should have. My question lives at another level. I wonder about higher education requirements and whether this content is something that needs to be taught in formal education or not. I wonder about the training the next generation of builders, writers, and creatives receives. We ask them to ‘Answer with an opinion’, ‘React to a class’ or ‘React to a reaction’. We give them points for breathing, as long as it is inside the classroom. If they want to breathe outside, they better have a piece of paper signed by an authority for the next class.\n\n\n\nMaking Friends by xkcd"
  },
  {
    "objectID": "posts/2017-11-16-about-teaching-nonsense/index.html#what-do-i-have-to-do-with-this",
    "href": "posts/2017-11-16-about-teaching-nonsense/index.html#what-do-i-have-to-do-with-this",
    "title": "About Teaching Nonsense",
    "section": "What do I have to do with this?",
    "text": "What do I have to do with this?\nAs a grad student, you don’t always get to choose who is paying your rent. I talk with my brain quite often, theoretical experiment ahead (pun intended):\n\nBrain: How much would you need to be paid to do XYZ?\nMe: XYZ? Pff…..a lot, I cannot even tell you how much…\nBrain: I can tell you how much, your stipend it’s coming from teaching XYZ class next semester.\nMe: What!?\nBrain: Yes, you’ve guessed…welcome to grad school.\n\nWhen I have to teach these classes, I cannot help but wonder: why are we doing this? what is the purpose?"
  },
  {
    "objectID": "posts/2017-11-16-about-teaching-nonsense/index.html#what-is-the-purpose-of-higher-education",
    "href": "posts/2017-11-16-about-teaching-nonsense/index.html#what-is-the-purpose-of-higher-education",
    "title": "About Teaching Nonsense",
    "section": "What is the purpose of higher education?",
    "text": "What is the purpose of higher education?\nI believe this question has no general answer. I find myself unsatisfied by the attempts to encapsulate higher education, particularly facing the fact that somewhere during this century human performance will be out-passed by computer’s performance in the vast majority of tasks. I host a mix of hope and fear about that, but I am somewhat confident in the fact that it will happen.\nWhatever answer we go for, it is going to be highly dependent on the field. It will not be the same if you are doing Physics or Business. The lens through which you see the world, your daily mental framework, is going to be different. Yet, it appears to me that, from Public Policy perspective, we should find overlapping opinions. To a certain extent, this is such a fundamental issue that we cannot afford to have it diverge in the way it is diverging now. It not only affects the training students receive from an academic perspective, it devaluates the degree on the job market. Could we find a set of experiences and abilities (professional or personal) that we agree upon?"
  },
  {
    "objectID": "posts/2017-11-16-about-teaching-nonsense/index.html#mix-and-match",
    "href": "posts/2017-11-16-about-teaching-nonsense/index.html#mix-and-match",
    "title": "About Teaching Nonsense",
    "section": "Mix and Match",
    "text": "Mix and Match\nArguments towards diversification of higher education are everywhere, should we aim towards teaching the so called well-rounded individuals 1? I believe the borders dividing areas are blurry but they do exist. In that blurry interface, I believe there’s a huge potential to win from interdisciplinary work. Moreover, I would venture to say that many of the groundbreaking work will only be possible due to interdisciplinary team efforts. I still debate myself about what is the correct mix and match for the optimal results. I will have a tentative, not extensive but forever updating, list below:\n\nEngineers using bio-mimicry to improve train design 2."
  },
  {
    "objectID": "posts/2017-11-16-about-teaching-nonsense/index.html#footnotes",
    "href": "posts/2017-11-16-about-teaching-nonsense/index.html#footnotes",
    "title": "About Teaching Nonsense",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nActually, this was one of the questions for my GRE exam. I hate how pre-formatted these cardboard questions are.↩︎\nBiology inspired Engineering https://www.youtube.com/watch?v=iMtXqTmfta0↩︎"
  },
  {
    "objectID": "posts/2019-11-14-on-pipelines/index.html",
    "href": "posts/2019-11-14-on-pipelines/index.html",
    "title": "On pipelines",
    "section": "",
    "text": "I have been thinking about different problems I have when writing code and the things that I normally try to do to keep my projects clean and functional. I wrote this post to put this thoughts out there, hopefully I will receive input from the great software engineers."
  },
  {
    "objectID": "posts/2019-11-14-on-pipelines/index.html#problems-in-mind",
    "href": "posts/2019-11-14-on-pipelines/index.html#problems-in-mind",
    "title": "On pipelines",
    "section": "Problems in mind",
    "text": "Problems in mind\n\nWhere do you live?\nCode files usually live in one folder, which is also a GitHub folder that you and your team commit/push to. So far, so good. But what do you do with the data to feed that monster pipeline of yours?\nI will assume that your concerns with data privacy are minor or you handled them accordingly (only private parties have access to the data).\nNow, you still have the problem of where to put this other folder, which is basically a size problem.\n\nSmall files can live with your data\n\nThis is the case for small and few text files of some thousand rows. Easy enough, you just go with your /repo-name/data/ and live happily ever after.\n\nMedium size files\n\nThese files are big enough to be a problem for hosting on GitHub. File formats start to be an issue here, images and video will not be easily accessible anywhere you take it.\nOptions: the cloud ☁️\nPros: It’s fluffy. Now, seriously, it’s good that your code can point to one place, download the stuff into local and use it. Every computer can do the same and there should be no problem. Because your sizes are not huge, you should be fine.\nCons: You need internet. No, internet it’s not everywhere all the time1. Internet is not in my cellphone on a second basement in a concrete building. Even with the fastest internet, it’s not trivial to setup your-favorite-cloud-service to allow access to the-sketchy-script-you-wrote2.\n\n\n\n\nOptions: Good old-fashioned external hard drive. 💾\nPros: This is a good one if your data size is in the Gb range and you don’t really need to share it with too many people.\nCons: Hard drives fail. Are you ready to lose your data? It starts to get really annoying when you have to do back-ups of your data and your data is big enough that you can’t use your computer’s hard drive (that’s why you chose an external hard drive in the first place). Should you have an external hard drive for the external hard drive? Are you planning to write the output of your code on those hard drives? Brace for impact.\n\n\n\nExternal hard-drives might have paths that change depending on which computer is connected to. This can easily be a path inferno. Moreover, some hard drives don’t work if you try to use them in different OS.\n\nLarge sizes\n\nI work with brains. Last time I checked, one mouse brain is ~2TB, n=1, just a few channels, not even the best resolution we can get.\nI think local/cloud servers are the only way to go here3. I don’t have a lot of experience with this, but I have suffered internet upload/download speed problems when I try to sync with my cloud back-ups or share image/video files with my team.\n\n\nPaths need to be absolute\nBecause your working directory is the folder where your code lives4, but your data folder lives elsewhere, you kind of need to use absolute paths all the time.\nI have only been able to fix this issue using functions that attempt to fix this when running the script.\n\nfix_working_environment &lt;- function(saved_path,\n                                    local_path){\n  # if the folder structure doesn't work as expected...\n  # this will explode \n  stringr::str_replace(saved_path,\n                       \"some_regular_expression\",\n                       local_path)\n}\n\nThis is particularly annoying when you have to run commands that involve calling things from console.\nLet’s call ImageJ from R.\n\nsystem(paste(\"/home/matias/Downloads/Fiji.app/ImageJ-linux64 --run\",\n             macro_to_run))\n\nThe moment somebody changes the Fiji folder, or tries to call ImageJ from another computer, that code brakes. I’m unaware of how to make sure these things bullet-proof, please enlighten me.\nLet’s call python from R. Wait, what version of python do you want? I rest my case.\n\n\nProcesses are identified by the files\nI have this problem quite often. It might be because my pipelines follow this logic.\n\n\n\n\n\n\nIt’s quite difficult to escape the infinite list all files –&gt; apply function to all files –&gt; write computations into new files loop. I don’t really know what’s on the other side.\nThe main problem is that your previous, current, and next files always serve as identifiers and you need to carry over their absolute path (to be able to read them form your data folder). Whenever these paths get corrupted (or you change your computer) things stop working.\nThis problem might stem from the fact that I normally have to process experimental units through the pipeline. I have to do many things to an experimental unit and have many many experimental units composing the data for one pipeline. That’s when my inner voice goes:\n\nBut I would also like to have the possibility to run or re-run just one (or just a few experimental units).\n\nThe way I handle this is by leaving open the door to hand selection of files (aka interactive mode, not fun). However, interactive mode somewhat helps with the problem below.\n\n\nDon’t move my files\nPeople do stuff people normally do, like moving folders around…that’s BAD, REALLY BAD. It’s also quite difficult to communicate the need to keep the file structure without casting the magic spells of everything will break5.\nI don’t feel good with the level of dependency on file structure that my projects always end up having. Please enlighten me on this one too!\n\n\nDon’t rename my files\nDon’t rename my files, except when I do. That would be a better subtitle of this section. A great way of not dealing with multiple copies of the same files. For example, let’s say you applied a mask to an image and then cropped, and rotated it. How many files do you keep? What if your image size was 1 Gb?\nMy hack around this is to rename the files (this include the cases where I just want to move files to specific sub-folders). Because I rely so much on the file names, this renaming usually comes back to bite me. I just 🤷.\n\n\nOperating systems\nI’m writing this in 2019, I thought the OS problem was solved. Turns out it’s not solved at all and developers shy away from it more often than they should. I understand them, developing for every OS is a huge pain and requires you to constantly check in multiple machines (or have access to a teammate that breaks your code as soon as you push it)."
  },
  {
    "objectID": "posts/2019-11-14-on-pipelines/index.html#what-is-your-approach",
    "href": "posts/2019-11-14-on-pipelines/index.html#what-is-your-approach",
    "title": "On pipelines",
    "section": "What is your approach?",
    "text": "What is your approach?\nThis is something I will continue to think for a long time, and my approach might need to be adjusted to each situation. What is your current approach?"
  },
  {
    "objectID": "posts/2019-11-14-on-pipelines/index.html#footnotes",
    "href": "posts/2019-11-14-on-pipelines/index.html#footnotes",
    "title": "On pipelines",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMaybe I should say moderately fast and stable internet.↩︎\nImage borrowed from here↩︎\nAnd yet, for many practical reasons, I never do this.↩︎\nAnd you should never forcefully set the working directory elsewhere.↩︎\nYes, your computer is on the line! And I will get all your passwords (?).↩︎"
  },
  {
    "objectID": "posts/2020-01-20-The-power-of-compounding/index.html",
    "href": "posts/2020-01-20-The-power-of-compounding/index.html",
    "title": "The power of compounding",
    "section": "",
    "text": "Paying attention to the small details sounds like a waste of time. Where’s that 80/20 rule when I need it? Well, yes, talking about a small increment sounds boring and useless. A 1% increase in performance might seem trivial. Unexciting. A rounding error.\nBut it is often neglected because we don’t have a good dimension of scales (e.g., A small percent of a lot of money it’s a lot of money) or even when we do, we fail to grasp what exponential phenomena look like. For example, if I double the amount of water on a glass each time I pour water into it, you will see the glass go from half empty to overfull in one round. Yes, doubling is a way bigger rate than 1% increase, but I don’t want to get into math1. I’m actually using compounding in quite a loose way.\nMoreover:"
  },
  {
    "objectID": "posts/2020-01-20-The-power-of-compounding/index.html#happiness",
    "href": "posts/2020-01-20-The-power-of-compounding/index.html#happiness",
    "title": "The power of compounding",
    "section": "Happiness",
    "text": "Happiness\nHow did we get to happiness? I don’t really know. I guess I just noticed that a couple of very small details, small daily decisions, have a tremendous impact in the way I feel, and the mental freedom that they bring is quite refreshing. We know what they are: eating healthy, sleeping well, moving the butt away from the chair and shaking it whatever way you fancy!\nTake meditation and the power of breathing. Simple, right? Well, not really, and it will take a while for you to set it up right. Eventually, you will be able to access the power in one breath. But it will not happen the first time you sit for a session. You need to keep your mind on breath after breath, minute after minute of being present. What you need is a system. And you need patience for letting your system to do the work.\nOne mindful breath is inconsequential, just an infinitesimal drop in the Universe of mindless breathing. But what happens when you start practicing, compounding the power of the infinitesimal? Can you feel the difference? Me too."
  },
  {
    "objectID": "posts/2020-01-20-The-power-of-compounding/index.html#investing",
    "href": "posts/2020-01-20-The-power-of-compounding/index.html#investing",
    "title": "The power of compounding",
    "section": "Investing",
    "text": "Investing\nIt’s no secret that all investment revolves around compounding. Set up your systems (aka, allocate your money) and let it run. I just thought I would remark words of a wise investor, letting us know that, even when we are talking about money, compounding is king but takes a little while to show up to the party.\n\nPatience and optimism are also keys to investing success. Patience because it takes time for the power of compounding to unfold. Optimism because we have to trust the future will allow it the opportunity.2"
  },
  {
    "objectID": "posts/2020-01-20-The-power-of-compounding/index.html#relationships",
    "href": "posts/2020-01-20-The-power-of-compounding/index.html#relationships",
    "title": "The power of compounding",
    "section": "Relationships",
    "text": "Relationships\nWhat if we apply the power of compounding to our relationships? How many more times are you going to suffer because of someone? I will give you a small system: two questions and one quote.\n\nWhat if you stopped hanging around people who bring conflict and suffering?\nWhat if you started hanging around people who don’t bring conflict and suffering?\n\n\nThe first rule of handling conflict is don’t hang around people who are constantly engaging in conflict. (. . .) All of the value in life, including in relationships, comes from compound interest. People who regularly fight with others will eventually fight with you. I’m not interested in anything that’s unsustainable or even hard to sustain, including difficult relationships. – Naval Ravikant"
  },
  {
    "objectID": "posts/2020-01-20-The-power-of-compounding/index.html#summary",
    "href": "posts/2020-01-20-The-power-of-compounding/index.html#summary",
    "title": "The power of compounding",
    "section": "Summary",
    "text": "Summary\nI hear what you are saying, Setting systems up is boring. Maybe. You have to troubleshoot them, you have to stress test them. True. Moreover, once you have them, they still haven’t solved the problem that first got you to think about setting them up. Of Course! You will find yourself asking whether your systems will ever accomplish something. This is because they actually do nothing for a while. Their power lives in the compounding, it feeds from the amount of time that the system has been running.\nSetting up systems is the best way to get rid of your problems. Why? Because once you have systems, your problems are no longer yours, they belong to the systems that you put in place. Slowly, but surely, your systems will kick ass.\nA random tweet reminded me that a small improvement each day for a year can make a big difference. Simple and to the point, the power of compounding:\n1.01³⁶⁵ ≈ 37.78\nWhat will you apply this power to?"
  },
  {
    "objectID": "posts/2020-01-20-The-power-of-compounding/index.html#footnotes",
    "href": "posts/2020-01-20-The-power-of-compounding/index.html#footnotes",
    "title": "The power of compounding",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nunless you read until the end, when the 1% comes for the revenge!↩︎\nI couldn’t have said this better, so I quote JL Collins↩︎"
  },
  {
    "objectID": "posts/2023-11-07-original-text/index.html",
    "href": "posts/2023-11-07-original-text/index.html",
    "title": "Original Text",
    "section": "",
    "text": "I have been lately noticing a glaring trend in some of the non-fiction books that I read: the use and abuse of verbatim quotes. They come in the shape of:\nOf course, there are no rules regarding the use of verbatim text1. But, if I can get a sense of overuse only from reading the book, it makes me curious to go get look at the data.\nHow much of the book is actually a verbatim text dump? Would I bet is 10%? Maybe 20%? Would lower percentages make me go easier on the author or is this a lost cause (i.e., if I notice the overuse by reading, all hope is lost)?"
  },
  {
    "objectID": "posts/2023-11-07-original-text/index.html#example-book",
    "href": "posts/2023-11-07-original-text/index.html#example-book",
    "title": "Original Text",
    "section": "Example Book",
    "text": "Example Book\nEnough of chatter. Let’s try to answer this by analyzing one of the books in question: “Do Nothing” by Celeste Headlee. Reading the book in R using the epubr package gives us this table:\n\n\nCode\n# read the epub\nbook_text &lt;- epubr::epub(\"Do Nothing - Celeste Headlee.epub\")\nbook_text$data[[1]] %&gt;% \n  mutate(text = str_sub(text, 0, 20),\n         text = paste(text, \"...\")) %&gt;% \n  gt::gt() # needs to get the text stings truncated\n\n\n\n\n\n\n\n\nsection\ntext\nnword\nnchar\n\n\n\n\ntitlepage\n...\n0\n0\n\n\npart0001.xhtml\n...\n0\n0\n\n\npart0002.xhtml\n...\n0\n0\n\n\npart0003.xhtml\nCopyright © 2020 by ...\n111\n1014\n\n\npart0004.xhtml\nCONTENTSCoverTitle P ...\n93\n685\n\n\npart0005.xhtml\nINTRODUCTIONIt will ...\n3242\n18783\n\n\npart0006.xhtml\nPART IThe Cult of Ef ...\n5\n28\n\n\npart0007.xhtml\nChapter 1MIND THE GA ...\n3270\n18268\n\n\npart0008.xhtml\nChapter 2IT STARTS W ...\n4880\n28878\n\n\npart0009.xhtml\nChapter 3WORK ETHICI ...\n3812\n22815\n\n\npart0010.xhtml\nChapter 4TIME BECOME ...\n8804\n52065\n\n\npart0011.xhtml\nChapter 5WORK COMES ...\n4271\n25298\n\n\npart0012.xhtml\nChapter 6THE BUSIEST ...\n4773\n28006\n\n\npart0013.xhtml\nChapter 7DO WE LIVE ...\n5022\n28949\n\n\npart0014.xhtml\nChapter 8UNIVERSAL H ...\n5930\n35332\n\n\npart0015.xhtml\nChapter 9IS TECH TO ...\n6036\n35402\n\n\npart0016.xhtml\nPART IILeaving the C ...\n12\n61\n\n\npart0017.xhtml\nLife-Back OneCHALLEN ...\n2345\n13558\n\n\npart0018.xhtml\nLife-Back TwoTAKE TH ...\n2570\n15330\n\n\npart0019.xhtml\nLife-Back ThreeSTEP ...\n3933\n22790\n\n\npart0020.xhtml\nLife-Back FourINVEST ...\n1925\n10856\n\n\npart0021.xhtml\nLife-Back FiveMAKE R ...\n2857\n16804\n\n\npart0022.xhtml\nLife-Back SixTAKE TH ...\n2161\n12337\n\n\npart0023.xhtml\nCONCLUSIONWe have ch ...\n2084\n12378\n\n\npart0024.xhtml\nFor Theresa, who has ...\n14\n72\n\n\npart0025.xhtml\nACKNOWLEDGMENTSI WOR ...\n355\n1995\n\n\npart0026.xhtml\nNOTESIntroduction\"Ou ...\n4067\n28683\n\n\npart0027.xhtml\nABOUT THE AUTHORCELE ...\n318\n1971\n\n\npart0028.xhtml\nWhat's next onyour r ...\n19\n139\n\n\n\n\n\n\n\nWe can get rid of the legal stuff that normally goes before the text and everything that comes after the content (i.e., acknowledgements and references).\n\n\nCode\n# A simple slice operation would do\nbook_txt &lt;- book_text$data[[1]] %&gt;% \n  slice(6:24)\n\n\nWe can also get some metadata from the text (will come useful for later).\n\n\n\nCode\n# bind previous word and character counts\nmeta &lt;- book_txt %&gt;% \n  select(section, nword, nchar) %&gt;% \n  mutate(part = paste0(\"part\", 5:23)) %&gt;% \n  select(-section)\n\n\nNow that we have the text, we can find all the instances of \"something in between these quotes here\" using stringr::str_locate_all():\n\n\nCode\n# extract the text\nmatch_df &lt;- stringr::str_locate_all(book_txt$text, '\"(.*?)\"') %&gt;% \n  # give names for future binding\n  # parts go from 5 to 23 (idx goes 6:24)\n  set_names(nm = paste0(\"part\", 5:23)) %&gt;% \n  # convert into tibble for easy binding\n  map(as_tibble) %&gt;% \n  bind_rows(.id =  \"part\")\n\n\nBelow, I’m showing a slice with an example of matched character positions and how they would look like in the text. I want to direct your attention to the second and third row. I hope you notice that these two quotes are, in fact, one single quote that was split into two.\n\n\nCode\n# This is an example\nmatch_df %&gt;% \n  slice(8:10) %&gt;% \n  mutate(quote = map2_chr(\n    start, end,  \n    function(.x, .y) str_sub(book_txt$text[[1]], .x, .y)\n  )) %&gt;% \n  gt::gt() %&gt;%\n  gt::tab_style(\n    style = gt::cell_text(weight = \"bold\"),\n    locations = gt::cells_column_labels()\n        )\n\n\n\n\n\n\n\n\npart\nstart\nend\nquote\n\n\n\n\npart5\n14893\n14905\n\"inefficient\"\n\n\npart5\n16002\n16132\n\"I can hunch over my computer screen for half the day churning frenetically through emails without getting much of substance done,\"\n\n\npart5\n16186\n16336\n\"all the while telling myself what a loser I am, and leave at 6:00 p.m. feeling like I put in a full day. And given my level of mental fatigue, I did!\"\n\n\n\n\n\n\n\n\nMerging Quotes\nThe issue of quotes being split arises not because of a bug in code, but because the author writes in this way. She would do something like:\n\n“A palm tree”, somebody said, “belongs to the Plant Kingdom.”\n\nThese stylistic choices will modify the statistics for the direct quotes (e.g., the average length of a quote will be much lesser than if these quotes were kept verbatim). I decided that I want to merge quotes if they are too close to each other (I will try 100 characters2). This will slightly inflate my % counts, since I’m attributing characters that are not direct quotes to actual quotes. Thus, when I calculate percentages, I will do so without merging (see @percentages-with-no-merging).\nThere’s one neat trick using lag and cumsum with a condition to achieve conditional grouping. We can see that rows 9 and 10 are marked as belonging to the same group now 🎉.\n\n\nCode\nthreshold &lt;- 100  # Define your threshold\n\nmerged_quotes &lt;- match_df %&gt;%\n  mutate(\n    .by = part, \n    prev_end = lag(end),\n    distance = start - prev_end,\n    merge_group = cumsum(ifelse(is.na(distance) | distance &gt; threshold, 1, 0))\n  ) \n\n# \nhead(merged_quotes, n = 10)\n\n\n# A tibble: 10 × 6\n   part  start   end prev_end distance merge_group\n   &lt;chr&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;    &lt;int&gt;       &lt;dbl&gt;\n 1 part5   574   597       NA       NA           1\n 2 part5  1342  1361      597      745           2\n 3 part5  1876  1904     1361      515           3\n 4 part5  6036  6051     1904     4132           4\n 5 part5  8751  8944     6051     2700           5\n 6 part5  9276  9373     8944      332           6\n 7 part5 13258 13265     9373     3885           7\n 8 part5 14893 14905    13265     1628           8\n 9 part5 16002 16132    14905     1097           9\n10 part5 16186 16336    16132       54           9\n\n\nThis intermediate step also gives us the answer to a new question:\n\nWhat is the average distance between quotes?\n\nThe answer is x̄= 740 ± sd = 970 . On average, you start a new qoute after 130 words of original content. Is that a lot? Is that too little?\nTo be honest, it feels true to the reading experience. My sensation was that the author was using the verbatim quotes with high frequency, and the data seems to align with that. But don’t take my word for it, let’s try to visualize it.\nWe are two steps away from the viz.\n\nDo the actual merge\nAdd the end of each chapter\n\nWe can do Step 1 using the code below:\n\n\nCode\nmerged_quotes &lt;- merged_quotes %&gt;%\n  summarize(\n    .by = c(merge_group, part),\n    part = first(part),\n    start = first(start),\n    end = last(end)\n  ) %&gt;% \n  # add the lag again to see where the original text starts\n  mutate(text_start = lag(end, default = 0), .by = part)\n\n\nRight now, we have the start of the original text in text_start and the start and end of each verbatim quote. We need to make use of the metadata stored in meta to add the end of the original content for of each chapter. This only matters for the very last portion that we are going to plot, so I will make a new data set that contains those values instead of merging everything together. To visualize it, I’m going to make use of a package I developed called ggethos. You can check it out here or adapt the code to work with geom_segment().\n\n\nCode\n# pad parts  for plotting\nformat_part &lt;- function(part_name) {\n  # Extract the numeric part\n  part_number &lt;- as.integer(str_extract(part_name, \"\\\\d+\"))\n\n  # Pad the number with zeros and prepend 'part'\n  formatted_part &lt;- str_c(\"part\", str_pad(part_number, width = 2, pad = \"0\"))\n  \n  return(formatted_part)\n}\n\n# make tail end segments\ntail_data &lt;- merged_quotes %&gt;% \n  summarise(.by = part, last_quote_end = max(end)) %&gt;% \n  left_join(meta, by='part') %&gt;% \n  # fix the padding after merging\n  mutate(part = format_part(part))\n\n# fix the padding here too\nmerged_quotes &lt;- merged_quotes  %&gt;% mutate(part = format_part(part))\n\n\nggplot(data=merged_quotes) + \n  geom_ethogram(aes(x=text_start, xend=start, y = part), color =\"gray30\") +\n  geom_ethogram(data=tail_data, aes(x=last_quote_end, \n                                    xend=nchar, y = part), color =\"gray30\") +\n  geom_ethogram(aes(x=start, xend=end, y = part), color = \"red\")+\n  cowplot::theme_nothing() +\n  labs(title = \"'Do Nothing' is Peppered by Quotes\",\n       subtitle = \"&lt;span style = 'color:gray30'&gt;Original Text&lt;/span&gt; and &lt;span style = 'color:red'&gt;Verbatim quotes&lt;/span&gt;\",\n       caption = \"Viz: Matias Andina\",\n       y = \"Chapter\") +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    plot.subtitle = ggtext::element_markdown(hjust = 0.5),\n    plot.background = element_rect(fill = \"black\"),\n    text = element_text(color = 'gray80'),\n    axis.title.y = element_text(angle = 90),\n    plot.caption = element_text(size = 8, hjust = .95))\n\n\n\n\n\n\n\n\n\nI believe this plot conveys a good mental image of what reading the book feels like in terms of verbatim text usage.\n\n\nPercentages with no merges\nAs mentioned in the beginning of the article, I was curious about how much verbatim text there was. Again, using the number of characters in each chapter stored in the meta object, we can easily calculate the percentage of all characters that are directly quoted:\n\n\nCode\nmatch_df %&gt;% \n  mutate(quote_chars = end - start) %&gt;% \n  summarise(.by = part, \n            quote_chars = sum(quote_chars)) %&gt;% \n  left_join(meta, by = \"part\") %&gt;% \n  mutate(quote_frac = quote_chars / nchar,\n         part = fct_reorder(part, quote_frac)) %&gt;% \n  ggplot() +\n  geom_hline(aes(yintercept = mean(quote_frac)), lty = 4) +\n  geom_point(aes(as.numeric(part), quote_frac), \n             size = 4, alpha = 0.9, color = \"darkred\") +\n  geom_label(aes(x = 15.5, y = 0.17,\n                 label = paste(part[which.max(quote_frac)],\n                               scales::percent(max(quote_frac)),\n                               sep=\"\\n\"\n                 ))) +\n  scale_y_continuous(labels = scales::label_percent(),\n                     expand = expansion(add = c(0.01, 0.05)))+\n  labs(y = \"Verbatim Quotes\",\n       x = \"Book Part\\n(ascending quote % order)\",\n       title = \"'Do Nothing' contains ~10% verbatim quoted text\",\n       subtitle = \"Some parts are as high as 17%!\")+\n  cowplot::theme_minimal_hgrid()"
  },
  {
    "objectID": "posts/2023-11-07-original-text/index.html#a-silver-lining",
    "href": "posts/2023-11-07-original-text/index.html#a-silver-lining",
    "title": "Original Text",
    "section": "A silver lining",
    "text": "A silver lining\nMost non-fiction books are a regurgitation of something somebody else said a long time ago (there’s nothing new under the sun). In a sense then, it’s more truthful for an author to quote verbatim from the original source than to paraphrase whatever they took out of it and hide the initial message under a footnote3."
  },
  {
    "objectID": "posts/2023-11-07-original-text/index.html#footnotes",
    "href": "posts/2023-11-07-original-text/index.html#footnotes",
    "title": "Original Text",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBut I’m sure a copyright lawyer would know much more than I do regarding how much verbatim text you can include and still claim ownership of your work.↩︎\nOf course, this threshold is arbitrary. How did I come up with it? I asked ChatGPT to come up with 10 interjections that were a bit longer than “they said” and phrases where sitting comfortably around 50. I doubled it to be super sure that we were not missing instances.↩︎\nThis paragraph was indeed a paraphrase of my editor’s (read wife’s) reaction to my article. Talking to her is a great exercise in positive reframing.↩︎"
  },
  {
    "objectID": "posts/2018-09-27-writing-challenge-day-28/index.html",
    "href": "posts/2018-09-27-writing-challenge-day-28/index.html",
    "title": "Writing challenge day 28",
    "section": "",
    "text": "This month has been fast. The writing has been really interesting, I was able to get some old stories written and had a lot of fun doing it. I would like to continue with the habit, it is already making it easier for me to put words into text. I’m not sure about the quality, but I experience the writing ease for both Spanish and English."
  },
  {
    "objectID": "posts/2018-09-27-writing-challenge-day-28/index.html#the-data",
    "href": "posts/2018-09-27-writing-challenge-day-28/index.html#the-data",
    "title": "Writing challenge day 28",
    "section": "The data",
    "text": "The data\nI had some pretty long bouts on Saturday and Sunday, writing close to 3000 words in aggregate. Of course, considering my overall trend, these instances look like outliers, but it didn’t feel that way.\nIt felt like a reasonable production, given the fact I was: 1) not tired, 2) somewhat isolated from the outside world, and 3) in a writing mood.\n\n\n\n\n\n\n\n\n\nThe aggregate trend has recovered a bit from a sloppy third week of the challenge. The cumulative average is 958.18, which sounds way better than just achieving 50% of the target. Still, ~1000 words a day is a solid number if I’m able to consistently put that amount of words outside my head and into a product."
  },
  {
    "objectID": "posts/2018-09-27-writing-challenge-day-28/index.html#general-comments",
    "href": "posts/2018-09-27-writing-challenge-day-28/index.html#general-comments",
    "title": "Writing challenge day 28",
    "section": "General comments",
    "text": "General comments\nIt’s been a great month of writing. I took a challenge knowing that it was going to be tough and interesting. It has certainly been a great way to force me to put words into text.\nI intend to continue pushing myself to write. I definitely should say that having a high bar is a great driving force, and knowing that I’m really unlikely to get 2000 words unless I sit to write several hours undisturbed is somewhat comforting. I would say the pressure to produce is at a good level and I’m confident I can maintain/slightly increase this amount of production.\nSolid work on the outline is hands down the best first step. Initial ideas or sub-headers can grow into a paragraph and, a coherent story starts to materialize before your eyes. The outline still needs good content to back it up, having a bunch of content/ideas is the key to write a good amount of quality text."
  },
  {
    "objectID": "posts/2018-09-27-writing-challenge-day-28/index.html#the-good",
    "href": "posts/2018-09-27-writing-challenge-day-28/index.html#the-good",
    "title": "Writing challenge day 28",
    "section": "The good",
    "text": "The good\nI finished another short story, two blog posts and was pretty close of hitting the word count on two days. I was afraid about the downward trend but I reversed bad third week and transformed it into a (so far) better fourth week. Weekend writing is the best!\nAfter a month, Sublime Text 3 and I are best friends. I use it for .csv, .md and sometimes even .Rmd files. I go back and forth between Sublime and Rstudio or couple Sublime with pandoc via command line. I found this writing experience to be better than what I had with Word. I specially like the aesthetics of the program, the dark background doesn’t mess with my eyes1 and I really enjoy the different functions to play around.\nI know this won’t happen but I am really over with Word and WYSIWYG programs (yes Excel, I’m talking to you too, you started all this!). The truth is that most people will continue to use them or find them more convenient. Hence, until a critical mass outside these type of programs is not reached, everyone will keep using them to write and share documents. Hopefully, with time and the new generations being taught how to program in school, Markdown (or something like it) will be more accessible and we will overcome this problem2."
  },
  {
    "objectID": "posts/2018-09-27-writing-challenge-day-28/index.html#to-improve",
    "href": "posts/2018-09-27-writing-challenge-day-28/index.html#to-improve",
    "title": "Writing challenge day 28",
    "section": "To improve",
    "text": "To improve\nWorking before writing the main portion of text is key. Having the ideas out is important but they don’t have to be fully materialized. Doing the work of outlining where the text will go can get you a long way. It can give you a hard substrate to anchor your writing bouts from. Connecting the dots is a separate work, you can’t flow if you are busy editing a sentence while you write it. First, flow, get the ideas out, all of them. Later connect and edit."
  },
  {
    "objectID": "posts/2018-09-27-writing-challenge-day-28/index.html#footnotes",
    "href": "posts/2018-09-27-writing-challenge-day-28/index.html#footnotes",
    "title": "Writing challenge day 28",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAlthough for some reason I can’t get myself to move away from the white (default) version of Rstudio.↩︎\nI wonder if I’m going to be 80 by then and not willing to learn new things.↩︎"
  },
  {
    "objectID": "posts/2017-12-19-about-grades/index.html",
    "href": "posts/2017-12-19-about-grades/index.html",
    "title": "About grades",
    "section": "",
    "text": "I’m writing this post to document my own learning on reading from Teaching Tips and Teaching at its Best."
  },
  {
    "objectID": "posts/2017-12-19-about-grades/index.html#calibrated-instruments",
    "href": "posts/2017-12-19-about-grades/index.html#calibrated-instruments",
    "title": "About grades",
    "section": "Calibrated instruments",
    "text": "Calibrated instruments\nMcKeachie’s chapter 10 starts with a set of questions that attack the problem of grading from a philosophical angle.\n\nWhat are grades?\n\n\nDo we agree that grades are communication tools?\n\nThat is an extremely challenging and insightful question. Grades are indeed communications tools and they convey information at many levels. A single letter (or number) on an arbitrary scale will be used to attempt to define person X in system Y. Define it? To whom? Mostly, to a third party who doesn’t know anything about person X in system Y. Grades are communication devices that arise from a totally utilitarian need. From this perspective, it is difficult to overlook how absurd the whole grading situation is.\nThe third party (aka, whoever reads that grade) will add another layer to this story. The grade’s value will be interpreted differently by students, parents, other teachers within the university, and future employers. When we assigning grades that do not represent the student , we can easily create false expectations and close opportunities. Miscommunication hurts everyone. More than anything, we negate the learning process by corrupting the mechanisms that are vital to growth.\nI stand with students first. Students have the right to know how well they performed, what their achievements are and the things that they need to improve. As far as I understand, learning and improving performance are contingent on positive and negative feedback. If I had to choose, I would have no grades but feedback, I would teach for mastery. But that is a discussion for another article and, since we live in a world dominated by grades, we have to play that game for now.\n\nHow valid is the instrument on which grades are based?\n\nWhatever we measure (and communicate), it has to be measured using a valid instrument. Indeed, selecting accurate indicators of student performance might be the hardest part of the job.\nI align with McKeachie’s advice because my natural tendency is to measure performance using an absolute scale. However, I am not alien of solid argumentation against this practice. First, I am not owner of the one and only absolute scale, I am subjective and conditioned by my own background. Second, I also fail. Teacher failure comes in many shapes and flavors (from not engaging and creating the proper learning environment to testing with wrong questions). In those situations, it is common sense to look for relative measures, evaluate how students performed in general terms. The reasoning would be that, if all students failed to understand concept A either:\n\nThe teacher was ineffective during the learning and feedback phase and/or\nThe teacher was ineffective during the testing phase\n\nIt would be really difficult to argue in favor of students getting punished by a suboptimal performance of the teacher. On the brightside, it is unlikely that, over the long run, consistently low performing students are such because they faced such a consistent set of low performing teachers. That is why there is bigger emphasis on averages than individual grades. Averages are useful within the individual but also to compare across individuals. Another measure used for comparisons across individuals are ranks.\nBeing the nth best of an extremely well performing group should not put you into an “under-achiever” box. Chance had you sharing space with a sample of highly capable, highly driven individuals. Furthermore, this group likely worked well because all students contributed extraordinarily into the production of the learning experience (i.e, assignments, projects, debates, discussions, role play). However, outstanding performance is unlikely to explain grade inflation (see below).\nBeing the best of a group that is homogeneously underperforming is not indicative of academic achievement or technical skill. Therefore, the individual should not be promoted by the label of “top of the class”. Reality says he is ranked first but that label is devoid of descriptive information and, most importantly, it fails to meet criteria.\n\nHow reliable is it?\n\nHere’s where we have to start thinking about big scale. Given a test, or any device, that produces value or score, we desire that score to be reliable across time and multiple graders. We feel as if people being measured by the same ruler were paramount. Given no temporal change on the average student profile, multiple-choice based tests are the most reliable devices because they are likely to produce the same result regardless of who is grading them. But what are they testing? Aren’t we sacrificing accuracy to gain reliability? Even forgetting about the temporal change on students, I consider constraining production to a standardized one-word answer to generate more damage than unreliable long essay questions. If the rigid testing structure is not constrained enough, we can add the time variable. Let’s put you on a one minute per question stopwatch!\nSo, how can we get the good of both worlds? Should we request essay only questions? Everyone’s workload increases if we do that. How are we going to deal with retaliation?"
  },
  {
    "objectID": "posts/2017-12-19-about-grades/index.html#what-are-grades-communicating",
    "href": "posts/2017-12-19-about-grades/index.html#what-are-grades-communicating",
    "title": "About grades",
    "section": "What are grades communicating?",
    "text": "What are grades communicating?\nGrades mean different things depending on who is reading. Students are (hopefully) curious about their learning and grade of achievement. They are looking for a concrete proof that they are on the correct path. Additionally, they are the only ones who know the background story of that grade (i.e, how they earned it). Parents are likely to be expecting a safety report on their investment, looking forward to be ensured that everything is under control. Employers are probably looking for a predictor of competency. I feel compelled to examine the phenomenon from the perspective of an employer because I believe that educational institutions have a social responsibility on the professionalization of the labor force. Employers come in many forms and requirements may vary. However, they entrust others the role of training at the entry level and prediction of future performance. For employers request different traits in suitable employees, they also weight differently on requirements for available jobs.\nBut employers face an ocean of candidates and must sort through to fill finite open positions. The pragmatic way of doing so within a constrained budged and time span is to be reductionist and assume things. They assume some things like “schools know what employers need and prepare students for it” or “good grades are predictive of absolute good performance”.\nGrades should not be the only thing to look at. PhD admissions committees are heavily weighted on a different trust system (i.e, letters of recommendation). Academics that want to hire a trainee to do research rely on other academics that have seen how the candidate works. Something like “the best predictor of being able to do something is having done it before”. This system is by no means perfect. I only mention it here as an example of a weighing method that adds another dimension to whatever grades provide."
  },
  {
    "objectID": "posts/2017-12-19-about-grades/index.html#the-inference-problem",
    "href": "posts/2017-12-19-about-grades/index.html#the-inference-problem",
    "title": "About grades",
    "section": "The inference problem",
    "text": "The inference problem\nYes, grades are communication tools, but they are also variables used for inference. In order for any predictor to be useful, it is often the case that one should know or assume the distribution.\n\n\n\n\n\n\n\n\n\nThe Normal distribution could be thought of as the general example. It may well be what most people have it as a first guess. Maybe they would not go for an exactly Normal distribution, but the main concepts are certainly there, they are going for a distribution that has the following properties:\n\nUnimodal and symmetrical, with mode being equal to the mean.\nVast majority of individuals are around acceptable deviations from the mean.\nExceptional individuals are infrequent (&lt; 2.5% on each side).\n\nBut reality doesn’t work like that, we have minimum and maximum grades, which dramatically constrains the range of the distribution. Additionally, we tend to have a passing grade, usually set for the equivalent of 60% of the available points. We have to acknowledge that majority of students are capable of working towards such grades and it is the purpose of any learning experience to make the goal achievable. So, let’s add some reality to the distribution. A more realistic distribution would be:\n\nSomewhat assymetrical, with mode being greater than the passing mark.\nEffective teaching prevents extreme individuals on the very low range.\n\nNow, we should try to see what happens when we progressively skew grades from zero towards a perfect score. Putting aside the cause of the shift for a moment, what effectively happens is that more people get into the exceptional area, which in this case I chose to be the top 5%.\n\n\n\n\n\n\n\n\n\nThinking about communication, we could see no actual problem. In fact, it’s correctly stated in McKeachie’s chapter 10. The meaning of grades is not a problem as long as those who interpret the grade do it understanding the current meaning. I see two main issues with this idea:\n\nIt is suboptimal to request people/managers to adapt to the current value. What reference frame would they use? That is why we have standard units, for example 1.\nIn any system that measures, a wider dynamic range is preferred. Collapsing all students to the 5% of the range, and further condensing that into one bin (letter A) eliminates variation."
  },
  {
    "objectID": "posts/2017-12-19-about-grades/index.html#grade-modification",
    "href": "posts/2017-12-19-about-grades/index.html#grade-modification",
    "title": "About grades",
    "section": "Grade modification",
    "text": "Grade modification\nAssuming a priori distributions might work well in statistical mechanics but it is certainly not the way to grade students. I do not align with practices like curving the grade, that is, assuming a grade distribution of any kind to force students into blocks where they don’t naturally belong. On chapter 10, McKechie stands strong against such policies: “grading on the curve is educationally dysfunctional”.\n\nDeflating\nDeflating grades on purpose is a practice that produces a fit to a normal distribution according to an arbitrary central tendency and spread. I find that practice completely against the goals of teaching, at any level. Enforcing higher standards produces a subsequent deflation, but it’s a different animal species that grading on the curve (more on that below).\n\n\nInflating\nHere is where the extra credit that shifts the distribution comes in. Granted, shifting by adding credit is not the only way, teachers might also lower the bar so much that anyone can walk past it. Inflation has been going in the United States for quite some time now, here is some data analysis. Pressure to increase grades comes from everywhere. Universities operating on a client based model depend upon costumer satisfaction to keep functioning. Moreover, whenever students do not pay themselves for the education, costumer and user become separated entities. Thus, universities can work as an insurance policy and a 4-year party at the same time. How do they keep the costumer satisfaction up? They know what they are selling, they are selling a promise and happy users. They serve themselves of the average salary of college graduates to advertise themselves, just as an insurance policy for retirement. They either enforce, overlook or seldom punish grade inflation. They keep users happy. They have all the incentives to fail in favor of the user, because the pressure is one sided. Where is the line of parents demanding grade deflation?\nThey also invest in branding because it is difficult to be critic of anything if you feel represented it. Brands use abstract symbols that are vague enough to be interpreted positively by everyone. I wonder what would happen if schools used the time course of their own grade inflation instead of their name initials or mascot on their hoodies, T-shirts and mugs. The truth is universities have it difficult, even if they tried to do something about it. There would have to be a broad acknowledgement of the fact, nation wide commitment, a top down (federal?) program to combat inflation. Such commitment should be transparent enough that it doesn’t hurt students’ future. Every other alternative will hinder the small group of students whose grades are deflated.\nGrade inflation existed for a noble cause in the past. If my students were dependent on good grades to stay out of a war, I would certainly inflate. Notice grade escalation during the Vietnam war period. This also serves as an example of how an unofficial rule can spread with no top down regulation. Teachers did what they thought best and that resulted in a systemic change. Maybe this collective behavior was not secret, during the 1970-1980 period grades deflated. Unfortunately, grades have been steadily increasing since then. If current trends continue, eventually no other letter than A will be awarded.\n\n\nLingering questions\nI have my questions about curving and the consequences of the current path. Where does the normal distribution come from? In other words, could it be possible that, for a given set of standards, the long-term distribution of fairly graded individuals follow a normal distribution?\nWhat happens when everyone gets an A? That is a question that has been seriously treated here. I will add some humor to an overly long and boring post."
  },
  {
    "objectID": "posts/2017-12-19-about-grades/index.html#a-systemic-problem",
    "href": "posts/2017-12-19-about-grades/index.html#a-systemic-problem",
    "title": "About grades",
    "section": "A systemic problem",
    "text": "A systemic problem\nI have to disagree with McKechie’s on this one. On chapter 10, it is stated that professors cannot change the meaning of grades individually. I disagree because individuals following simple rules can create massive patterns. Teaching, as currently done by humans, is not devoid of individual human factors that make it far from perfect. Days have only 24 hours. Teachers get tired and have other obligations to devote their energies to. Even with over-achiever teaching rockstars, devotion for their profession and waking ours have a limit. The load of grading and managing hundreds of individual complains about the most minuscule difference in interpretation is unbearable. And yet, it is a right of the students to be given effective feedback. It is only natural for them to look for ways to diminish the burden. Teachers not only game the system, their own career is dependent on it. They know that hard graders receive negative evaluations by students and any rational individual would act to avoid negative impact on their path towards tenure. However, I would like to conclude this section with the words of professor Harvey Mansfield: “A professor has the right to give the grades he wants, but that’s always been understood with a responsibility that’s attached to it, mainly to see to it that you don’t debase our common currency”."
  },
  {
    "objectID": "posts/2017-12-19-about-grades/index.html#general-improvement",
    "href": "posts/2017-12-19-about-grades/index.html#general-improvement",
    "title": "About grades",
    "section": "General Improvement",
    "text": "General Improvement\nIt is probably true that teaching has improved. Productivity, access to information, and computer proficiency have certainly improved. Those are fantastic news, but they should not serve as supporting evidence for scale deformation. We might be training the best generation of students ever, but shortening the dynamic range is not informative (remember grades are a communication tool). If anything, the rescaling should maintain an informative dynamic range and there is good reason to set higher competency standards."
  },
  {
    "objectID": "posts/2017-12-19-about-grades/index.html#meet-criteria",
    "href": "posts/2017-12-19-about-grades/index.html#meet-criteria",
    "title": "About grades",
    "section": "Meet criteria",
    "text": "Meet criteria\nEducational content should provide concepts and skills that are critical to promote achievement of goals. How critical is the content? If indeed critical (e.g, techniques and procedures for open heart surgery), then it is self-evident that criterion based methods should be enforced. If not, we should then have a conversation about why we have non-critical content in higher education."
  },
  {
    "objectID": "posts/2017-12-19-about-grades/index.html#footnotes",
    "href": "posts/2017-12-19-about-grades/index.html#footnotes",
    "title": "About grades",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYes, I know standard measures change in tiny amounts over time! The General Conference on Weights and Measures is actually going to vote so that they are defined only using constants. Take a look at the Kilogram↩︎"
  },
  {
    "objectID": "posts/2023-06-10-goodreads/index.html",
    "href": "posts/2023-06-10-goodreads/index.html",
    "title": "goodreads",
    "section": "",
    "text": "Around early January 2023, an idea came to my mind: the books I read were all written men. My initial guess was 90% male. That’s bad, I know. So I decided to do two things:\n\nCheck my current ratio\nWhatever my current number was, make the effort to read more women authors.\n\nAs a bonus point, I would get to toy with the data (yay 🙌). If you have no interest in checking the dataviz analysis you can jump right into the gender ratios."
  },
  {
    "objectID": "posts/2023-06-10-goodreads/index.html#a-challenging-new-year",
    "href": "posts/2023-06-10-goodreads/index.html#a-challenging-new-year",
    "title": "goodreads",
    "section": "",
    "text": "Around early January 2023, an idea came to my mind: the books I read were all written men. My initial guess was 90% male. That’s bad, I know. So I decided to do two things:\n\nCheck my current ratio\nWhatever my current number was, make the effort to read more women authors.\n\nAs a bonus point, I would get to toy with the data (yay 🙌). If you have no interest in checking the dataviz analysis you can jump right into the gender ratios."
  },
  {
    "objectID": "posts/2023-06-10-goodreads/index.html#fetch-data",
    "href": "posts/2023-06-10-goodreads/index.html#fetch-data",
    "title": "goodreads",
    "section": "Fetch Data",
    "text": "Fetch Data\nI use goodreads to store info about the books I read or those that I am about to read. It turns out that goodreads lets you export your library (go here).\n\nDisclaimer\nIt’s not the cleanest data:\n\nRanking is in whole point increments from 1 to 5, which somewhat constrains what the average values for books might be[^people-read].\nThe number of pages might be wrong (some titles have 0 pages)\nThe year of publishing might obscure the real year of writing (multiple editions obscure this more)\nUnfortunately I don’t have good info on the days it took for me to read the book because I enter whatever ‘feels close enough’ by hand.\n\n[^people-read]: People tend to read “good books”, so it’s expected that this average is skewed towards higher values, but see my other comments in the analysis."
  },
  {
    "objectID": "posts/2023-06-10-goodreads/index.html#data-cleaning",
    "href": "posts/2023-06-10-goodreads/index.html#data-cleaning",
    "title": "goodreads",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nData comes mostly clean and can be imported into R (or any language of your choice).\n\n\nCode\nlibrary(tidyverse)\ntheme_set(theme(text = element_text(size = 16, family = \"Ubuntu\")))\n\ndf &lt;- read_csv(\"mla_goodreads_library_export.csv\",show_col_types = FALSE) %&gt;% \n  janitor::clean_names() %&gt;% \n  # remove books I haven't read or rated\n  filter(my_rating &gt; 0)\n\n# create subtitle, might not be inclusive but probably pretty good\ndf &lt;- df %&gt;% \n  # make a copy\n  mutate(ori_title = title) %&gt;% \n  separate(col = \"title\",\n           into = c(\"title\", \"subtitle\"), \n           sep = \": \")"
  },
  {
    "objectID": "posts/2023-06-10-goodreads/index.html#peoples-ratings-are-super-high",
    "href": "posts/2023-06-10-goodreads/index.html#peoples-ratings-are-super-high",
    "title": "goodreads",
    "section": "People’s Ratings are Super High",
    "text": "People’s Ratings are Super High\nThe average rating for books is pretty high. Even for those at the very bottom.\n\n\nCode\nstars_df &lt;- data.frame(\n  x = c(0, 0:1, 0:2, 0:3, 0:4), \n  y = c(1, rep(2, 2), rep(3, 3), rep(4, 4), rep(5,5)))\nstars_df$x &lt;- stars_df$x / 10\n\nlabel_df &lt;- data.frame(y = 1:5 - 0.25,\n                       x = .15,\n                       label = c(\"didn't like it\" , \n                                 \"it was OK\", \n                                 \"liked it\", \n                                 \"really liked it\", \n                                 \"it was amazing\"))\n                       \ndf %&gt;% \n  ggplot(aes(x=1L, average_rating)) + \n  ggbeeswarm::geom_quasirandom(width=0.3, alpha = 0.5) +\n  #xlim(0,2) +\n  ylim(0.5, 5) +\n  geom_point(data = stars_df, \n             aes(x, y),\n             pch = \"★\", size = 5, color = \"gold\") +\n  geom_text(data = label_df, aes(x, y, label=label)) +\n  theme_void() +\n  theme(panel.grid.major.y = element_line(color = \"gray50\", linetype = 2),\n        plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5)) +\n  labs(title = \"goodgeads readers 'really like' most books\",\n       subtitle = \"Each dot is the average community rating of a book\")\n\n\n\n\n\n\n\n\n\nI think there are a few things happening here:\n\nPeople are way more optimistic and less critic than I am (no surprise)\nPeople who really don’t like a book stop reading it early and might not bother rating it 1. Hence, there’s a sort of ‘survivor bias’ in the data.\nPeople just don’t care about GoodRead’s rating and do their own versions which might have nothing to do with the 5 stars system .\nMaybe I just happen to read “mostly good books”, so I would not find any book with very little rating.\n\nJust to point out the comparison with my own ratings.\n\n\nCode\ndf %&gt;% \n  mutate(delta = my_rating - average_rating,\n         id = fct_reorder(factor(1:n()), delta)) %&gt;% \n  ggplot(aes(x = 1, y = delta, color = delta)) +\n  annotate(geom=\"text\", x=1.7, y=1.5, \n           label=\"Books I liked\\nmore than the\\naverage person\",\n           color = \"red\") +\n  annotate(geom=\"text\", x=.3, y=-3, \n           label=\"Books I liked\\nless than the\\naverage person\",\n           color = \"black\") +\n  ggbeeswarm::geom_quasirandom() + \n  scale_color_gradient(low = \"black\", high = \"red\")+\n  ylim(-4, 4) +\n  xlim(0, 2) +\n  theme(legend.position = \"none\", \n        panel.grid = element_blank(),\n        panel.grid.major.y = element_line(color = \"gray90\"),\n        plot.background = element_blank(),\n        panel.background = element_blank(),\n        axis.line.x = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_blank()) +\n  labs(x = element_blank(),\n       y = \"Rating Difference\",\n       title = \"Differences in Ratings\",\n       subtitle = \"Difference  between my and people's ratings\")"
  },
  {
    "objectID": "posts/2023-06-10-goodreads/index.html#top-and-bottom",
    "href": "posts/2023-06-10-goodreads/index.html#top-and-bottom",
    "title": "goodreads",
    "section": "Top and Bottom",
    "text": "Top and Bottom\nThis analysis couldn’t continue without giving you the gossip of who’s at the top and bottom in my dataset. Mind you, these are the people’s ratings for the books I have read.\n\n\nCode\ndf %&gt;% \n  arrange(desc(average_rating)) %&gt;% \n  select(title, author, average_rating) %&gt;% \n  slice_head(n = 10) %&gt;% \n  gt::gt(caption = \"Top 10 books in dataset\") %&gt;% \n  gt::cols_label(title = \"Title\",\n                 author = \"Author\",\n                 average_rating = \"Average Rating\")\n\n\n\n\n\n\nTop 10 books in dataset\n\n\nTitle\nAuthor\nAverage Rating\n\n\n\n\nI Am Number Four Collection\nPittacus Lore\n4.55\n\n\nI'm Glad My Mom Died\nJennette McCurdy\n4.49\n\n\nThe Storyteller\nDave Grohl\n4.49\n\n\nThe Price We Pay\nMarty Makary\n4.48\n\n\nThe Winners (Beartown, #3)\nFredrik Backman\n4.47\n\n\nEducated\nTara Westover\n4.47\n\n\nStory of Your Life\nTed Chiang\n4.47\n\n\nHarry Potter and the Sorcerer's Stone (Harry Potter, #1)\nJ.K. Rowling\n4.47\n\n\nA Civic Technologist's Practice Guide\nCyd Harrell\n4.45\n\n\nAli\nJonathan Eig\n4.44\n\n\n\n\n\n\n\nHere’s the bottom of the pile\n\n\nCode\ndf %&gt;% \n  arrange(average_rating) %&gt;% \n  select(title, author, average_rating) %&gt;% \n  slice_head(n = 10) %&gt;% \n  arrange(desc(average_rating)) %&gt;% \n  gt::gt(caption = \"Bottom 10 books in dataset\") %&gt;% \n  gt::cols_label(title = \"Title\",\n                 author = \"Author\",\n                 average_rating = \"Average Rating\")\n\n\n\n\n\n\nBottom 10 books in dataset\n\n\nTitle\nAuthor\nAverage Rating\n\n\n\n\nKurashi at Home\nMarie Kondō\n3.59\n\n\nRandomize\nAndy Weir\n3.52\n\n\nHow to Win Every Argument\nMadsen Pirie\n3.52\n\n\nSubtract\nLeidy Klotz\n3.51\n\n\nThe Year of Less\nCait Flanders\n3.48\n\n\nYou Have Arrived at Your Destination\nAmor Towles\n3.48\n\n\nMake, Think, Imagine\nJohn Browne\n3.48\n\n\nLimetown\nCote Smith\n3.36\n\n\nHype\nGabrielle Bluestone\n3.33\n\n\nThe Listening Path\nJulia Cameron\n3.29"
  },
  {
    "objectID": "posts/2023-06-10-goodreads/index.html#searching-for-my-type",
    "href": "posts/2023-06-10-goodreads/index.html#searching-for-my-type",
    "title": "goodreads",
    "section": "Searching for my type",
    "text": "Searching for my type\nI was curious to see whether there authors that I should be reading more of, so I averaged all my ratings for each author.\nRanked data doesn’t show very nicely when you try to do a simple correlation plot, but we can just check it out.\n\n\nCode\n# How off am I from the average rating for each author?\n# looks like 3 stars is a breaking point\ndf %&gt;% \n  group_by(author) %&gt;% \n  summarise(n = n(), \n            my_rating = mean(my_rating),\n            people_rating = mean(average_rating)) %&gt;% \n  ggplot(aes(my_rating, people_rating)) +\n  geom_point(alpha = 0.5)+\n  theme_linedraw() +\n  labs(x = \"My Rating for an Author\",\n       y = \"People's Rating an Author\",\n       title = \"Is there more agreement at the author level?\",\n       subtitle = \"There seems to be a breakpoint at 4\")"
  },
  {
    "objectID": "posts/2023-06-10-goodreads/index.html#as-years-go-by",
    "href": "posts/2023-06-10-goodreads/index.html#as-years-go-by",
    "title": "goodreads",
    "section": "As years go by",
    "text": "As years go by\nI have been adding info to goodreads for a while now, so I wanted to check things in time. In terms of quality, it seems that 2016 and 2023 were very good reading years. 2020 was quite bad.\n\n\nCode\ndf %&gt;% \n  # for some reason, \n  # date_read is na even though I did read them\n  filter(my_rating &gt; 0) %&gt;%\n  # fill with the date added, \n  # which should be close enough \n  # (anyway the only thing we have)\n  mutate(date_read = if_else(is.na(date_read), date_added, date_read)) %&gt;% \n  arrange(date_read) %&gt;% \n  group_by(lubridate::year(date_read)) %&gt;% \n  mutate(year_order = 1:n()) %&gt;% \n  ggplot(aes(year_order, lubridate::year(date_read), fill=factor(my_rating)))+\n  geom_tile(color = \"gray90\", linewidth = 0.5) +\n  paletteer::scale_fill_paletteer_d(\"NineteenEightyR::miami2\", direction = -1) +\n  scale_y_continuous(breaks = seq(2015, 2023, 1)) +\n  scale_x_continuous(limits = c(0, 50), \n                     breaks = c(1, 10, 20, 30, 40),\n                     expand = expansion(0, 0))+\n  labs(x = \"Books Read\", \n       y = element_blank(),\n       fill = \"My Rating\") +\n  theme(panel.background = element_rect(fill=\"gray4\"),\n        plot.background = element_rect(fill=\"gray4\"),\n        legend.position = \"bottom\",\n        panel.grid = element_blank(),\n        panel.grid.major.x = element_line(color = \"gray90\", linetype = 3),\n        legend.background = element_rect(fill=NA),\n        text = element_text(color = \"gray90\")) +\n  coord_fixed(ratio = 1.2)"
  },
  {
    "objectID": "posts/2023-06-10-goodreads/index.html#whats-the-best-book-length-for-me",
    "href": "posts/2023-06-10-goodreads/index.html#whats-the-best-book-length-for-me",
    "title": "goodreads",
    "section": "What’s the best book length for me?",
    "text": "What’s the best book length for me?\nOne quick an easy thing to check is whether I enjoy longer or shorter books. There doesn’t seem to be much to see here, but the plot was nice and I decided to keep it.\n\n\nCode\nmin_pages &lt;- 50\nmax_pages &lt;- 1000\n\ndf %&gt;% \n  filter(between(number_of_pages, min_pages, max_pages)) %&gt;% \n  ggplot(aes(x=my_rating, y=number_of_pages)) + \n  geom_point(alpha=.3, pch=\"-\", size=10) +\n  stat_summary(aes(x = my_rating + 0), \n               geom =\"pointrange\", color=\"steelblue\", fun.data=mean_se) +\n  #coord_flip() +\n  labs(title = \"Do I prefer longer books?\",\n       subtitle = glue::glue(\"Showing books with more than {min_pages} and less than {max_pages} pages.\"),\n       y = \"Number of Pages\", \n       x = \"My Rating\")+\n  theme_bw() +\n  scale_x_continuous(expand = c(0.3, 0), breaks = 1:5) +\n  scale_y_continuous(limits = c(0, 800))"
  },
  {
    "objectID": "posts/2023-06-10-goodreads/index.html#show-me-the-ratios",
    "href": "posts/2023-06-10-goodreads/index.html#show-me-the-ratios",
    "title": "goodreads",
    "section": "Show me the Ratios",
    "text": "Show me the Ratios\nI promised to check the gender ratios. So I used the gender package to predict gender using the information in the author’s name. There’s a few caveats:\n\nThe prediction might be dead wrong for many reasons. Author’s of the package acknowledge this right away, so I feel I shouldn’t repeat this info.\nThere are some authors that might fail to parse. Since this is a simple analysis, I didn’t want to delve too much on fixing things.\nSome books are written by more than one author2. For simplicity’s sake, I’m going to go with the first author (I doubt it would change the ratio anyway). In goodreads’ database, we have an author_l_f column with only one person. So that’s what I am going to be using.\n\n\n\nCode\n# Before doing anything, only look at data from before 2023\nbefore2023 &lt;- filter(df, \n                     lubridate::year(date_added) &lt; 2023)\n# get distinct authors\nauthor_names &lt;- unique(before2023$author_l_f)\n# split last-first and get first\nauthor_names &lt;- sapply(stringr::str_split(author_names, pattern = \", \"), function(x) x[[2]])\n# predict\ngender_prediction &lt;- gender::gender(author_names)\n\n\nThe gender dataset looks like this\n\n\nCode\ngender_prediction\n\n\n# A tibble: 117 × 6\n   name   proportion_male proportion_female gender year_min year_max\n   &lt;chr&gt;            &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 Adam            0.996             0.0041 male       1932     2012\n 2 Alan            0.997             0.0032 male       1932     2012\n 3 Alex            0.966             0.0343 male       1932     2012\n 4 Allie           0.0396            0.960  female     1932     2012\n 5 Amor            0.177             0.823  female     1932     2012\n 6 Anand           1                 0      male       1932     2012\n 7 Andrew          0.996             0.004  male       1932     2012\n 8 Andy            0.988             0.0119 male       1932     2012\n 9 Anne            0.0025            0.998  female     1932     2012\n10 Annie           0.0053            0.995  female     1932     2012\n# ℹ 107 more rows\n\n\nThe ratio of predicted names is 0.71, which is not bad. There are some authors missing that failed to parse. For example, J.K. Rowling:\n\n# fails to parse, also no Joannes in data!\nany(gender_prediction$name == \"Joanne\")\n# FALSE\n# She was in the dataset as J.K.\nany(author_names == \"J.K.\")\n# TRUE\n\nTo be honest, I am not worried about the parsing failures (it would take a long time to fix too…). If anything, it would make my male dominant dataset less male dominant. Let’s finally check this out.\n\n\nCode\ngender_prediction %&gt;% \n  count(gender) %&gt;% \n  mutate(percentage = round(n / sum(n), 3) * 100) %&gt;% \n  gt::gt(caption = \"Number of Books by Gender before 2023\")\n\n\n\n\n\n\nNumber of Books by Gender before 2023\n\n\ngender\nn\npercentage\n\n\n\n\nfemale\n16\n13.7\n\n\nmale\n101\n86.3\n\n\n\n\n\n\n\nI was aiming for 10%, so 13.7% is right there. Again, this is bad. But let’s check this year.\n\n\nCode\n# 2023 onwards\nafter2023 &lt;- filter(df, lubridate::year(date_added) &gt;= 2023)\n# get distinct authors\nauthor_names &lt;- unique(after2023$author_l_f)\n# split last-first and get first\nauthor_names &lt;- sapply(stringr::str_split(author_names, pattern = \", \"), function(x) x[[2]])\n# predict\ngender_prediction &lt;- gender::gender(author_names)\n\n\n\n\nCode\ngender_prediction %&gt;% \n  count(gender) %&gt;% \n  mutate(percentage = round(n / sum(n), 3) * 100) %&gt;% \n  gt::gt(caption = \"Number of Books by Gender after 2023\")\n\n\n\n\n\n\nNumber of Books by Gender after 2023\n\n\ngender\nn\npercentage\n\n\n\n\nfemale\n11\n42.3\n\n\nmale\n15\n57.7\n\n\n\n\n\n\n\nThis is WAY better: 42.3%! Yes, I did have to search google for good titles written by women. And yes, I do have access to a bookworm at home who happens to be my wife and can recommend me great pieces of writing. The good news is that the bar to find content by women was really low. It didn’t require extra effort on my side to vastly improve my reading experience.\nThe reading experience is the most important thing. I was missing out on great work. I cannot help but wonder why it was so difficult for me to get exposed to these pieces."
  },
  {
    "objectID": "posts/2023-06-10-goodreads/index.html#what-i-have-learnt",
    "href": "posts/2023-06-10-goodreads/index.html#what-i-have-learnt",
    "title": "goodreads",
    "section": "What I have learnt",
    "text": "What I have learnt\nI confirmed my own bias (~10% of content written by women) and found ways to improve it (currently getting to 50% for 2023!).\nI have found amazing books written by female authors this year. To name a few, I have read Educated, Infidel, Tomorrow, and Tomorrow, and Tomorrow, I’m Glad My Mom Died, and My year of meats. All of them 5 stars! The lessons learned are outside the scope of this piece.\nI am confident that there’s much else to learn from female voices. A huge world that I would never experience if I were to stay within my biased interpretation of reality. I hope to continue pushing for a higher women ratio. It’s not that I was actively trying to avoid reading women, but it’s quite telling that even these masterpieces didn’t make it to my reading list without me purposefully searching for “Good books by women” and the like."
  },
  {
    "objectID": "posts/2023-06-10-goodreads/index.html#footnotes",
    "href": "posts/2023-06-10-goodreads/index.html#footnotes",
    "title": "goodreads",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI sometimes believe that it is unfair to rate a book you didn’t read. If I start a book and don’t like it, but having committed through it, I don’t rate it.↩︎\nI am reading Poor Economics co-written by a man and a woman. This approach would remove Duflo from the analysis.↩︎"
  },
  {
    "objectID": "posts/2021-04-25-Icebreaker/index.html",
    "href": "posts/2021-04-25-Icebreaker/index.html",
    "title": "Icebreaker",
    "section": "",
    "text": "There are many managers I admire, some of whom I was lucky personally meet, others I “know” through reading their books/blogs.\nI have have borrowed different questions and suggestions I liked from these managers to create an Icebreaker, a set of questions that can quickly provide me information about a new person I’m working with. At the same time, I wrote this post to provide new people in our team with basic knowledge about me, the new person they are working with.\nHere are the questions and my answers1.\nAverage. As of 2021, I am transitioning from night owl to early bird. Ask me again in a couple years.\nI generally dislike to be praised. If anything must be said, I prefer that private and informal.\nUnless it’s concrete feedback about a piece of work, something that is better written onto the work itself (e.g., code comments), I prefer conversation during a meeting. Written communication must be super accurate to avoid misunderstandings.\nBoth.\nFood.\nIn general, having to do too many thing at the same time makes me grumpy and turns me off. Scheduling conflicts turn me off.\nMy face will be good evidence.\nI believe I need more time to reflect and brainstorm. I have a thing for going after that new shiny challenge and investing more time on that than something that has known problems that must be solved.\nI work best with people who communicate often and honor commitments. I work best with people who communicate with candor and create an atmosphere of trust where we can say things to each other because everyone wants the best for their teammate.\nAdvice. Guidance. Intellectual stimulation.\nAdvice. Guidance. Intellectual stimulation. Space to think, support to do, pressure to maintain my work level and keep improving.\nIn general you can interrupt me. When I’m coding or reading are two of the worst moments though.\nDisplays of preference towards a member of the team. Being extremely hands-off, especially as a result of not liking the member’s work or the outcomes the work is generating."
  },
  {
    "objectID": "posts/2021-04-25-Icebreaker/index.html#footnotes",
    "href": "posts/2021-04-25-Icebreaker/index.html#footnotes",
    "title": "Icebreaker",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMy own answers might change overtime, I will try my best to keep a note of that change in this log.↩︎"
  },
  {
    "objectID": "posts/2020-06-13-python-keylogger/index.html",
    "href": "posts/2020-06-13-python-keylogger/index.html",
    "title": "Python Keylogger",
    "section": "",
    "text": "I have been struggling a lot with pain in my fingers and wrists1.\nI have tried setting up timers for using the keyboard. But the only thing that seems to work is reducing the typing (aka get away from the computer and stop using the keyboard!).\nTracking hours didn’t work well, so I guess a last resort to force me is counting the actual number of keys per day and trying to keep that to a minimum.\nAfter a little google searching, I found inspiration on this post: Design a Keylogger in Python.\nI wanted something simple. Just count and save to file, in case future me wants to do some sort of analysis. Nothing fancy, no optimization. Most importantly something I can trust is not sending every keystroke I write over the internets.\nYou can read the pynput docs here.\nOf note, pip3 install pynput failed. If only python made it possible for people to install things … 🤷\nAnyway, mystery aside, python -m pip install pynput worked. I kind of trust this library is safe enough (read: blind faith in open-source) . And my passwords are kinda there scrambled somewhere in the text file but I don’t plan to host the file anywhere so it’s reasonably safe (please don’t get remote access to my computer 🙏).\nThe output to console (again, keep it simple) looks like this:\nFor those of you who want to use it, you can find the code below:\nCheck the Code\nfrom pynput.keyboard import Key, Listener\nimport logging\nimport os\nimport datetime\n\n# log_dir defaults to Desktop\nlog_dir = '/home/matias/Desktop'\n# updates every 100 keystrokes\nupdate_every = 1000\nday_count = 0\ntoday = datetime.date.today().isoformat()\n   \ndef on_press(key):\n    logging.info(str(key))\n\ndef key_count(key):\n    global today, day_count, update_every\n    if today == datetime.date.today().isoformat():\n        if day_count % update_every == 0:\n            print(f\"Today is {today} and the key count is: {day_count}\")\n        # always update the counter\n        day_count = day_count + 1\n    else:\n        # update today's value\n        today = datetime.date.today().isoformat()\n        print(\"Today is a brand new day :)\")\n        # reset the counter\n        day_count = 0\n    return\n\ndef main():\n    logging.basicConfig(filename = (os.path.join(log_dir, \"keylog.txt\")),\n     level=logging.DEBUG,\n     format='%(asctime)s: %(message)s')\n    with Listener(\n        on_press=on_press,\n        on_release=key_count) as listener:\n        listener.join()\n\nif __name__ == '__main__':\n    print(\"Starting keylogger\")\n    main()"
  },
  {
    "objectID": "posts/2020-06-13-python-keylogger/index.html#footnotes",
    "href": "posts/2020-06-13-python-keylogger/index.html#footnotes",
    "title": "Python Keylogger",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt’s 2023 and I still struggle with this. Buy a proper keyboard and pay attention to ergonomics. Physical Therapy helps. Get help. Take care of yourself.↩︎"
  },
  {
    "objectID": "posts/2019-04-14-data-visualization-challenge-2/index.html",
    "href": "posts/2019-04-14-data-visualization-challenge-2/index.html",
    "title": "Data Visualization Challenge 2",
    "section": "",
    "text": "This post is made as a backup for the data visualization challenge number 2. Data comes from the daily posts of the members of the Data Visualization Society (DVS) on the DVS Slack channels. You can see everybody’s submissions for the challenge here.\nI am also very motivated to explore the dark versions of the ggplot themes. The package I’m going to be using is called ggdark."
  },
  {
    "objectID": "posts/2019-04-14-data-visualization-challenge-2/index.html#dive-in",
    "href": "posts/2019-04-14-data-visualization-challenge-2/index.html#dive-in",
    "title": "Data Visualization Challenge 2",
    "section": "Dive in",
    "text": "Dive in\nThese are the libraries we’ll need:\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(ggdark)\nlibrary(lubridate)\n\n\nWe read the data from the repository.\n\n\nShow the code\ndf &lt;- read_csv(\"https://raw.githubusercontent.com/data-visualization-society/datavizsociety/master/challenge_data/dvs_challenge_2_channel_topics_over_time/flattened_channel_data.csv\")\n\n\nLet’s perform some summary stats. There’s 62 channels, but I will focus on the top 15 channels as ranked by their total volume of characters. I’m using this metric because the correlation between characters and the number of posts is, naturally, good.\n\n\nShow the code\nggplot(df, aes(posts+responses, characters))+\n  geom_smooth(method = \"lm\", se=FALSE, lty=2)+\n  geom_point(alpha=0.4)+\n  dark_theme_bw()+\n  scale_y_continuous(labels = scales::label_number_si())\n\n\n\n\n\n\n\n\n\nSummary.\n\n\nShow the code\nsum_df &lt;- df %&gt;% group_by(channel) %&gt;%\n  summarise(total_channel = sum(characters),\n         median_channel = median(characters)) %&gt;%\n  top_n(15, wt =  total_channel) %&gt;%\n  arrange(desc(total_channel))\n\n\nModify the original data and do some stats.\n\n\nShow the code\ndf &lt;- df %&gt;% group_by(channel) %&gt;%\n  mutate(total_channel = sum(characters),\n         median_channel = median(characters),\n         char_per_ping = characters/(posts+responses))   %&gt;%\n  ungroup() %&gt;%\n  group_by(date) %&gt;%\n  mutate(daily_flow = sum(characters),\n         daily_posts = sum(posts+responses))%&gt;%\n  ungroup()"
  },
  {
    "objectID": "posts/2019-04-14-data-visualization-challenge-2/index.html#first-pair",
    "href": "posts/2019-04-14-data-visualization-challenge-2/index.html#first-pair",
    "title": "Data Visualization Challenge 2",
    "section": "First pair",
    "text": "First pair\nThe idea behind the first pair of plots is to see the sheer amount of volume on certain channels.\nA good way of seeing how the top channels are ordered according to output is to do an ordered boxplot.\n\n\nShow the code\ntop_box &lt;-  df %&gt;%\n  filter(channel %in% unique(sum_df$channel)) %&gt;%\n  mutate(channel=fct_reorder(factor(channel), median_channel)) %&gt;%\nggplot(aes(channel, log10(characters)))+\n  geom_boxplot()+\n  coord_flip()+\n  dark_theme_bw()+\n  labs(x=\"\")+\n  ggtitle(sprintf(\"Top %s Channels\",\n                  length(unique(sum_df$channel))),\n          \"Metric: median characters\")\n\n\nI’m also curious about how persistent in time the flow is.\n\n\nShow the code\nwave &lt;- df %&gt;%\n  filter(channel %in% unique(sum_df$channel)) %&gt;%\n  mutate(channel=fct_reorder(factor(channel), total_channel)) %&gt;% \n  ggplot(aes(date, channel, color=log10(characters))) +\n  geom_line(aes(lwd=characters))+\n  dark_theme_bw()+\n  labs(y = \"\", x=\"Date\")+\n  guides(color = FALSE)+\n  scale_color_gradient(low = \"#613A00\", high=\"#FA9800\")+\n  ggtitle(\"Top 15 channels\", \n          \"Metric: total characters\")+\n  scale_y_discrete(position = \"right\")+\n  theme(legend.position = \"none\")\n\n\nWe put everything together with the cowplot package.\n\n\nShow the code\ncowplot::plot_grid(top_box, wave)\n# Save the plot\n#ggsave(\"box_wave.svg\", width = 8, height = 4, units = \"in\", dpi=\"retina\")\n\n\nI later modified this output a bit using Inkscape."
  },
  {
    "objectID": "posts/2019-04-14-data-visualization-challenge-2/index.html#lengthy-channels",
    "href": "posts/2019-04-14-data-visualization-challenge-2/index.html#lengthy-channels",
    "title": "Data Visualization Challenge 2",
    "section": "Lengthy channels",
    "text": "Lengthy channels\nWhile most of the channels have a low median, even below a full tweet, it looks like some channels tend to have very lengthy posts.\n\n\nShow the code\n# Calculate median\nmedian_post &lt;- median(\n  df$characters/(df$posts +df$responses))\n\n# Do the plot  \nlengthy &lt;- ggplot(df, aes(log10(total_channel),\n               char_per_ping))+\n  dark_theme_bw()+\n  geom_hline(yintercept = 280, lty=2)+\n  geom_hline(yintercept = median(\n    df$characters/(df$posts +df$responses)), lty=2)+\n  annotate(\"text\", x = 3, y= c(200, 340), label=c(\"Median post\",\n                                            \"One tweet\"))+\n         geom_point(aes(color=channel), alpha=0.9)+\n         scale_color_viridis_d(direction = -1)+\n         theme(legend.position = \"none\")+\n  ggrepel::geom_text_repel(data=filter(df,\n                                       char_per_ping &gt; 850),\n            aes(label = channel, color=channel))+\n  labs(x=bquote(\n    log10 ~\"(total characters)\"),\n    y=\"characeters per post\")+\n  ggtitle(\"Channels with lengthy posts\")\n\n\n# Save\n# ggsave(\"lengthy.svg\", width = 8, height = 4, units = \"in\",dpi=\"retina\")"
  },
  {
    "objectID": "posts/2019-04-14-data-visualization-challenge-2/index.html#share-of-flow",
    "href": "posts/2019-04-14-data-visualization-challenge-2/index.html#share-of-flow",
    "title": "Data Visualization Challenge 2",
    "section": "Share of flow",
    "text": "Share of flow\nWhat is the share of each channel on the total flow within the DataViz Slack?\n\n\nShow the code\ntop_top_channels &lt;- sum_df %&gt;%\n  arrange(desc(total_channel)) %&gt;%\n  slice(1:5)\n\nshare &lt;- df %&gt;% group_by(date) %&gt;%\n  mutate(big_channel = ifelse(channel %in% top_top_channels$channel,\n                              channel, \"other\"),\n    total=sum(characters),\n    rel_char = characters/total) %&gt;%\n  ggplot(aes(date, rel_char, fill=big_channel))+\n  geom_col(width = 1)+\n  scale_fill_viridis_d(direction = -1)+\n  dark_theme_bw()+\n  theme(legend.position=c(.85,.5))+\n  labs(x=\"\", y=\"Relative share\", fill=\"Channel\")+\n  ggtitle(\"Share of the conversation\",\n          \"Relative share of the total characters per day\")+\n  scale_x_date(limits = c(as.Date(\"2019-02-18\"),\n                          as.Date(\"2019-04-23\")),\n               date_breaks = \"1 week\", \n               date_labels = \"%b-%d\")\n\n\nIt seems the initial bump was driven by many (lengthy) introductions, and nowadays the discussion has moved towards other channels.\n\n\nShow the code\nintro_decay &lt;-  ggplot(df, aes(date, daily_flow))+\n  geom_line()+\n  geom_line(data=filter(df, channel %in% c(\n    \"-introductions\")),\n            aes(date, characters), color=\"yellow\")+\n  dark_theme_bw()+\n  xlab(\"\") + \n  ylab(\"Daily characters\")+\n  annotate(\"text\", x=as.Date(c(\"2019-04-10\",\n                               \"2019-04-08\")),\n           y = c(1000, 50000),\n           label=c(\"-introductions\", \"all channels\"),\n           color=c(\"yellow\", \"white\"))+\n    scale_x_date(limits = c(as.Date(\"2019-02-18\"),\n                              as.Date(\"2019-04-23\")),\n                 date_breaks = \"1 week\", \n                 date_labels = \"%b-%d\") +\n  scale_y_continuous(labels = scales::label_number_si())\n\n\nLet’s see how it looks like.\n\n\nShow the code\n# We put everything together with cowplot\ncowplot::plot_grid(share,intro_decay,\n                   nrow = 2, rel_heights = c(2,1))\n# save\nggsave(filename = \"share_plot.svg\", \n       width = 12, \n       height = 6, \n       dpi=\"retina\")\n\n\nThe final version is this one."
  },
  {
    "objectID": "posts/2019-04-14-data-visualization-challenge-2/index.html#weekday-news",
    "href": "posts/2019-04-14-data-visualization-challenge-2/index.html#weekday-news",
    "title": "Data Visualization Challenge 2",
    "section": "Weekday news",
    "text": "Weekday news\nBecause everything is seasonal, let’s analyze by days of the week. Seems like Tuesday to Thursday are the days with most movement, waning down on Friday and into the weekend.\n\n\nShow the code\nggplot(df, aes(wday(date, label=TRUE, abbr = TRUE, week_start = 1),\n               daily_posts))+\n  geom_line(color=\"gray80\")+\n  stat_summary(geom = \"point\",\n               fun = median, size=2.5)+\n  dark_theme_bw()+\n  labs(x=\"\", y=\"Number of daily posts\",\n       title = \"Weekly post variations\",\n       subtitle = \"Points represent median daily post.\\nLines show full data range.\")\n\n# ggsave(filename= \"weekly_vars.svg\", width = 8, height = 6 , dpi=\"retina\")"
  },
  {
    "objectID": "posts/2017-12-17-any-questions/index.html",
    "href": "posts/2017-12-17-any-questions/index.html",
    "title": "Any Questions?",
    "section": "",
    "text": "I’m writing this post to document my own learning on reading from Teaching Tips and Teaching at its Best. This particular post has to do with engaging students to contribute with questions to class."
  },
  {
    "objectID": "posts/2017-12-17-any-questions/index.html#checking-student-understanding",
    "href": "posts/2017-12-17-any-questions/index.html#checking-student-understanding",
    "title": "Any Questions?",
    "section": "Checking student understanding",
    "text": "Checking student understanding\nDuring class, there is a moment in which you feel that you’ve covered a couple of ideas in the way you wanted to and you decide to check if everything is alright. Other times, you feel as if you weren’t able to convey the points in the way you wanted to. Again, the same feeling hits you and you fall into the following invitation to the students:\n\nAny questions?\n\nNo, no, no and no…wrong way to go! You get silence. Ten more seconds. You get looks that are impossible to decipher. Yes, if you are lucky enough you might get a hand up, willing to break the frozen air. But usually that’s not the case and you get trapped into believing that, indeed, there are no questions. So, how should we foster greater participation for self-assessment in class? How can we facilitate manifestation of doubts?\nI’ll give more tries to this active pause method:\n\nTake one minute to review your notes.\nWrite a question you would like to answer.\nShare with a peer you questions.\nShare with the class and/or turn in such questions."
  },
  {
    "objectID": "posts/2017-12-17-list-common-errors/index.html",
    "href": "posts/2017-12-17-list-common-errors/index.html",
    "title": "List of Common Errors",
    "section": "",
    "text": "I’m writing this post to document my own learning on reading from Teaching Tips and Teaching at its Best."
  },
  {
    "objectID": "posts/2017-12-17-list-common-errors/index.html#assignments-and-feedback",
    "href": "posts/2017-12-17-list-common-errors/index.html#assignments-and-feedback",
    "title": "List of Common Errors",
    "section": "Assignments and Feedback",
    "text": "Assignments and Feedback\nChapters 7 and 8 on McKeachie’s Teaching tips are quite interesting and develop different lines of thought about assignments. One thing I can relate to is the idea of providing general examples of errors students normally do. This procedure comes handy when dealing with essay questions (the ones I like far more than any other). The idea is to provide feedback that can be helpful while, at the same time, having a place for students to identify common mistakes and help them internalize good practices while writing.\nI will be developing further this idea, polishing the list and hopefully including it on my future rubrics.\n\nIdea needs further development.\nSupporting evidence is absent or insufficient.\nUnrelated to the question."
  },
  {
    "objectID": "posts/2021-01-02-art-in-a-new-year/index.html",
    "href": "posts/2021-01-02-art-in-a-new-year/index.html",
    "title": "Art in a New Year",
    "section": "",
    "text": "I have not made resolutions for this year1. I have instead spent that time playing with patterns on a canvas. It’s been a while since I wanted to have some time to do generative art using geometric figures and I finally got down to it on the very first day of this year. I found a fountain of joy after just a few hours of tinkering, something inside tells me this could be a new New Year’s tradition for me.\nI started out with a simple form like a hexagon and started looking for patterns that were appealing to me, like the color palette.\nSomething that I like about generative art is that I’m not in control, it’s a discovery process. I just set a few rules and pleasant aesthetics appear before me. The patterns might appear by design, but often they are the result of a benign error that created something beautiful. After tinkering with these patterns for a bit, I arrived at these new forms. They take me to a special place.\nI have selected some of the results that I like the most below."
  },
  {
    "objectID": "posts/2021-01-02-art-in-a-new-year/index.html#footnotes",
    "href": "posts/2021-01-02-art-in-a-new-year/index.html#footnotes",
    "title": "Art in a New Year",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWell, at least not in the classical way, mostly just wishing we could get back to normal life sometime before 2022.↩︎"
  },
  {
    "objectID": "posts/2020-01-02-escribiendo-en-pocas-palabras/index.html",
    "href": "posts/2020-01-02-escribiendo-en-pocas-palabras/index.html",
    "title": "Escribiendo en pocas palabras",
    "section": "",
    "text": "Lo bueno, si breve, dos veces bueno.\nMe tomo la libertad de este juego literario1 para introducir un ensayo con algunas pautas interesantes para escribir. En pocas palabras, Paul Graham nos regala una lista de acciones que es conveniente tomar para producir escritos de calidad.\nLa siguiente es una traduccion del ensayo “Writing, Briefly” escrito por Paul Graham2."
  },
  {
    "objectID": "posts/2020-01-02-escribiendo-en-pocas-palabras/index.html#escribir-en-pocas-palabras",
    "href": "posts/2020-01-02-escribiendo-en-pocas-palabras/index.html#escribir-en-pocas-palabras",
    "title": "Escribiendo en pocas palabras",
    "section": "Escribir, en pocas palabras",
    "text": "Escribir, en pocas palabras\n(En el proceso de responder un email, accidentalmente escribí un pequeño ensayo sobre cómo escribir. Normalmente dedico semanas a cada ensayo. Este tardó 67 minutos – 23 para escribirlo y 44 para reescribirlo.)\nCreo que escribir bien es mucho más importante que lo que la gente se imagina. Escribir no sólo comunica ideas, las genera. Si eres malo escribiendo y no te gusta hacerlo, perderás la oportunidad de generar la mayoría de las ideas que la escritura te hubiera regalado.\nEn cuanto a cómo escribir bien, aquí dejo una breve sugerencia: Escribe una primera versión mala tan rápido como puedas; reescríbela una y otra vez; remueve todo aquello que sea innecesario; escribe en un tono de conversación; desarrolla un buen olfato para detectar malos escritos, de modo que así puedas arreglar los tuyos; imita a escritores que te gusten; Si no puedes empezar, cuéntale a alguien sobre lo que planeas escribir, luego escribe lo que les contaste; ten en cuenta que 80% de las ideas en un ensayo ocurren después de empezar a escribirlo, y 50% de las ideas con las que empiezas estarán erradas; mantén la confianza para remover texto; comparte tus textos con amigos de confianza, para que te digan qué partes son confusas o lentas; no siempre es necesario empezar con un esquema del texto; reflexiona sobre las ideas por un par de días antes de escribirlas; lleva contigo un anotador o pedazos sueltos de papel; empieza a escribir cuando pienses en la primera oración; si una fecha límite te fuerza a empezar antes de tener la primera frase, escoge la frase más importante para comenzar; escribe sobre cosas que te gusten; no trates de impresionar con tu vocabulario; no dudes en cambiar el tópico en el medio del texto; usa notas al pie para conservar digresiones; usa anáforas para entretejer las oraciones; lee tus ensayos en voz alta para ver (a) dónde suenan extraños y (b) qué pedazos son aburridos (aquellos que odias leer); intenta decirle al lector algo nuevo y útil; trabaja durante períodos prolongados; cuando debas comenzar de nuevo, hazlo releyendo lo que tienes hasta ahora; cuando debas terminar, deja algo que facilite comenzar de nuevo; acumula notas de tópicos que planeas escribir al final del archivo; no te sientas obligado a tratar ninguno de ellos; escribe para un lector que no va a leer tu ensayo con tanto cuidado como tú, del mismo modo que las canciones pop fueron diseñadas para sonar de manera aceptable en radios baratas; si dices algo equivocado, corrígelo inmediatamente; pregunta a amigos cuáles son las oraciones de las que te arrepentirás; vuelve al texto y baja el tono de los comentarios duros; publica tus textos online, porque tener una audiencia te hace escribir más y, por lo tanto, generar más ideas; imprime borradores en vez de mirarlos en la pantalla; utiliza palabras simples; aprende a distinguir sorpresas de digresiones; aprende a reconocer la oportunidad para finalizar y, cuando uno aparezca, tómala."
  },
  {
    "objectID": "posts/2020-01-02-escribiendo-en-pocas-palabras/index.html#footnotes",
    "href": "posts/2020-01-02-escribiendo-en-pocas-palabras/index.html#footnotes",
    "title": "Escribiendo en pocas palabras",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLa coma en “Writing, briefly” cambia el significado de “escribiendo” a “escribir”. Me permito jugar a estar escribiendo en pocas palabras sobre cómo escribir en pocas palabras.↩︎\nhttp://www.paulgraham.com/writing44.html↩︎"
  },
  {
    "objectID": "posts/2017-12-28-rules-composition/index.html",
    "href": "posts/2017-12-28-rules-composition/index.html",
    "title": "Rules of Composition",
    "section": "",
    "text": "There is a table in the hallway of the Psychology builiding at UMass Amherst. It’s the “Free Stuff” Table. As I was passing by, I came across The Writing Teacher’s Sourcebook edited by Gary Tate and Edward P.J. Corbett. I glanced over it today and found these fantastic rules. The authors called them rules of composition. I thought I should share. This list has no numeric order, author’s touch there1.\nEleventh law of composition: Some things precede other things. Invention precedes structure. Thinking and feeling and being precede writing. Structure made without invention are false or superficial. There probably is a fit sequencing of things, even if we don’t always see it.\nEighteenth law of composition: You are always standing somewhere when you say something. You are in a world, you have thoughts, you’ve made choices (whether or not consciously) any time you say anything. If you are in a position whenever you say anything, it’s probably best to know what the position is.\nTwenty-fifth law of composition: Invention is an invitation to openness. It asks of you that you open yourself to the ways other people think, to the knowledge that already exists, to the intricacies and whims of your own beings. It asks of you that you therefore be tentative a while, consider alternatives a while, be in process a while.\nTwenty-sixth law of composition: But structure is a closure. You can’t organize an essay or a sonata unless you have ruled out other organizations. When structure begins to be made, you are no longer open: you have made choices.\nTwenty-seventh law of composition: Invention and structure, then, represent a way of being in the world. They exert certain demands upon you, and they afford you certain pleasures. Invention invites you to be open to a creation filled with copious wonders, trivialities, sorrows, and amazements. Structure requires that you close. You are asked to be open and always closing.\nThirty second law of composition: What follows feeds, enlarges, and enriches what precedes. Invention precedes and is open. Structure follows and closes. That may seem a narrowing disappointment, a ruling out of possibilities. It needn’t be. Every choice, every decision, every structure has the potential of being another entry in the inventive world you live in, modifying it, punching it in here, punching it out there, giving color to it yonder. Invention precedes, structure follows, but invention does not cease thereby. The structure we make today may give grace to tomorrow’s invention. That means that if today we fail to be wise and generous and good, tomorrow we may succeed, and if not, we may fail at a higher level."
  },
  {
    "objectID": "posts/2017-12-28-rules-composition/index.html#footnotes",
    "href": "posts/2017-12-28-rules-composition/index.html#footnotes",
    "title": "Rules of Composition",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThese were extracted from the “What I learned at school” chapter written by Jim W. Corder.↩︎"
  },
  {
    "objectID": "posts/2018-04-20-t-test-correlation/index.html",
    "href": "posts/2018-04-20-t-test-correlation/index.html",
    "title": "t-tests for correlation",
    "section": "",
    "text": "I have been having a recurrent nightmare in the Statistics class I’m teaching. The nightmare goes like:\nIt also comes in the shape of non-significant data, maybe even more haunting than the first version.\nThese phrases are not actual statements made by my students on paper but they come quite close. I have been trying to wrap my head around their confusion between correlation and t-test, which persists even though I remove points on papers and make announcements in class. It persists whether I repeat the material from a different angle, draw stuff on the board, and have face to face interaction with firm believers of the t-correlation theory. I have made frustrated comments during TA meetings only to find that other TAs experience the same issue. One of the TAs was adamant about it, she even makes them sing! But the confusion is still intact."
  },
  {
    "objectID": "posts/2018-04-20-t-test-correlation/index.html#how-to",
    "href": "posts/2018-04-20-t-test-correlation/index.html#how-to",
    "title": "t-tests for correlation",
    "section": "How to",
    "text": "How to\nIt’s not my full intention to preach about what a t-test or a correlation analysis are (although see below). I’m trying to figure out what makes students be so confused about these two. More thoughts on this topic will come in the near future."
  },
  {
    "objectID": "posts/2018-04-20-t-test-correlation/index.html#two-different-worlds",
    "href": "posts/2018-04-20-t-test-correlation/index.html#two-different-worlds",
    "title": "t-tests for correlation",
    "section": "Two different worlds",
    "text": "Two different worlds\nCorrelation and factor analysis are essentially different. Continuous vs discrete. A car’s transmission can be automatic or manual. There is no scale, no range of values. We can study how a continuous variable (such as miles per gallon) is affected by the type of transmission.\n\n\n\n\n\n\n\n\n\nMoreover, we are trying to collapse the groups into a comparison of their means, standard deviations, and N. But let’s focus on the means, let’s reduce the previous scatterplot to two dots.\n\n\n\n\n\n\n\n\n\nThe underlying question is: Are these 2 values statistically different? We can test for it, using a t-test!\nUsing a t-test, we can find that the magnitude of the difference between groups is 7.245, which is unlikely to be found by chance. If our assumptions hold, the probability of finding such a difference when there is none is p=0.001.\nOn the other hand, we could be interested in studying the relationship between two continuous variables. The nature of these variables is fairly different from discrete ones. Both variables are unconstrained regarding the values they can take (Big, small and fractions allowed!). In this case, we can take a look at how the weight of cars is related to the miles per gallon.\n\n\n\n\n\n\n\n\n\nWe would see that, overall, heavier cars tend to be able to drive less miles per gallon of gas. In principle, with a correlation analysis we wouldn’t be able to establish causal relationships. Is it the weight causing the mileage per gallon to go down? We can’t address that question with this type of analysis. But we can test whether there is a significant association using a linear correlation test.\nThe correlation analysis shows a significant negative correlation (r = -0.868), which is different from zero, unlikely to be found by chance (p = \\(1.2939587\\times 10^{-10}\\))."
  },
  {
    "objectID": "posts/2019-02-01-r-pipeline-its-a-trap/index.html",
    "href": "posts/2019-02-01-r-pipeline-its-a-trap/index.html",
    "title": "R pipeline – It’s a trap!",
    "section": "",
    "text": "Here’s how the story usually goes: It starts with you, sitting in front of the computer screen, thinking: What a lovely day for some data analysis!.\nThis day is great because you have collected some data or maybe because you happened to find some data somewhere, begging to be part of a smoking good plot."
  },
  {
    "objectID": "posts/2019-02-01-r-pipeline-its-a-trap/index.html#raw-data-inside-tin-software",
    "href": "posts/2019-02-01-r-pipeline-its-a-trap/index.html#raw-data-inside-tin-software",
    "title": "R pipeline – It’s a trap!",
    "section": "Raw data inside Tin Software",
    "text": "Raw data inside Tin Software\nHaving data feels so good, that itching feeling in your fingers, the rush to know. How does it look like? You need to know. But let’s be honest, no dataset comes clean. You have to fight your way to finally get the squeaky clean readable data.frame.\nWhy is data dirty? Well, there are many reasons. Today I want to focus on the manufacturers.\nManufacturers don’t care for tidiness or openness, they use the format that works for them. They care for proprietary software solutions that help them help you, for a little contribution 1."
  },
  {
    "objectID": "posts/2019-02-01-r-pipeline-its-a-trap/index.html#reclaiming-your-data",
    "href": "posts/2019-02-01-r-pipeline-its-a-trap/index.html#reclaiming-your-data",
    "title": "R pipeline – It’s a trap!",
    "section": "Reclaiming your data",
    "text": "Reclaiming your data\nOf course, your data is your own and there has to be a way of getting it out of the tin software. So, you pull up your sleeves and go for it: there must be a way to get a text file out of this. Fortunately, but not after a lot of clicking, you find a way to get your raw_data.csv file exported."
  },
  {
    "objectID": "posts/2019-02-01-r-pipeline-its-a-trap/index.html#untitled1",
    "href": "posts/2019-02-01-r-pipeline-its-a-trap/index.html#untitled1",
    "title": "R pipeline – It’s a trap!",
    "section": "Untitled1",
    "text": "Untitled1\nYou are now more than ready to read.csv(...) your file. You open a new empty script and rush through commands. Read it. Do some quick wrangling. Maybe even a manual edit? The goal at this stage is to swift through it. You accept hard-coding some values, why not?\ndf &lt;- df[3:3245, 18:22] doesn’t sound that bad at this stage.\nBefore you can tell, you end up with less than 50 lines of code that accomplish the task, a sort of x + y + z -&gt; result. This feels good!"
  },
  {
    "objectID": "posts/2019-02-01-r-pipeline-its-a-trap/index.html#plot",
    "href": "posts/2019-02-01-r-pipeline-its-a-trap/index.html#plot",
    "title": "R pipeline – It’s a trap!",
    "section": "Plot",
    "text": "Plot\nYou plot the data. Yes, you might even use base graphics at this stage. It looks horrible, but the relationship is there, and you smile. It’s encouraging, in your next meeting, you will discuss the great advances in your research line (added n=3 to a group)."
  },
  {
    "objectID": "posts/2019-02-01-r-pipeline-its-a-trap/index.html#periodic",
    "href": "posts/2019-02-01-r-pipeline-its-a-trap/index.html#periodic",
    "title": "R pipeline – It’s a trap!",
    "section": "Periodic",
    "text": "Periodic\nBut something holds you when you try to escape back to the real world. You know that this process is gonna happen more than once. Hopefully, every week or so, you’re going to get your sample of interest read in that machine of hell and, if you want a quick taste of that sweet data, you’d better clean your script and make it a nice pipeline. Here’s where your Advanced R (http://adv-r.had.co.nz/) voice whispers to you: Everything to parameters, parameters to functions."
  },
  {
    "objectID": "posts/2019-02-01-r-pipeline-its-a-trap/index.html#functions",
    "href": "posts/2019-02-01-r-pipeline-its-a-trap/index.html#functions",
    "title": "R pipeline – It’s a trap!",
    "section": "Functions",
    "text": "Functions\nLet’s think in more general terms now…Where is the regular expressions God when I need it?\nYou don’t really remember, but there’s Google and there’s stringr. You’re quite sure you’ll find a way to extract all relevant info and recycle it through the pipeline. You create universal values that are shared, ID_this, ID_that. Naming names wasn’t that difficult after all!\nYour vision is still clear, maybe it’s because of the coffee, so you quickly wrap your sequential steps into functions that directly resemble them. You proudly name the files after the steps they perform (as if you would remember what clean_and_tidy_df.R does three months from now).\nYou still have a sequence: it now looks like x = f(...) + y = g(...) + z = h(...) -&gt; result. Not bad for a couple hours of work!"
  },
  {
    "objectID": "posts/2019-02-01-r-pipeline-its-a-trap/index.html#climax",
    "href": "posts/2019-02-01-r-pipeline-its-a-trap/index.html#climax",
    "title": "R pipeline – It’s a trap!",
    "section": "Climax",
    "text": "Climax\nYou find yourself at the climax of your endeavor. You can easily do a sequence of calls to functions. Moreover, everything is fresh in your memory and you have not tested your functions extensively. Things appear to work because you haven’t invested enough effort to break them.\n\nWhat will happen when dates change?\nIs your description of the problem(s) correct?\nAre your solutions working despite not being authoritative (aka, how many unknown unknowns waiting to bite you?)?\nYou are the only one running this code now, is that going to be the case forever?\nPackages will update tomorrow. Are you ready for that?\n\nAll this ignorance feels a lot like happiness. Again, you are strongly tempted to get up from your desk and go live your life."
  },
  {
    "objectID": "posts/2019-02-01-r-pipeline-its-a-trap/index.html#wake-up",
    "href": "posts/2019-02-01-r-pipeline-its-a-trap/index.html#wake-up",
    "title": "R pipeline – It’s a trap!",
    "section": "Wake up",
    "text": "Wake up\nIt doesn’t really matter how long it takes, your code will break. You realize your functions depend on global parameters, the date, the folder structure, the file names, and a stupid pattern on row 7 that is manually typed on machine of hell and therefore might contain human errors.\nYou cry, at least for a microsecond. Then you decide it’s time to bring the big guns to the fight. Everything will be standardized. It’s time for list.files() and lapply() to save the day."
  },
  {
    "objectID": "posts/2019-02-01-r-pipeline-its-a-trap/index.html#everything-is-a-list",
    "href": "posts/2019-02-01-r-pipeline-its-a-trap/index.html#everything-is-a-list",
    "title": "R pipeline – It’s a trap!",
    "section": "Everything is a list",
    "text": "Everything is a list\nYou create wrapper_of_clean_and_tidy_df.R and all sorts of other wrapper functions that can handle lists. Why? Because everything is a list now. You will load all the data, make a huge list, loop over, and apply all-purpose functions. Remember x? Neither do I, but it’s somewhere there, running at the bottom.\nBut this list -&gt; list -&gt; list -&gt; list of results is a bit confusing, where is the stuff I care about?. Don’t even get me started with the lists of lists."
  },
  {
    "objectID": "posts/2019-02-01-r-pipeline-its-a-trap/index.html#checkpoints",
    "href": "posts/2019-02-01-r-pipeline-its-a-trap/index.html#checkpoints",
    "title": "R pipeline – It’s a trap!",
    "section": "Checkpoints",
    "text": "Checkpoints\nBy this point, even if you will be the only one running this code, you are in deep need of some hints that point you towards what is going on.\nYou also have big objects that were only meaningful on the very first time, things that you never want to calculate again. But What if some day…? Yes, you know how this one goes. So, go ahead, you are allowed to add this line:\nreadline(prompt = \"This was only computed once in the life of the Universe, compute again [Y/N]? &gt; \")\nCheckpoints are also useful tools to handle errors that might come in the data, find typos, sort out dates, prevent the wrong data type from entering a function, among others.\nFor example, you might be expecting something that looks like yyyy-mm-dd_filename.csv, but for some reason you end up trying to read other_filename.csv. Well, good luck converting other to date format. Moreover, are you really trying to use \"[0-9]{4}-[0-9]{2}-[0-9]{2}\" as the pattern to match to yyyy-mm-dd? Come on…you can do better than this!\nWondering where are those NAs coming from? Wondering what’s the length of a NULL character? Been there, done that. Don’t worry, enough checkpoints can solve the problem.\nAt this point, after another couple hours, your code is working under several layers of if(condition) {...} else {...} statements. Good job, feeling like a programmer yet?"
  },
  {
    "objectID": "posts/2019-02-01-r-pipeline-its-a-trap/index.html#what-does-the-code-do-again",
    "href": "posts/2019-02-01-r-pipeline-its-a-trap/index.html#what-does-the-code-do-again",
    "title": "R pipeline – It’s a trap!",
    "section": "What does the code do again?",
    "text": "What does the code do again?\nBecause your code is now looking like:\nwrapper of wrapper of x + wrapper of wrapper of y + wrapper of wrapper of z -&gt; ???\nYou have no idea what’s going on unless you run the whole chain. But you did your work and it pays off. The code works like a charm! You can see lines of code dropping, messages being printed, prompts being asked and answered, graphs popping every now and then. You even check some errors by trying to run corrupted data, errors are being thrown! Heaven.\nYou are ready now, the code is working, time to collect a shit ton of data."
  },
  {
    "objectID": "posts/2019-02-01-r-pipeline-its-a-trap/index.html#why-are-you-doing-this-to-yourself",
    "href": "posts/2019-02-01-r-pipeline-its-a-trap/index.html#why-are-you-doing-this-to-yourself",
    "title": "R pipeline – It’s a trap!",
    "section": "Why are you doing this to yourself?",
    "text": "Why are you doing this to yourself?\nYou didn’t optimize for that much data. Your code is now running at a brand new speed: turtle. You can go get coffee, stop to chat a for a while with the students in another lab and come back to the computer, only to see it improve a few % points.\nYou did this to yourself. You don’t need those 17 extra plots in between calculations every single time. You have no time to look at them if you are running hundreds of tiny files as inputs. Besides, you already know that your method works, the calibration curve is calibrated and the machine parameters lie within the expected distribution."
  },
  {
    "objectID": "posts/2019-02-01-r-pipeline-its-a-trap/index.html#save-before-continue",
    "href": "posts/2019-02-01-r-pipeline-its-a-trap/index.html#save-before-continue",
    "title": "R pipeline – It’s a trap!",
    "section": "Save before continue",
    "text": "Save before continue\nGaming rules apply: before fighting the boss, save the game, just in case you know…you die in the fight. You don’t want to start all over, do you?\nBut we seldom do this unless we absolutely have to. We don’t like to leave a trail of intermediate computations. In a sense, this might be dangerous, what if something broke in between, but because we have an old copy we are able to go through the pipeline without noticing? It makes me cringe.\nBut you don’t want to calculate years of data just to look how much the new addition changes the output, right? Start saving intermediate steps!"
  },
  {
    "objectID": "posts/2019-02-01-r-pipeline-its-a-trap/index.html#package",
    "href": "posts/2019-02-01-r-pipeline-its-a-trap/index.html#package",
    "title": "R pipeline – It’s a trap!",
    "section": "Package",
    "text": "Package\nYou said you were going to have the data analysed by tomorrow, but you spent the whole day waiting for it to run, debugging and trying to make the pipeline even more tight. That thing you did, the saving intermediate files, turned out to be far more complicated than what you expected. You realized exactly how many environment variables were being created and used during the pipeline. You re-learn the term scoping and you learn to worship it. Nah, 10 arguments it’s not too much, you will surely remember them all when you call the function! No data so far, no data that you can show to another human being.\nIt’s well past midnight, your eyes hurt, there’s no more ice cream to keep you excited. But a question creeps into your mind, a revelation:\nShould I start from scratch and make this my own package? Yes..that’s what I’m gonna do…!\n\nStop! It’s a trap!"
  },
  {
    "objectID": "posts/2019-02-01-r-pipeline-its-a-trap/index.html#steps-of-the-race-to-the-bottom",
    "href": "posts/2019-02-01-r-pipeline-its-a-trap/index.html#steps-of-the-race-to-the-bottom",
    "title": "R pipeline – It’s a trap!",
    "section": "Steps of the race to the bottom",
    "text": "Steps of the race to the bottom\nJust as recap…be my guest.\n\nYou have a raw_data.csv.\nYou open an empty script (Untitled1) and write a few lines that do sequentially something like x + y + z -&gt; result.\nYou make a very raw representation (even using base graphics).\nYou know this is going to be periodically done, so you shift to functions, you parametrize some things.\nNow you have x = f(…), y = g(…), and z(…).\nYou are happy because you still can do something like f(…) + g(…) + z(….).\nYou realize your functions depend on global parameters like the date in which you do each of your experiments.\nYou decide you are going to take this a step forward, here comes list.files and lapply() to save the day.\nYou wrap your main functions inside other functions that are compatible with lists (because now EVERYTHING is a list).\nYou realize that you lost.\nYou add checkpoints, questions, exceptions (for all those moments where data behave as well…normal messy data).\nYour code has grown so much you have no clue what it does unless you run the full script, so now it looks like wrapper of wrapper x + wrapper of wrapper y + wrapper of wrapper z -&gt; entangled results.\nYou are so happy that you can run all those files like a charm (and it works!) so you acquire a shit ton of data.\nYou didn’t optimize for “a shit ton of data”, your code is slow now. Also…what were you thinking? You don’t need those 17 extra plots in between calculations. You have no time to look at them now that you are running hundreds of tiny files as inputs.\nCalculations take forever now. You realize that you had to save intermediate steps.\nEven if you tried saving them, most of your functions are dependent on other tiny values (dates, number of trials, number of rows of matrix named “WHATEVER”).\nMake your own R package."
  },
  {
    "objectID": "posts/2019-02-01-r-pipeline-its-a-trap/index.html#footnotes",
    "href": "posts/2019-02-01-r-pipeline-its-a-trap/index.html#footnotes",
    "title": "R pipeline – It’s a trap!",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTo be fair, companies are not entirely to blame here. The scientific community (aka, the consumers) demands a visually friendly display, with Excel Copy+Paste capabilities. No wonder why universal text formats are many many clicks away in most software and you need several data wrangling steps before having the thing you want: a tidy table. Data coming from organizations (e.g, UN) is not always super clean either (countries keep changing their names, how dare they!?).↩︎"
  },
  {
    "objectID": "posts/2019-09-13-skype-a-scientist-2/index.html",
    "href": "posts/2019-09-13-skype-a-scientist-2/index.html",
    "title": "Skype a Scientist part 2",
    "section": "",
    "text": "I have been lucky to participate in the project lead by Sarah McAnulty called Skype a scientist. I recently did two more calls with 11th grade students who attend Dowal school in Honduras. This was the first time I did Skype a Scientist in Spanish!\nAs usual, I should say it was great to be able to talk with students, tell them about science in general and my research. It was really fun to answer their questions.\nI compiled the Q&A session below."
  },
  {
    "objectID": "posts/2019-09-13-skype-a-scientist-2/index.html#personal-questions",
    "href": "posts/2019-09-13-skype-a-scientist-2/index.html#personal-questions",
    "title": "Skype a Scientist part 2",
    "section": "Personal questions",
    "text": "Personal questions\nThe “how did you choose to be a neuroscientist” question was quite popular. It came in all these forms:\n\nHow did your professional career begin?\nWhat was your motivation for studying this career?\nWhat made you decide you wanted to be a neuroscientist?\nWhat was your inspiration to study that exact part of the body? Why not other?\nHow did you decide Neuroscience was your passion? And what things made you decide this career?\nWhat inspired you to choose this career?\nWhat led you to study this career?\nWhen did you start to have an interest in this career and why?\n\nThe answer is:\nI just felt really curious about how the natural world works. I could see beauty in Math and later in Chemistry, and wanted to learn more. Living things are by far the most complex systems we know of, so I was totally down for the challenge of figuring out how they work.\nHow many years did you study?\nI can tell you how many years I’ve been in school. Elementary school was 7, middle/high-school was 5, then university added another 5-6 years, 2 years for my Master’s of Science degree. School is just the beginning.\nHow young where you when you decided to study your major?\nI was 17. In Argentina you chose your “major” in the last year of high school. I chose Biology. Then, 3 years into the career, I chose neuroscience specifically (I was probably 21-22), to specialize in for 2-3 more years.\nWhat would you have studied if it was not neuroscience? // What do you think would have been of yourself if you had studied something else?\nProbably Chemistry or Engineering. If I could go back in time, I would tell myself to study Physics or Computer Science.\nWas neuroscience always your passion, or did you want to study something else?\nI started with biology but rather quickly wanted to study the brain specifically.\nWhat ability do you think separates humans from other animal species?\nSo far that we know, humans are aware (see below) and can communicate that quite clearly. Humans can also build upon knowledge of previous generations.\nWhat is the hardest topic or part to understand about Neuroscience?\nNeuroscience is full of hard, really hard, problems. I would have to say the most difficult is consciousness. How are we aware of being aware? How is it that we distinguish ourselves as an I that exists and has a will? Where is that voice in your head coming from? I wrote a short story about it, I might publish it some day :).\nWhat is the part that isn’t too important of the brain?\nI would have to say that you need 100% of your brain to function the way you do. I am not quite eager of losing any part of my brain, all of them are too important.\nWhat is the difference between brain neurons and normal neurons?\nI am a bit confused about the term normal neurons. There are neurons in the brain, neurons in the spinal cord and in our digestive tract. The basic principles of all neurons are the same, but, even in the brain, they differ in shape and physiological properties. Here’s a drawing from Cajal showing different neurons in the human cortex.\n\nDoes your family feel good with what are you studying or they wanted you to study another thing?\nMy family was always supportive. As usual, a career in science looks difficult and it might seem like it’s quite tough to get a job, so there were some doubts, but nonetheless my family was supportive.\nWhy do we sometimes forget the words we were going to say at the time?\nThis is a great question. I don’t have a clue! Please, if you find out, let me know!\nWhy is your research important?\nIt’s quite difficult to pinpoint why any experiment is important. But we can think about knowledge as a circle, and questions as a frontier to the outside (the unknown). The more we study and answer questions, the more the circle grows. The larger the circle, the more things the scientific field can improve our lives. Interestingly, the more the circle grows, the larger the frontier, meaning we have more questions to answer1!\nHave you learned any lesson from your years of work that you can apply in your daily life? // Does this job affect your personal life in any way?\nYes, see below.\nWhat event in your life as a neuroscientist marked you in your personal life and why?\nBeing a scientist involves dealing with information about the world. A fact is a fact. Now, there are different ways of making sense of that fact, and the way scientists do it is by building models (creating a hypothesis) and making predictions. Scientists should always start by trying to prove themselves wrong. Only when they failed to prove themselves wrong, they tend to accept a version of the world (the one that better fits that fact).\nPresented with an observation people tend to go with intuitive explanations of why it happened. Normally, that intuitive is close to whatever that person has at hand, whatever personal experience they have (we call this reasoning by induction, which is quite useful and intuitive, but often fails). Scientific training should aim to teach to think in counterintuitive ways.\nFor example:\n\n\n\nWhite swan (Cygnus olor)\n\n\n\nObservation 1: I see a white swan.\nObservation 2: I have never seen a black swan.\n\nIt is tempting to conclude that black swans do not exist. And the whole world thought that way until they went to Australia and New Zeland and found a black one!\n\n\n\nBlack swan (Cygnus atratus)\n\n\nThe bottom line is, being a scientist teaches you to approach facts by questioning your intuition and making testable arguments. Do not belive whatever is writen, no mater where it is writen, judge the content, hold it to the highest standard.\nDo you think that later on, when neuroscience is more advanced, there could exist the cure to neurological diseases?\nWhen nuroscience advances more, we will find more cures or treatments. There will be no unique cure, because there is no unique neurological disease.\nWhy is it that when you read the words that describe an experience, you can sometimes, perfectly imagine the situation?\nThat is a great question. I was recently discussing this with other neuroscientists. The truth is that we do not know. But our mind is perfectly capable of creating images (when you sleep yo can see even though your eyes are closed!). The mind is quite powerful at doing this. Try the following task. If you want to be scientific about it, you can time yourself!\n\nTrial 1: As fast as you can, read out loud the word sequence.\nTrial 2: As fast as you can, name out loud the color.\n\n\n\n\n\n\n\n\n\n\nWhat task was more difficult? You can see how powerful the interference of reading is. It’s quite difficult to name the color when the word clearly says another thing! This is called Stroop Effect\nWhat was the most difficult subject while studying to become a neuroscientist?\nPhysics has some concepts that are difficult to grasp like Optics, Thermodynamics and Electromagnetism.\nHow are memories stored and retrieved?\nThat’s one of the best questions I ever had. Actually, that was one of the questions that lead me to study neuroscience in the first place. Although there are certain hypothesis, we know little about how it is done. You can start here. If you want to know more scientific detail, hit me up and I will send you a bunch of papers :)\nAre you happy or satisfied with what you’ve attained in your life so far?\nWow this is getting to the hard stuff! I would say I have come a long way but there’s definitely room for improvement and new challenges to tackle.\nWhat goals are you looking forward to achieve in your career as a neuroscientist?\nI would like to be able to keep doing research in science. Hopefully be able to contribute to a better understanding of disase and treatment.\nWhat is the most difficult part of being a neuroscientist? // What is the hardest part of your carreer?\nDealing with failure. We must be perseverant and smart about pivoting directions when we fail.\nDo you have seen something that does not have any explication about animal behavior? // Have you ever had a situation in which you don’t know the solution to in your career?\nEvery day, all day, all the time! No, seriously, we need help trying to figure animal behavior, please come help us!\nWhat’s your biggest inspiration to do science? // What is the thing you like most about it?\nIt’s beautiful. The feeling of understanding something deep about our Universe is quite rewarding.\nHow did studying in MIT help develop more your career?\nI got to work with nice people and do a bunch of cool science. That will get the research published (hopefully soon!) and it will allow me to continue my career as a scientist.\nWhat tips can you give for those people that what to study neuroscience?\nPerseverance, we all fail, push forward. Find a team and contribute to that team with the best you can. Other people have given better advice than me here. It’s long but I highly recommend it.\nWould you consider the outdoor activities you practice important for any scientific or personal development?\nYes, running, playing sports, dancing, and any outdoor/physical activities are crucial to physical well-being and mental health.\nHow can animals feel pain?\nWe animals have pain receptors that comunicate the sensation of pain. See more info here.\n\nWhat is the process you most use when researching animal behavior?\nI would say looking at the animals with my eyes. Everything else is a way to quantify and modify the intuition I get from looking at the animals.\nDo animals can have the same diseases as humans?\nSome diseases affect all animals, some diseases affect some animals, some are human-specific (for example HIV - AIDS).\nDoes this work involve more physiology or biochemistry?\nIt involves everything. Everything you learn can be useful (even if it’s not scientific per se).\nCan you explain the difference between MRI and CT scans?\nBoth techniques can be used to image tissue. The main difference is that one uses X-rays (CT scan) and the other uses magnetic fields and radiowaves to do it (MRI)\nWhat would happen if a kid is born without a developed central nervous system and how you will solve it?\nIt probably depends on the type of underdevelopment. Unfortunately, I would not be able to solve it (I do not work with clinical patients, I work with animals in basic research).\nWhat is the behavior of cockroaches after being irriated with co-60?\nI have no idea. After googling co-60, that is cobalt 60, I found out it is actually a radioactive form of cobalt. My question is: how much co-60 are we talking about? Anyway, that experiment has been done, in a paper called Some Early Effects of Ionizing Radiation on the German Cockroach,Blattella germanica. Authors concluded that radiation destroys reproductive organs and produces death. I don’t think behavior was affected but definitely lifespan was.\nHave you ever been scared of working with a specific animal? // Is it complicated being Neuroscientist and interacting with a certain species?\nYes, working with animals is a little bit challenging. Rodents are, in general, much easier to work with than other animals. Some animals are more scary to work with (owls, bats, monkeys, pigs). Wild animals are probably really hard to work with.\nWhat made you interact with animals instead of humans? // Why did you wanted to study animal behavior instead of human behavior? // Why did you choose to be a neuroscientist that focused on animals?\nI’m most interested in interventions. We can’t open human brains to see what’s going on.\nHow difficult and compromising is to choose neuroscience as a future career?\nIt definitely involves commitment. And commitment alone will not get you very far. You can be very committed and still not get good things out of it. Do not underestimate your network and pure luck!\nHow has working with animals improved your knowledge about human behavior?\nIt has given me the possibility to test ideas about human behavior on animal models.\nWhat has been the most difficult species of animal that you have worked with?\nAll species have their perks and difficulties. I wouldn’t be able to tell.\nHow do you understand the behavior of animals?\nWe normally try to condense behavior into variables that we can measure. That help us build theories we can test with more experiments.\nWhat is the dominant mammalian animal species used in a neuroscience research?\nBy dominant I pressume you mean frequent. It’s probably mice (mus musculus).\nDo you ever get intimidated by others in your same career?\nOf course, unfortunately, science is not devoid of hierarchical structures and people at the top can be scary sometimes.\nWhat type of breakfast do you have every morning?\nI have either a toast with cream cheese or an omlette. Some fruit maybe, banana milkshake if I’m feeling like it’s a special day. Tea always.\nHow hard did you work to have this job?\nNot like REALLY REALLY hard. Coal miners have it way worse than me, and they have to work in a harmful environment (for a lot less pay!). That being said, I feel that I have had to put effort into it, it was not automatic.\nIs it true that if humans eat other humans their brains get damaged and if it is true why it only affects humans?\nYes, if they it other human brains and those brains have something called prions, it can happen. It’s not specific to humans, see Bovine spongiform encephalopathy.\nHow you compare your work with your social life? // Do you have extra time for your family and other business? // How do you manage to balance your work and your social life?\nDuring the week, work comes first. During the weekend, social life comes first. About the “other business”, I’m not sure, do you have an offer I can’t refuse?\nAre there any animal models of self-mutilation?\nI have not come across any during my experience, but other people definitely have something to say about it, see here.\nWhat is something you always wanted to do that you have not done it yet and why?\nI have visited many places in the World. I still would like to travel more.\nEven though humans are more envolved, do animals have a more complex brain?\nHumans are not more evolved2. Human brains are definitely more complex.\nWhat concepts of the study of animals you dislike the most?\nThe first thing I should note is that, regardless of how careful and caring we are, we won’t be able to do neuroscience without intervention on animals. These animals live confined and we often perform surgeries on them. That’s not awesome, but it’s\nBeyond that, I would say I don’t like the fact that we can’t talk to them. They can’t express how they feel, what’s going on on their minds whenever we study them. We have to work around this, infer their mental state from other variables.\nHave you ever be surprised by an animal case with a mutation?\nWe work with mutants all the time. A mutation is not how you might see it pictured in movies. In fact, your own DNA is mutating all the time, for the rest of your life. That being said, there are certain mutations that produce very interesting effects (we call them phenotypes). Here’s the ob/ob mouse model:\n\n\n\nob/ob mouse on the left, normal mouse on the right\n\n\nThese animals can’t produce leptin, a hormone involved in energy balance and metabolism. They never experience a signal that tells them to stop eating (leptin) and hence they eat much more and gain a lot of weight."
  },
  {
    "objectID": "posts/2019-09-13-skype-a-scientist-2/index.html#questions-about-argentina",
    "href": "posts/2019-09-13-skype-a-scientist-2/index.html#questions-about-argentina",
    "title": "Skype a Scientist part 2",
    "section": "Questions about Argentina",
    "text": "Questions about Argentina\nWhere would you have liked to study if it wasn’t in Argentina?\nProbably in Europe. Assuming I have to do it in English, the UK, or any international school in Germany, Switzerland or Finland. Assuming I speak German, in Germany or Switzerland.\nDo you think Argentina is one of the most expensive countries to study in? // Do you consider Argentina a country where is rentable to study? // Do you think Argentina is one of the most expensive countries to go study?\nEducation in Argentina at public unviersities is free. The overall quality is good, even compared to top schools in the US or Europe."
  },
  {
    "objectID": "posts/2019-09-13-skype-a-scientist-2/index.html#on-favorites",
    "href": "posts/2019-09-13-skype-a-scientist-2/index.html#on-favorites",
    "title": "Skype a Scientist part 2",
    "section": "On favorites",
    "text": "On favorites\nI am really really bad at chosing favorites and don’t like rankings. But below is my attempt to answer these questions.\nWhat is your favorite aspect of your research?\nThinking about questions and making theories about how the whole thing works.\nWhat is your all time favorite book?\nI don’t have a favorite book. You can see the books I have read in the last years here. There’s a lot of books I have read but not registered (sorry about that ¯\\_(ツ)_/¯)\nWhat’s your favorite series?\nI’m not good with choosing favorites, so I will go with the one I’ve watched the most growing up: The Simpsons."
  },
  {
    "objectID": "posts/2019-09-13-skype-a-scientist-2/index.html#footnotes",
    "href": "posts/2019-09-13-skype-a-scientist-2/index.html#footnotes",
    "title": "Skype a Scientist part 2",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis idea is not mine, I took it from Ignorance by Stuart Firestein↩︎\nAll species are quite evolved to a certain niche. In fact, we humans have not been around for a lot of time, compared to other animals.↩︎"
  },
  {
    "objectID": "posts/2018-07-13-XY-density-maps/index.html",
    "href": "posts/2018-07-13-XY-density-maps/index.html",
    "title": "XY-density-maps",
    "section": "",
    "text": "I have been playing around with tracking software and the ways to visualize the position of an animal in an arena over time. Even with normal cameras (30 Hz) and relatively small periods of time (~5 min) we get massive datasets and the only way to make sense of them is to aggregate data over time. I have been interested in neat visualizations (using R), thus here are some ways/comments. I will explore different packages and doing comparisons of the results."
  },
  {
    "objectID": "posts/2018-07-13-XY-density-maps/index.html#packages",
    "href": "posts/2018-07-13-XY-density-maps/index.html#packages",
    "title": "XY-density-maps",
    "section": "Packages",
    "text": "Packages\n\nsuppressPackageStartupMessages(library(RColorBrewer))\nsuppressPackageStartupMessages(library(KernSmooth))\nsuppressPackageStartupMessages(library(raster))\nsuppressPackageStartupMessages(library(ggplot2))\nsuppressPackageStartupMessages(library(dplyr))"
  },
  {
    "objectID": "posts/2018-07-13-XY-density-maps/index.html#data",
    "href": "posts/2018-07-13-XY-density-maps/index.html#data",
    "title": "XY-density-maps",
    "section": "Data",
    "text": "Data\nI have an example dataset with XY position of the animal. I also have other morphological data that is irrelevant for this case. See example below:\n\n\n         X        Y Orientation MinorAxis MajorAxis\n1 325.5522 158.9700   -1.550285  50.47925  130.5679\n2 323.3055 156.6896   -1.569706  51.29840  130.3467\n3 321.7411 154.8107    1.545683  52.13492  130.3881\n4 320.8373 153.3294    1.512549  52.67011  130.6403\n5 320.0381 152.5523    1.477299  53.07783  130.1023\n6 319.3961 152.2019    1.439171  53.75891  129.6706\n\n\nI will do some setup that is common for all the graphs (aka, color palette and the dimensions of the arena).\n\n# Get color palette \n\nrefCol &lt;- colorRampPalette(rev(brewer.pal(6,'Spectral')))\nmycol &lt;- refCol(6)\n\n# define bin sizes\nbin_size &lt;- 40\n\n# Camera resolution is 640x480. Hence...\nxbins &lt;- 640/bin_size \nybins &lt;- 480/bin_size"
  },
  {
    "objectID": "posts/2018-07-13-XY-density-maps/index.html#ggplot2",
    "href": "posts/2018-07-13-XY-density-maps/index.html#ggplot2",
    "title": "XY-density-maps",
    "section": "ggplot2",
    "text": "ggplot2\nThe reason I always go to ggplot2 first is because it’s awesome, I buy into the grammar and find it intuitive to accumulate layers over layers. The underlying thought is that ggplot2 handles all problems. In this case, the result has some pros (layers, layers and more layer), and some cons (basically, it doesn’t look amazing).\n\np &lt;- ggplot(rat_pos, aes(X,Y)) + \n  stat_density2d(geom = 'tile', \n                 aes(fill = after_stat(density)), \n                 contour = FALSE,\n                 n = c(xbins, ybins)) +\n  coord_equal() +\n  theme_minimal() +\n  scale_fill_gradientn(colors =  mycol) +\n  geom_vline(xintercept = c(0,640))+\n  geom_hline(yintercept = c(0,480))\n\nprint(p)\n\n\n\n\n\n\n\n\nThis is interesting because we can overlay things into the plot. For example the trace:\n\np + geom_path(alpha=0.1)\n\n\n\n\n\n\n\n\nWe can further remove the axis (or any other modifications we feel like doing).\n\np + geom_path(alpha=0.5) + theme_void()\n\n\n\n\n\n\n\n\n\nHelping ggplot from outside\nI found that, if we calculate the density externally, it looks smoother. This is a mixed, bkde2D mediated, ggplot2 approach (aka the best of 2 worlds).\n\nbins &lt;- bkde2D(as.matrix(rat_pos[,1:2]), \n               bandwidth = c(xbins, ybins),\n               gridsize = c(640L, 480L))\n\n# reshape\nbins_to_plot &lt;- tidyr::pivot_longer(\n  data.frame(bins$fhat) %&gt;% tibble::rownames_to_column(), \n  cols = -rowname) %&gt;% \n  # fix the name column\n  mutate(name = stringr::str_remove(name, \"X\")) %&gt;% \n  # convert to numeric\n  mutate_all(as.numeric)\n\n\nggplot(bins_to_plot, aes(rowname, name, fill = value)) +\n  geom_raster()+\n  coord_equal() +\n  theme_minimal() +\n  scale_fill_gradientn(colors =  mycol) +\n  geom_vline(xintercept = c(0,640))+\n  geom_hline(yintercept = c(0,480))+\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n2023 Update\nAs of 2023, ggplot is able to handle 2D density heatmaps much better. I haven’t played too much with it, but here are some examples\n\nggplot(rat_pos, aes(X,Y)) + \n           stat_density_2d(geom = \"polygon\", \n                           contour = TRUE,\n                           aes(fill = after_stat(level)),\n                  bins = 8)+\n  coord_equal() +\n  scale_fill_gradientn(colors =  mycol) +\n  theme_minimal() +\n  geom_vline(xintercept = c(0,640))+\n  geom_hline(yintercept = c(0,480))+\n  theme_void()\n\n\n\n\n\n\n\n\n\nggplot(rat_pos, aes(X,Y)) + \n  geom_density_2d_filled(bins = 8)+\n  coord_equal() +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Spectral\", direction = -1)+\n  geom_vline(xintercept = c(0,640))+\n  geom_hline(yintercept = c(0,480))+\n  theme_void()"
  },
  {
    "objectID": "posts/2018-07-13-XY-density-maps/index.html#using-raster-package",
    "href": "posts/2018-07-13-XY-density-maps/index.html#using-raster-package",
    "title": "XY-density-maps",
    "section": "Using raster package",
    "text": "Using raster package\nThe raster package is maybe an older solution, which is surprisingly low demand. In 3 lines of code we get a perfectly functional plot. On the other hand, it’s not the best looking graph and we get the caveats (yes, base graphics).\n\nr &lt;- raster(xmn=0, ymn=0, xmx=640, ymx=480, res=20)\nx &lt;- rasterize(rat_pos[,1:2], r, fun='count')\nplot(x, xlim=c(0,640), ylim=c(0,480), xaxt='n', ann=FALSE, yaxt='n')"
  },
  {
    "objectID": "posts/2018-07-13-XY-density-maps/index.html#documentation",
    "href": "posts/2018-07-13-XY-density-maps/index.html#documentation",
    "title": "XY-density-maps",
    "section": "Documentation",
    "text": "Documentation\nI have found a good amount of advice and inspiration in the links below.\n\nhttp://stat405.had.co.nz/ggmap.pdf\nhttps://stackoverflow.com/questions/24078774/overlay-two-ggplot2-stat-density2d-plots-with-alpha-channels?lq=1\nhttps://www.r-bloggers.com/5-ways-to-do-2d-histograms-in-r/\nhttps://stackoverflow.com/questions/24845652/specifying-the-scale-for-the-density-in-ggplot2s-stat-density2d"
  },
  {
    "objectID": "posts/2018-04-24-matlab-list-files/index.html",
    "href": "posts/2018-04-24-matlab-list-files/index.html",
    "title": "Matlab list_files",
    "section": "",
    "text": "My preferred programing language is R. But, for many purposes, I find myself in need of a Graphical User Interface (GUI). Thus, I experienced a forceful transition to Matlab. Let’s be honest, Matlab can do powerful things, and it’s a great language to attempt to dominate (note to self: learn Python!1). Still, I find myself over and over thinking in R mode. Something along the lines of:\nCan be easily accomplished in R with list.files()\nlist.files(...)\nThis command can handle many many options, with pattern being among my favorite. More importantly, this command returns a useful character vector. No extra dots, no list of lists, no array. Just, useful. On the other hand, Matlab has dir and ls both of which are not satisfactory.\ndir\n\n.                                                      \n..                                    \nsomething.m                            \nsomething_else.m                               \nLookHere.m\nYes… Matlab’s version can also handle some form of regular expression matching. But, mind the dots and the structure! Matlab’s dir has a ton of things on it.\n&gt;&gt; q = dir\n\nq = \n\n  20×1 struct array with fields:\n\n    name\n    folder\n    date\n    bytes\n    isdir\n    datenum\nMatlab ls function is also full of deadly traps.\nqq = ls\n\nqq =\n\n  20×28 char array\n\n    '.                           '\n    '..                          '\n    'many_things_here.ext        '"
  },
  {
    "objectID": "posts/2018-04-24-matlab-list-files/index.html#the-solution",
    "href": "posts/2018-04-24-matlab-list-files/index.html#the-solution",
    "title": "Matlab list_files",
    "section": "The solution",
    "text": "The solution\nI found myself fighting for a character vector or array (nx1) that I could feed into a function/loop/whatever.\nThus, after many many many times fighting against classes, with functions that expect char instead of string, or cell, or whatever, I decided to create something that resembles (at least partially) the functionality I was looking for. It’s not perfect. Please enlighten me with a better approach. For now, I will be using list_files.m. Wanna use it? Be my guest, see below:\n\n\nShow Matlab Code\n% The idea of this function is to have something that works to list files\n% Matlab has too many weird things with dir/patterns/etc...\n% It could be slow if calling in a BIG dir and then subsetting\n% Otherwise it should work pretty fast\n\nfunction filenames = list_files(varargin)\n\n% Open input parser\np = inputParser();\n\n% Add possible values\naddOptional(p, 'Interactive', true, @islogical)\naddOptional(p, 'Dirname', '0', @ischar)\naddOptional(p, 'Pattern', {'.'}, @iscell)\naddOptional(p, 'FullPath', false, @islogical)\n\n\n% parse\nparse(p, varargin{:});\n\n% retrieve things from parser\nInteractive = p.Results.Interactive;\nDirname = p.Results.Dirname;\nPattern = p.Results.Pattern;\nFullPath = p.Results.FullPath;\n\n\n%% Dirname goes first\n% If we didn't provide a Dirname, both defaults will hold\n% If we provided a Dirname, we will read from there\n\nif (Interactive && string(Dirname) == '0')\n\n    dirname = uigetdir();\n\nelse\n    \n    dirname = Dirname;\n    \nend\n\n\nif ~isdir(dirname)\n    error('Dirname not valid, check dirname provided is character and exists.')\nend\n\n% actually call dir\n    d=dir(dirname);\n% Remove the dots matlab puts to things\n    d=d(~ismember({d.name},{'.','..'}));\n\n    % Get filenames\n    % Output as an mx1 cell\n    \n    filenames = {d.name}';\n\n        \n%% Subset by pattern\n    \n    default_pattern = string(Pattern) == '.';\n\n    if (~default_pattern) % non default case\n    \n    % join cell patterns separated by the 'or' regular expression\n    query_expression = strjoin(Pattern, '|');\n    \n    %  Subset the patterns\n    filenames = filenames(~cellfun(@isempty,regexp(filenames, query_expression)));\n    end\n    \n    % By default we return just the name\n    % If you want the full path, call it!\n    % it currently works only for 1 folder\n    % Recursive = TRUE will be super nice!\n    \n    if FullPath\n    filenames = fullfile(unique({d.folder}), filenames);\n    end\nend"
  },
  {
    "objectID": "posts/2018-04-24-matlab-list-files/index.html#footnotes",
    "href": "posts/2018-04-24-matlab-list-files/index.html#footnotes",
    "title": "Matlab list_files",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn 2023, having learned Python, I celebrate that my Matlab days are long over. That being said, I ended up writing an rlist_files package for Python due to fighting against os and Path libraries. You can find it here.↩︎"
  },
  {
    "objectID": "posts/2020-06-13-birthday-problem/index.html",
    "href": "posts/2020-06-13-birthday-problem/index.html",
    "title": "Birthday Problem",
    "section": "",
    "text": "We have all been there, classic math riddle:\n\nHow many people need to be in one room so that the probability of two of them having the same birthday is more than 0.5?\n\nIn a recent bootcamp exercise we tackled this in python and I wanted to share, just because it’s fun. I did it for a range of probabilities. My solution is probably not the fastest/shortest/most pythonic, but it’s a little thing I put out there so, if you want to use/improve it, please do!\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef prob(n):\n    numerator = np.math.factorial(365) / np.math.factorial(365-n)\n    denominator = 365 ** n\n    return(1 - numerator/denominator)\n\nprobs = list(map(prob, range(100)))\n\nplt.plot(range(len(probs)), probs)\nplt.show()\n\nA bad translation into R, it would be something like:\n\nprob &lt;- function(n){\n  numerator = exp(lfactorial(365) - lfactorial(365-n))\n    denominator = 365 ** n\n    return(1 - numerator/denominator)\n}\n\nprobs = sapply(0:99, function(n) prob(n))\n\nplot(0:99, probs, type=\"l\",\n     main=\"Probability of 2 people having same birthday\",\n     xlab = \"People in a room\", ylab=\"probability\")\n\n\n\n\n\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{andina2020,\n  author = {Andina, Matias},\n  title = {Birthday {Problem}},\n  date = {2020-06-13},\n  url = {https://matiasandina.netlify.app/posts/2020-06-13-birthday-problem/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndina, Matias. 2020. “Birthday Problem.” June 13, 2020. https://matiasandina.netlify.app/posts/2020-06-13-birthday-problem/."
  },
  {
    "objectID": "posts/2021-02-07-queens-gambit/index.html",
    "href": "posts/2021-02-07-queens-gambit/index.html",
    "title": "Queen’s Gambit",
    "section": "",
    "text": "I love games. I enjoy the fun in many formats: cards, boards, consoles. Yet, over the years, chess has consolidated as the game I play most often, and it is likely the only game I both really like and can limit the amount I play1.\nIt’s been a couple of months since I noticed something weird going on in my games, a sort of disturbance in the force. I felt I was playing a type of position called Queen’s Gambit way more often than normal.\nBecause there’s currently a sort of chess mania fueled by the very successful new series that gets the name from the gambit2, I thought it would be pretty easy to find out whether the data reflected my gut feeling."
  },
  {
    "objectID": "posts/2021-02-07-queens-gambit/index.html#too-long-didnt-read",
    "href": "posts/2021-02-07-queens-gambit/index.html#too-long-didnt-read",
    "title": "Queen’s Gambit",
    "section": "Too Long — Didn’t Read",
    "text": "Too Long — Didn’t Read\nI apologize for writing long posts. Be my guest and jump directly to the popularity of the Queen’s Gambit across time by clicking here"
  },
  {
    "objectID": "posts/2021-02-07-queens-gambit/index.html#the-data",
    "href": "posts/2021-02-07-queens-gambit/index.html#the-data",
    "title": "Queen’s Gambit",
    "section": "The data",
    "text": "The data\nTo do any sort of analysis, I needed data. Although there are probably many places where I could have found it, www.chess.com has a large dataset and it is somewhat accessible (if you can parse websites, see how). It is also the place where I play, so the dataset of my games comes from there.\nThroughout this post, I will be using two datasets:\n\nMaster Games: games played by Chess Masters where Queen’s Gambit is played.\nMy games: All my games at chess.com, spanning from 2012 to January 2021.\n\nLet’s minimally explore the dataset of Master Games for the sake of DataViz 📈.\nI first started looking at a dataset with 37287 entries, from games played between chess Masters (aka Master Games) that open with Queen’s Gambit. Games were played between 2000 and 2020. After a bit of data cleaning, there were a few quick things I wanted to look at.\nBelow, you can find a violin plot of the number of moves a chess match between masters normally has.\n\n\n\n\n\n\n\n\n\nIf you are curious, here’s a table with summary statistics for the number of moves in the games by result.\n\n\n\n\n\n\n\n\nResult\nmin\nq25\nmedian\nmean\nq75\nmax\n\n\n\n\nwhite wins\n5\n31\n40\n41.85\n50\n161\n\n\ndraw\n2\n21\n34\n36.84\n49\n204\n\n\nblack wins\n4\n33\n41\n43.82\n53\n183"
  },
  {
    "objectID": "posts/2021-02-07-queens-gambit/index.html#what-is-this-queens-gambit-anyway",
    "href": "posts/2021-02-07-queens-gambit/index.html#what-is-this-queens-gambit-anyway",
    "title": "Queen’s Gambit",
    "section": "What is this “Queen’s Gambit”, anyway?",
    "text": "What is this “Queen’s Gambit”, anyway?\nGenerally speaking, a gambit is a position where both players stand to exchange pieces. In the particular case of the Queen’s Gambit, we are most often referring to this position (although see further below)."
  },
  {
    "objectID": "posts/2021-02-07-queens-gambit/index.html#when-does-it-happen",
    "href": "posts/2021-02-07-queens-gambit/index.html#when-does-it-happen",
    "title": "Queen’s Gambit",
    "section": "When does it happen?",
    "text": "When does it happen?\nQueen’s gambit is an opening, this means it occurs early in the game. In this case, the gambit is overwhelmingly proposed on the second move and just a few games develop into a position that is equivalent to the Queen’s gambit later in the game."
  },
  {
    "objectID": "posts/2021-02-07-queens-gambit/index.html#who-benefits",
    "href": "posts/2021-02-07-queens-gambit/index.html#who-benefits",
    "title": "Queen’s Gambit",
    "section": "Who benefits?",
    "text": "Who benefits?\nIn chess, whoever starts (aka plays white) has advantage. However, Chess Masters are heavily trained in defense, so it’s not a walk in the part for white. It always depends heavily on the initial positions (openings), gambit proposals, and responses (accepted or declined with many variations). In the case of the Queen’s Gambit, the machines predict white is favorite to win, while accepting the gambit increases the likelihood of draws. Interestingly, if we calculate the expected value for black from the observed frequencies, it might be slightly better for black to accept the gambit3.\n\n\n\n\n\n\n\n\n\nI wouldn’t treat this tiny increment as something real. It is likely just noise. In reality, Chess Masters decline the Queen’s gambit way more often than they accept it (about 2.6 times more often!).\nIn fact, if we check with the chess engines, they seldom accept the Queen’s Gambit either. On a personal, totally amateur note, I don’t feel like accepting the Queen’s Gambit, it just doesn’t feel right."
  },
  {
    "objectID": "posts/2021-02-07-queens-gambit/index.html#where-do-they-go",
    "href": "posts/2021-02-07-queens-gambit/index.html#where-do-they-go",
    "title": "Queen’s Gambit",
    "section": "Where do they go?",
    "text": "Where do they go?\nAfter the opening, chess games branch into a myriad possible positions. I believe a cool way to see this is to use alluvial plots to visualize how the games evolve in time. It would really be a mess to visualize all moves, so I just focused on certain games and kept it short around proposal/response in regards to Queen’s Gambit. I added the result of the game, just out of curiosity.\n\n\n\n\n\n\n\n\n\nAs you can see, I focused on the two cases where white moves their pawn to c4 (c4) to actually “propose” the gambit. This is how both situations look on the board.\n\n\n\n\n\n\n\n1. d4 d5 2. c4\n1. d4 Nf3 2. c4\n\n\n\n\n\n\n\n\n\n“Accepting” the gambit means taking the pawn at c4 (dxc4). As we said before, this is the least frequent case, and it is interesting to see that almost none of the Nf6 games go to accept the gambit. It’s also quite telling that the vast majority of Masters respond to Queen’s Gambit with e6. This is how it looks in the board.\n\n\n\n\n\n\n\n1. d4 d5 2. c4 e6\n1. d4 Nf6 2. c4 e6\n\n\n\n\n\n\n\n\n\n\nSpecial case\nThere’s a special case where games take a bit longer to reach the gambit. These are the games where the second move is Nf3 Nf6. I made the same alluvial plot for these games. For some reason, this position prompts the majority of Chess Masters to accept the gambit.\nThus, if you like to play the gambit as white, and you like the other side to accept it, you should delay your c4 advance just one move.\n\n\n\n\n\n\n\n\n\nThis is how your board should look like if you want black to accept the gambit."
  },
  {
    "objectID": "posts/2021-02-07-queens-gambit/index.html#popularity",
    "href": "posts/2021-02-07-queens-gambit/index.html#popularity",
    "title": "Queen’s Gambit",
    "section": "Popularity",
    "text": "Popularity\nEnough with the chess lingo, show me what I came here for! Alright, alright, don’t bark. Here’s the popularity plot on the Masters Games.\n\n\n\n\n\n\n\n\n\nThis plot shows that the popularity of the gambit increased dramatically. But my prediction was wrong, the gambit was super popular a few years ago and I didn’t notice anything.\nWhat happened between 2017 and 2019? I don’t really know if the total amount of games just skyrocketed, but it’s possible. Maybe the 2018 world championship had something to do with it. I didn’t parse the full database, so I can’t tell you about the frequency of all games.\nWhat happened in 2020? If you are from the future, I would like to remind you that there was a Global Pandemic on 2020, stay safe whenever you are.\nThe popularity graph didn’t show what I was expecting to see, but I still trust my gut feeling. It all started just a few months ago, I’m pretty sure that there is something in the data that needs to be unmasked.\nData from 2020 will not reflect my gut feeling easily. First, the pandemic messed up with the tournament schedule. Second, the TV series might be super popular with normal people, but we shouldn’t expect Chess Masters to change the way they open games because of it.\nSo…what about simple mortals like you and me? Well, my friend, if you made it this far, I trust you to go on with my game database."
  },
  {
    "objectID": "posts/2021-02-07-queens-gambit/index.html#my-games",
    "href": "posts/2021-02-07-queens-gambit/index.html#my-games",
    "title": "Queen’s Gambit",
    "section": "My games",
    "text": "My games\nFirst, you should know that I mostly play blitz games (either 3 minutes in the clock at start or 3 minutes with 2 seconds added per move). Nonetheless, it looks like the move distribution of games is quite similar to what we see in Masters games.\n\n\n\n\n\n\n\n\n\nThere are two places where I see differences. First, Master Games show a bump quite early, even below 20 moves. This is unusual because it takes more moves to break a good defense. We will come back later to this.\n\n\n\n\n\n\n\n\n\nSecond, there is another bump on longer games. I attribute this to the nature of my skill level. I can’t play much more than an opening and a few attempts on the “middle game”. Most of my games will evolve pretty quickly (either because the clock forces you to be aggressive or because our chess level is not good). It’s often the case that one player makes a crucial mistake and the game evolves fast after that.\n\n\n\n\n\n\n\n\n\nThere’s another explanation for these differences. If we separate games according to result. We see that Masters games have almost the same shape as mine but, when they get to a draw, they get to it really fast. For us novices, getting a draw is often an accident. Masters deliberately draw, and they do it early.\n\n\n\n\n\n\n\n\n\n\nDrawing is a signal of quality\nAt my level, drawing a game is not common. For pros, it’s quite the opposite. Something else that draws my attention is that black is really unlikely to win a game. This is similar to other games, like tennis, where the player that serves (aka, starts the game) has a huge advantage.\n\n\n\n\n\n\n\n\n\n\n\nMy openings\nIn my database, I have all the games I played. Up to February 2021, it was a total of 15130. Yes, I know, a lot, roughly 4.61 per day since 2012. Anyway, I can get all the openings from my games.\nI selected the ten most frequent first moves when I am playing white and when I am playing black. You can see that d4 d5 doesn’t appear on the top ten at all when playing white. The only times I face the d4 d5 opening (which might lead to Queen’s Gambit) is when I play black.\n\n\n\n\n\n\n\n\n\n\n\nHow often do I play against the gambit?\nThe amount I play has not been even over the years, but the frequency of games where I play the gambit seems somewhat marginal and independent of the how much I play.\n\n\n\n\n\n\n\n\n\nMarginal proportions are hard to see on a frequency graph, so maybe this graph is a bit more evident. I have played the Queen’s Gambit in roughly 5.0% of the games. The game sample from 2021 might not be big enough to be representative (only January is present).\n\n\n\n\n\n\n\n\n\nThe small increases in late years are not big enough for me to notice, especially when spread evenly across the year. But what happens if we only look at 2020? The TV series started in October. In November, we see a big jump in the Queen’s Gambit’s frequency!\n\n\n\n\n\n\n\n\n\nThis is increase is not big. In fact, during November I also played the Scandinavian Defense4 more often. But even so, the Queen’s Gambit got a ~3X jump on November, something that is way more noticeable when going from ~2.5% to ~ 7.5% than a 5-10% increase in an opening that I already play pretty often."
  },
  {
    "objectID": "posts/2021-02-07-queens-gambit/index.html#wrap",
    "href": "posts/2021-02-07-queens-gambit/index.html#wrap",
    "title": "Queen’s Gambit",
    "section": "Wrap",
    "text": "Wrap\nThis project started with just an intuition and went down the rabbit hole of exploration. It took quite a long time to write (and re-write), but I was a lot of fun to mess around with. I hope you have enjoyed it if you made it this far!"
  },
  {
    "objectID": "posts/2021-02-07-queens-gambit/index.html#more-readings",
    "href": "posts/2021-02-07-queens-gambit/index.html#more-readings",
    "title": "Queen’s Gambit",
    "section": "More readings",
    "text": "More readings\nIf you want to read a nice, data rich, post about the “Chess Boom” that the series brought about, check https://www.bloomberg.com/graphics/2020-chess-boom/.\nEven if you ask, Chess.com does not give you all the games in your database nicely (probably trying to limit the bandwidth). So I slightly adapted the following script https://github.com/arnsholt/chesscom-games for the purpose of “Gimme all my games with almost no clicks involved!”. It uses the API, so I think it’s all cool :).\nI should have probably searched for somebody’s package. Here are some references that might be useful:\n\nhttps://github.com/JaseZiv/chessR\nhttps://github.com/curso-r/chess"
  },
  {
    "objectID": "posts/2021-02-07-queens-gambit/index.html#footnotes",
    "href": "posts/2021-02-07-queens-gambit/index.html#footnotes",
    "title": "Queen’s Gambit",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHowever, after reading this post you might believe I’m a bit of a chess junkie 😝↩︎\nBut mostly because the star is a kick-ass woman.↩︎\nI’m using the observed frequencies and calculating one point per win and 0.5 points per draw. This gives the expected values for each player.↩︎\nScandinavian Defense is a position characterized by 1.e4 d5, it’s by far my prefered opening when playing black.↩︎"
  },
  {
    "objectID": "posts/2018-05-18-making-atlas-images/index.html",
    "href": "posts/2018-05-18-making-atlas-images/index.html",
    "title": "Making atlas images",
    "section": "",
    "text": "This post contains a quick description/recipe for making vector files of brain atlas images suitable for publication and presentations. This post is as much for communication as for personal records."
  },
  {
    "objectID": "posts/2018-05-18-making-atlas-images/index.html#step-1---the-atlas-itself",
    "href": "posts/2018-05-18-making-atlas-images/index.html#step-1---the-atlas-itself",
    "title": "Making atlas images",
    "section": "Step 1 - The atlas itself",
    "text": "Step 1 - The atlas itself\nStarting material should be as good quality as possible. I used .eps files from Paxinos Rat Brain Atlas for doing this. On the cheap side of life, a very nice online resource can be found here. Images could be imported to most image processing freeware and processed in a very similar way."
  },
  {
    "objectID": "posts/2018-05-18-making-atlas-images/index.html#step-2---load-images-into-inkscape",
    "href": "posts/2018-05-18-making-atlas-images/index.html#step-2---load-images-into-inkscape",
    "title": "Making atlas images",
    "section": "Step 2 - Load images into Inkscape",
    "text": "Step 2 - Load images into Inkscape\nThe process looks like this. Import files into Inkscape using the import dialog box.\n\n\n\nInkscape Import box"
  },
  {
    "objectID": "posts/2018-05-18-making-atlas-images/index.html#step-3---convert-to-grayscale",
    "href": "posts/2018-05-18-making-atlas-images/index.html#step-3---convert-to-grayscale",
    "title": "Making atlas images",
    "section": "Step 3 - Convert to Grayscale",
    "text": "Step 3 - Convert to Grayscale\nThe blue borders are not really my thing. I would rather have them in grayscale (or white, to overlay ). We’ll do grayscale for now. In Inkscape, do:\nExtensions &gt; Color &gt; Grayscale"
  },
  {
    "objectID": "posts/2018-05-18-making-atlas-images/index.html#step-4---get-rid-of-the-junk",
    "href": "posts/2018-05-18-making-atlas-images/index.html#step-4---get-rid-of-the-junk",
    "title": "Making atlas images",
    "section": "Step 4 - Get rid of the junk",
    "text": "Step 4 - Get rid of the junk\nProbably, all the letters are . We can do Ctr+F and find an a (or other letters). That helps us to get rid of the text fast and with little clicking."
  },
  {
    "objectID": "posts/2018-05-18-making-atlas-images/index.html#final-thoughts",
    "href": "posts/2018-05-18-making-atlas-images/index.html#final-thoughts",
    "title": "Making atlas images",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nFrom here on, just fixing it the way you want it. Here’s the final result, after cleaning all the stuff I didn’t like."
  },
  {
    "objectID": "posts/2018-05-18-making-atlas-images/index.html#afterward",
    "href": "posts/2018-05-18-making-atlas-images/index.html#afterward",
    "title": "Making atlas images",
    "section": "Afterward",
    "text": "Afterward\nAt some point, I decided to create a package to do this in R. If you are familiar with R and ggplot2, but you can get vector images programatically from the Allen Brain Atlas. Check nobrainr here."
  },
  {
    "objectID": "posts/2019-09-10-los-mismos-de-siempre/index.html",
    "href": "posts/2019-09-10-los-mismos-de-siempre/index.html",
    "title": "Los mismos de siempre",
    "section": "",
    "text": "Hace rato que vengo viendo ciertas tendencias de gente que hace algo de análisis de datos con la actualidad Argentina. Por ejemplo, Daniel Schteingart comúnmente publica este tipo de análisis en diversos medios. Esto me llevó a buscar datos abiertos disponibles para jugar un rato.\nEn general, a mí me interesa más debatir sobre la implementación de políticas y pienso que el Poder Legislativo merece mayor atención. Siento que sobrestimamos (para bien y para mal) el poder de un Ejecutivo que es, al final del día, mucho menos determinante que aquellos que escriben las leyes según las cuales se rige el Estado. Exigimos representantes que nos representen."
  },
  {
    "objectID": "posts/2019-09-10-los-mismos-de-siempre/index.html#senadores-por-período",
    "href": "posts/2019-09-10-los-mismos-de-siempre/index.html#senadores-por-período",
    "title": "Los mismos de siempre",
    "section": "Senadores por período",
    "text": "Senadores por período\nEmpecemos mirando cómo evolucionaron las bancas a lo largo de nuestra historia. Vemos un comienzo con algo de recambio, seguido de un período con bajo recambio. A partir de 1930, empezamos a ver los vaivenes de un país pendular que recae en dictaduras con disolución del Poder Legislativo. Las últimas dos dictaduras están marcadas en rojo, seguidas de un alto recambio en el número de bancas (asignación de nuevos Senadores cuando se restituye el Poder Legislativo). El otro pico que llama la atención es el 2001, máximo evento de recambio en nuestra historia. Hoy por hoy, podría decirse que gozamos de unas dos décadas de estabilidad en las instituciones, en donde vemos un recambio de entre 20 y 30 bancas regularmente."
  },
  {
    "objectID": "posts/2019-09-10-los-mismos-de-siempre/index.html#cuánto-dura-un-período",
    "href": "posts/2019-09-10-los-mismos-de-siempre/index.html#cuánto-dura-un-período",
    "title": "Los mismos de siempre",
    "section": "¿Cuánto dura un período?",
    "text": "¿Cuánto dura un período?\nLa longitud de los mandatos ha cambiado a lo largo de la historia. Además, existieron períodos en los cuales intentar ser senador no parecía una carrera a largo término (cortesía de la inestabilidad de las instituciones argentinas durante gran parte del siglo XX).\nUn gran número de senadores no cumplen su mandato completo, ya sea por muerte, renuncia (a veces para ocupar otro cargo) o por destitución. Afortunadamente, en las últimas décadas, la tendencia parece estabilizarse hacia la longitud de mandato designada en la ley."
  },
  {
    "objectID": "posts/2019-09-10-los-mismos-de-siempre/index.html#quién-quiere-repetir",
    "href": "posts/2019-09-10-los-mismos-de-siempre/index.html#quién-quiere-repetir",
    "title": "Los mismos de siempre",
    "section": "¿Quién quiere repetir?",
    "text": "¿Quién quiere repetir?\nLa permanencia de un senador – o de cualquier otro político – en el cargo es un tema de debate interesante. En primer lugar, uno esperaría que cualquier trabajador con experienca y conocimiento sobre su área sea más eficiente y productivo. En criollo, si me tengo que operar de apendicitis mañana, prefiero un médico que haya realizado 1350 cirugías de apéndice que uno que haya realizado 122. Luego, si trasladamos este razonamiento a nuestras instituciones deberíamos querer que el Senado esté compuesto de políticos con muchos años de experiencia en la cámara alta.\nEl problema radica en identificar cuáles son los puestos que se parecen al del médico haciendo la misma cirugía y aquellos que no. Tomemos el ejemplo de los burócratas del Estado de carrera, los estadístas, los responsables de mantenimiento de infraestructura. Esta es la gente que hace funcionar el aparato, sin ellos, no hay implementación, no hay político que pueda hacer nada. Una cosa podría que identificarlos es que todos ellos deberían ser apolíticos y, como tales, deberían poder cumplir su labor siendo independientes del gobierno de turno3. Los investigadores del CONICET y los maestros también son buenos ejemplos de este tipo de empleados de Estado4.\nEn cambio, el rol de los integrantes del Poder Legislativo está contaminado por el hecho de que, entre otras cosas:\n\nLegislan su propio salario.\nTienen la postestad de regular actividades económicas de manera conveniente.\nTienen re-elección indefinida.\n\nPor supuesto, podemos sumar el cúmulo de beneficios legales (como canjear viáticos por dinero en efectivo y discresionalidad de presupuesto) y aquellos no legales (en criollo, corrupción). Pero éste no es el lugar para analizar eso, focalicemos en los períodos. Los datos mandan, así que veamos qué es lo que dicen. Históricamente, la gran mayoría de senadores no repite su cargo.\n\n\n\n\n\n\n\n\n\n\nRepetidores por su nombre\nEste fenómeno puede explicarse por múltiples causas. Antes de cualquier análsis, veamos quiénes son los repetidores.\n\n\n\n\n\n\n\n\n\nAcá vemos por primera vez algunos nombres que suenan más a calle o a localidad que a lo que hicieron. Pero también vemos algunos nombres bastante familiares y actuales. Recordemos que 4 o 5 mandatos son, según la duración de mandato actual, entre 24 y 30 años en el mismo puesto!\n\n\nMáximos reemplazantes\nUna curiosidad ingenua me llevó a preguntarme cuáles son los máximos reemplaznates. Me pregunté si, quizás, existía una caja con senadores de repuesto que pudieran cumplir los roles una vez que la figurita política de turno lograba conseguir los votos.\nEn otras palabras: ¿Es posible que exista un grupo de senadores comodín? Los datos presentan una triste realidad, una en la que la historia del Poder Legislativo Argentino está contaminada por sucesivas disoluciones. Llamémosles por su nombre, Golpes de Estado. Aún así, hay algunos senadores que han tomado la posta como suplente en reiteradas ocasiones.\n\n\n\n\n\n\n\n\nReemplazante\nNúmero de veces\n\n\n\n\nDISOLUCION DEL P. LEGISLATIVO,\n269\n\n\nSin Datos\n55\n\n\nES SENADOR SUPLENTE,\n18\n\n\nVACANTE HASTA DISOLUC.P.L.,\n15\n\n\nVACANTE HASTA FIN DEL PERIODO,\n9\n\n\nJUAREZ, CARLOS ARTURO\n5\n\n\nMAYANS, JOSÉ MIGUEL ÁNGEL\n5\n\n\nMORALES, GERARDO RUBÉN\n5\n\n\nROMERO, JUAN CARLOS\n5\n\n\nVERNA, CARLOS ALBERTO\n5"
  },
  {
    "objectID": "posts/2019-09-10-los-mismos-de-siempre/index.html#dias-en-el-poder-legislativo",
    "href": "posts/2019-09-10-los-mismos-de-siempre/index.html#dias-en-el-poder-legislativo",
    "title": "Los mismos de siempre",
    "section": "Dias en el poder legislativo",
    "text": "Dias en el poder legislativo\nQuiero hacer énfasis en la cantidad de días en el poder que implica repetir mandatos. Por eso, ordené a los senadores según el número de días que estuvieron (o aún están). Esta herramienta es simplemente para facilitar la visualización de los casos más extremos.\n\n\n\n\n\n\n\n\n\nSólo dos ejemplos, el resto de tarea:\nVidal fue gobernador de la provincia de Corrientes (dos veces), diputado, senador por 32 años (literalmente hasta su muerte, a los 80 años). Mendoza fue legislador provincial, diputado, gobernador de San Luis y luego senador por 27 años.\nIndependientemente de lo grandiosas o nefastas de sus contribuciones y el servicio a su País, queda claro que la carrera política les permitió perpetuarse de formas que no sólo son imposibles en la mayoría de otros gremios, sino que son mucho más peligrosas para la Nación.\nUna cosa que resulta interesante es que pareciera que existen dos tipos de senadoes. Existe un quiebre entre aquellos que se quedan 1 o 2 mandatos y los demás, que se perpetúan en el cargo al mejor estilo Julio Grondona (#TodoPasa).\n\n\n\n\n\n\n\n\n\nEs interesante que en ningún momento busqué los 10 años como punto de quiebre, este valor surgió de hacer una simple regresión partida. Podemos ver que los casos más extremos se desvían bastante de la regresión (sugiriendo un incremento aún más fuerte que uno lineal), pero me pareció una forma de simplificar el asunto: más de dos mandatos y estás para quedarte, no por un rato, para quedarte a vivir."
  },
  {
    "objectID": "posts/2019-09-10-los-mismos-de-siempre/index.html#esto-es-nuevo",
    "href": "posts/2019-09-10-los-mismos-de-siempre/index.html#esto-es-nuevo",
    "title": "Los mismos de siempre",
    "section": "Esto es nuevo",
    "text": "Esto es nuevo\nA pesar que los ejemplos con el récord de días tienen más de un siglo, la realidad es que, desde el retorno a la democracia, el cierto grado de estabilidad institucional ha permitido que estas prácticas se exacerben.\nEn el siguiente gráfico muestra los períodos de los 60 legisladores con mayor cantidad de días en el poder. Vale recalcar que dos mandatos de 9 años te colocan muy arriba en el el ránking, mientras que necesitarías 3 mandatos completos con la nueva longitud de 6 años para llegar a la misma cantidad. Es fácil ver que, entre 1850 y 1930, existieron períodos con ciertos senadores ocupando las bancas de manera prolongada. Sin embargo, estamos hablando de un período político en donde ni siquiera se había legislado la Ley Saenz Peña o recién se había implementado. Ver tanto abuso no es sorprendente si el proceso es fundamentalmente anti-democrático y los períodos son tan largos.\nDesde el último retorno a la democracia, un número importante de senadores se han perpetuado de manera impresionante, incluso integrando más de un partido político (ver también el color de la camiseta)."
  },
  {
    "objectID": "posts/2019-09-10-los-mismos-de-siempre/index.html#color",
    "href": "posts/2019-09-10-los-mismos-de-siempre/index.html#color",
    "title": "Los mismos de siempre",
    "section": "El color de la camiseta",
    "text": "El color de la camiseta\nTradicionalmente, la UCR y el partido Justicialista son los grupos más convocantes. El siguiente gráfico muestra la evolución de las bancas para cada partido.\n\n\n\n\n\n\n\n\n\nAdemás de demostrar lealtad a su partido, los senadores son representantes según su provincia. Uno esperaría que tengan fuerte arraigo y sean portavoces de las necesidades particulares de aquellos a los que representan. Esto, en gran medida, se ve reflejado en que la mayoría de senadores únicamente se presenta por una provincia. Sin embargo, hay senadores fueron electos en más de una provincia:\n\n\n\n\n\n\n\n\nSenador\nNúmero de Provincias Representadas\n\n\n\n\nECHAGÜE, PASCUAL\n2\n\n\nELIAS, ANGEL\n2\n\n\nFERNÁNDEZ DE KIRCHNER, CRISTINA E.\n2\n\n\nFERRE, PEDRO\n2\n\n\nGARCIA, ALFREDO\n2\n\n\nIGARZABAL, RAFAEL\n2\n\n\nIRIGOYEN, BERNARDO DE\n2\n\n\nLEGUIZAMÓN, MARÍA LAURA\n2\n\n\nLEIVA, MANUEL\n2\n\n\nROCA, JULIO ARGENTINO\n2\n\n\nSAGUIER, FERNANDO\n2\n\n\nVILLANUEVA, BENITO\n2\n\n\n\n\n\n\n\nUn hecho más interesante es el cambio de partido. No me queda claro si esto habla de la inestabilidad en el tiempo de los partidos o de que algunos senadores se orientan con la corriente (por ponerlo de manera políticamente correcta).\nLos siguientes senadores ejercieron cargo representando a más de un partido:\n\n\n\n\n\n\n\n\nSenador\nNúmero de Partidos Representadas\n\n\n\n\nALPEROVICH, JOSÉ JORGE\n2\n\n\nBASUALDO, ROBERTO GUSTAVO\n3\n\n\nBLAS, INÉS IMELDA\n2\n\n\nBRAVO, LEOPOLDO\n2\n\n\nBRITOS, ORALDO NORVEL\n2\n\n\nCARO, JOSE ARMANDO\n2\n\n\nCASTILLO, OSCAR ANÍBAL\n3\n\n\nCASTRO, MARÍA ELISA\n2\n\n\nCLOSS, MAURICE FABIÁN\n2\n\n\nCOLAZO, MARIO JORGE\n2\n\n\nELÍAS DE PEREZ, SILVIA BEATRIZ\n2\n\n\nESCUDERO, SONIA MARGARITA\n2\n\n\nFELLNER, LILIANA BEATRIZ\n2\n\n\nFERIS, GABRIEL\n2\n\n\nFERNÁNDEZ DE KIRCHNER, CRISTINA E.\n3\n\n\nFERNÁNDEZ, NICOLÁS ALEJANDRO\n2\n\n\nFUENTES, MARCELO JORGE\n2\n\n\nGARCIA, ALFREDO\n2\n\n\nGIACOPPO, SILVIA DEL ROSARIO\n2\n\n\nGIUSTINIANI, RUBÉN HÉCTOR\n2\n\n\nGONZÁLEZ, MARÍA TERESA MARGARITA\n2\n\n\nGUASTAVINO, PEDRO GUILLERMO ÁNGEL\n2\n\n\nGUINLE, MARCELO ALEJANDRO HORACIO\n2\n\n\nJENEFES, GUILLERMO RAÚL\n2\n\n\nLATORRE, ROXANA ITATÍ\n2\n\n\nLEGUIZAMÓN, MARÍA LAURA\n2\n\n\nMARINO, JUAN CARLOS\n3\n\n\nMARTIARENA, JOSE H.\n3\n\n\nMAYANS, JOSÉ MIGUEL ÁNGEL\n3\n\n\nMAZA, ADA MERCEDES\n2\n\n\nMENEM, CARLOS SAÚL\n2\n\n\nMORALES, GERARDO RUBÉN\n2\n\n\nMURGUIA, EDGARDO P.\n2\n\n\nNEGRE DE ALONSO, LILIANA TERESITA\n3\n\n\nPETCOFF NAIDENOFF, LUIS CARLOS\n2\n\n\nPICHETTO, MIGUEL ÁNGEL\n3\n\n\nREUTEMANN, CARLOS ALBERTO\n3\n\n\nRODRÍGUEZ SAÁ, ADOLFO\n3\n\n\nROMERO, JUAN CARLOS\n3\n\n\nSAADI, VICENTE LEONIDAS\n3\n\n\nSANZ, ERNESTO RICARDO\n2\n\n\nSNOPEK, CARLOS\n2\n\n\nTEISAIRE, ALBERTO\n2\n\n\nVIUDES, ISABEL JOSEFA\n2"
  },
  {
    "objectID": "posts/2019-09-10-los-mismos-de-siempre/index.html#y-luego-2001",
    "href": "posts/2019-09-10-los-mismos-de-siempre/index.html#y-luego-2001",
    "title": "Los mismos de siempre",
    "section": "Y luego, 2001",
    "text": "Y luego, 2001\n\nExplosión de partidos\n\nSi esto no les gusta, hagan un partido político, ganen las elecciones y después hagan lo que quieran – CFK, 2012\n\nDesde el comienzo de los tiempos, el número total de partidos registrados que lograron obtener una banca en el Senado es de 109 partidos.\n\n\n\n\n\n\n\n\nEtapa\nPartidos (#)\n\n\n\n\npre-2001\n47\n\n\npost-2001\n70\n\n\n\n\n\n\n\n\nAjuste por alianzas\nAún cuando ajustamos por las múltiples alianzas (los justicialistas, la UCR y las distintas formas de Kirchnerismo y PRO). Las diferencias entre antes y después del 2001 son impresionantes.\n\n\n\n\n\n\n\n\nEtapa\nPartidos (#)\n\n\n\n\npost-2001\n48\n\n\npre-2001\n41\n\n\n\n\n\n\n\nEsta etapa también viene de la mano de la desaparición de los partidos tradicionales y la aparición de otros5."
  },
  {
    "objectID": "posts/2019-09-10-los-mismos-de-siempre/index.html#partidos-por-su-nombre",
    "href": "posts/2019-09-10-los-mismos-de-siempre/index.html#partidos-por-su-nombre",
    "title": "Los mismos de siempre",
    "section": "Partidos por su nombre",
    "text": "Partidos por su nombre\nPor pura diversión, imaginemos que queremos crear un partido político en argentina. Pero no queremos un partido cualquiera, queremos un partido que logre poner bancas en el senado. Si nos olvidamos de la política y las promesas de campaña, ¿cuáles son las palabras que deberían aparecer en el nombre para lograr nuestro cometido?"
  },
  {
    "objectID": "posts/2019-09-10-los-mismos-de-siempre/index.html#la-red-de-partidos",
    "href": "posts/2019-09-10-los-mismos-de-siempre/index.html#la-red-de-partidos",
    "title": "Los mismos de siempre",
    "section": "La red de partidos",
    "text": "La red de partidos\nEl siguiente gráfico muestra cómo están conectados los senadores con los distintos partidos. Es interesante ver la corriente ideológica del justicialismo, desde el Peronismo hasta el Frente para la Victoria. También es bastante claro el bunker de la UCR."
  },
  {
    "objectID": "posts/2019-09-10-los-mismos-de-siempre/index.html#footnotes",
    "href": "posts/2019-09-10-los-mismos-de-siempre/index.html#footnotes",
    "title": "Los mismos de siempre",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDesde Wikipedia a su pantalla: “Los requisitos para ser elegidos senador son: tener la edad de treinta años, haber sido seis años ciudadano de la Nación, y ser natural de la provincia que lo elija, o con dos años de residencia inmediata en ella.”↩︎\nNi hablar de una coronaria a corazón abierto, no?↩︎\nCuando digo apolíticos no me refiero a sus conviciones personales e inclinaciones políticas. Al menos en mi mente, el funcionamiento interno de ciertas dependencias del Estado debe ser autónomo. Un dato es un dato (sea la masa de un átomo o el desempleo), un docente o un estadísta del INDEC no deberían estar afectados en su labor por el partido de turno.↩︎\nIncluso en estas circunstancias, no considero que los investigadores o los maestros deban estar haciendo siempre la misma tarea. Al contrario, sendos gremios deberían funcionar con una estructura que permita aprovechar la experiencia para el mejor funcionamiento en vez de promover una jerarquía absurda por la jerarquía misma. Una científica jóven quizás esté mejor capacitada para hacer experimentos con sus propias manos. Un maestro jóven para interactuar con alumnos sin que haya un salto generacional muy pronunciado. Aquellos que tengan mayor experiencia, quizás pueda impulsar cambios estructurales para mejorar aspectos que no estén funcionando.↩︎\nPara simplificar, elegí partidos que lograron obtener más de 3 bancas.↩︎"
  },
  {
    "objectID": "previous_lifes/index.html",
    "href": "previous_lifes/index.html",
    "title": "Previous lifes",
    "section": "",
    "text": "Photochemistry\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRadiopharmaceuticals\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "previous_lifes/Bacon/index.html",
    "href": "previous_lifes/Bacon/index.html",
    "title": "Radiopharmaceuticals",
    "section": "",
    "text": "I worked for several years at  Laborations Bacon  under the supervision of Lic. Guillermo Casale, producing and developing different radiopharmaceuticals.\nAs a part of the R&D team, I developed the synthesis of two important drugs, namely:\nExample of brain PET-scans of healthy and sick patients using 18F-DOPA:\nEarly mornings (~4 am), I used to have fun carrying out nuclear reactions, more precisely 18O(p,n)18F, to synthesize  18F-FDG .\nWould like to know more? Check out this  video  on how the 18F nucleus is created inside the IBA cyclotrons."
  },
  {
    "objectID": "previous_lifes/Bacon/index.html#related-publications",
    "href": "previous_lifes/Bacon/index.html#related-publications",
    "title": "Radiopharmaceuticals",
    "section": "Related Publications",
    "text": "Related Publications\n\n\n    \n      \n      \n    \n\n\n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            Novel, simple and fast automated synthesis of 18F-choline in a single Synthera module\n            \n            \n                \n                \n                    Yair  Litman, Pablo Pace, Leandro Silva, Carlos Hormigo, Ricardo Caro, Hector Gutierrez, Maria Bastianello, Guillermo Casale \n                \n                \n                \n                    \n                        AIP Conf. Proc., 1509, 223-227 (2012)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n\n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n  \n              \n\n        \n    \n    \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "research/AqueousInterfaces/index.html",
    "href": "research/AqueousInterfaces/index.html",
    "title": "Aqueous Interfaces",
    "section": "",
    "text": "Where water meets the world: exploring the dynamic and complex realm of aqueous interfaces"
  },
  {
    "objectID": "research/AqueousInterfaces/index.html#related-publications",
    "href": "research/AqueousInterfaces/index.html#related-publications",
    "title": "Aqueous Interfaces",
    "section": "Related Publications",
    "text": "Related Publications\n\n\n    \n      \n      \n    \n\n\n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            Learning Electronic Polarizations in Aqueous Systems\n            \n            \n                \n                \n                    Arnab Jana, Sam Shepherd, Yair Litman, David M. Wilkins \n                \n                \n                \n                    \n                        J. Chem. Inf. Model., 64, 4426-4435 (2024)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n\n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n  \n              \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            Surface stratification determines the interfacial water structure of simple electrolyte solutions\n            \n            \n                \n                \n                    Yair Litman, Kuo-Yang Chiang, Takakazu Seki, Yuki Nagata, Mischa Bonn \n                \n                \n                \n                    \n                        Nature Chemistry, 16, 644-650 (2024)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n\n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n  \n              \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            Fully First-Principles Surface Spectroscopy with Machine Learning\n            \n            \n                \n                \n                    Yair Litman, Jinggang Lan, Yuki Nagata, David M. Wilkins \n                \n                \n                \n                    \n                        J. Phys. Chem. Lett, 14, 8175-8182 (2023)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n\n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n  \n              \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            Decisive role of nuclear quantum effects on surface mediated water dissociation at finite temperature\n            \n            \n                \n                \n                    Yair Litman, Davide Donadio, Michele Ceriotti, Mariana Rossi \n                \n                \n                \n                    \n                        J. Chem. Phys., 148, 102320 (2018)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n\n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n  \n              \n\n        \n    \n    \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "research/OrganicInorganicInterfaces/index.html",
    "href": "research/OrganicInorganicInterfaces/index.html",
    "title": "Organic/Inorganic Interfaces",
    "section": "",
    "text": "Exploring the fascinating intersection where organic and inorganic worlds collide."
  },
  {
    "objectID": "research/OrganicInorganicInterfaces/index.html#related-publications",
    "href": "research/OrganicInorganicInterfaces/index.html#related-publications",
    "title": "Organic/Inorganic Interfaces",
    "section": "Related Publications",
    "text": "Related Publications\n\n\n    \n      \n      \n    \n\n\n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            A Hybrid-DFT Study of Intrinsic Point Defects in MX2 (M=Mo, W; X=S, Se) Monolayers\n            \n            \n                \n                \n                    Alaa Akkoush, Yair Litman, Mariana Rossi \n                \n                \n                \n                    \n                        Phys. Status Solidi A, 221, 2300180 (2024)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n\n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n  \n              \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            First-Principles Simulations of Tip Enhanced Raman Scattering Reveal Active Role of Substrate on High-Resolution Images\n            \n            \n                \n                \n                    Yair Litman, Franco P. Bonafé, Alaa Akkoush, Heiko Appel, Mariana Rossi \n                \n                \n                \n                    \n                        J. Phys. Chem. Lett., 14, 6850-6859 (2023)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n\n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n  \n              \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            Charge Transfer-Mediated Dramatic Enhancement of Raman Scattering upon Molecular Point Contact Formation\n            \n            \n                \n                \n                    Borja Cirera, Yair Litman, Chenfang Lin, Alaa Akkoush, Adnan Hammud, Martin Wolf, Mariana Rossi, Takashi Kumagai \n                \n                \n                \n                    \n                        Nano Lett.., 22, 2170-2176 (2022)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n\n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n  \n              \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            Multidimensional Hydrogen Tunneling in Supported Molecular Switches: The Role   of Surface Interactions\n            \n            \n                \n                \n                    Yair Litman, Mariana Rossi \n                \n                \n                \n                    \n                        Phys. Rev.  Lett., 125, 216001 (2020)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n\n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n  \n              \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            \n            \n            \n                \n                \n                    Takashi Kumagai, Janina N. Ladenthin, Yair Litman, Mariana Rossi, Leonhard Grill, Sylwester Gawinkowski, Jacek Waluk, Mats Persson \n                \n                \n                \n                    \n                        J. Chem. Phys., 148, 102330 (2018)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n\n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n  \n              \n\n        \n    \n    \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "research/QuantumDynamics/index.html",
    "href": "research/QuantumDynamics/index.html",
    "title": "Quantum Dynamics",
    "section": "",
    "text": "Understanding, predicting and manipulating nuclear quantum dynamics."
  },
  {
    "objectID": "research/QuantumDynamics/index.html#related-publications",
    "href": "research/QuantumDynamics/index.html#related-publications",
    "title": "Quantum Dynamics",
    "section": "Related Publications",
    "text": "Related Publications\n\n\n    \n      \n      \n    \n\n\n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            i-PI 3.0: a flexible, efficient framework for advanced atomistic simulations\n            \n            \n                \n                \n                    Yair Litman, Venkat Kapil, Yotam MY Feldman, Davide Tisi, Tomislav Begušić, Karen Fidanyan, Guillaume Fraux, Jacob Higer, Matthias Kellner, Tao E Li, Eszter S Pós, Elia Stocco, George Trenins, Barak Hirshberg, Mariana Rossi, Michele Ceriotti \n                \n                \n                \n                    \n                        arXiv, 2405.15224,   (2024)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n\n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n  \n              \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            Quantum rates in dissipative systems with spatially varying friction\n            \n            \n                \n                \n                    Oliver Bridge, Paolo Lazzaroni, Rocco Martinazzo, Mariana Rossi, Stuart C. Althorpe, Yair Litman \n                \n                \n                \n                    \n                        J. Chem. Phys., (accepted) ,   (2024)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n\n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n  \n              \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            Is Unified Understanding of Vibrational Coupling of Water Possible? Hyper-Raman  Measurement and Machine Learning Spectra\n            \n            \n                \n                \n                    Kazuki Inoue, Yair Litman, David M. Wilkins, Yuki Nagata, Masanari Okuno \n                \n                \n                \n                    \n                        J. Phys. Chem. Lett., 14, 3063-3068 (2023)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n\n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n  \n              \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            Dissipative tunneling rates through the incorporation of first-principles electronic  friction in instanton rate theory. II. Benchmarks and applications\n            \n            \n                \n                \n                    Yair Litman, Eszter S. Pós, Connor L. Box, Rocco Martinazzo, Reinhard J. Maurer, Mariana Rossi \n                \n                \n                \n                    \n                        J. Chem. Phys., 156, 194107 (2022)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n\n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n  \n              \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            Dissipative tunneling rates through the incorporation of first-principles electronic  friction in instanton rate theory. I. Theory\n            \n            \n                \n                \n                    Yair Litman, Eszter S. Pós, Connor L. Box, Rocco Martinazzo, Reinhard J. Maurer, Mariana Rossi \n                \n                \n                \n                    \n                        J. Chem. Phys. , 156, 194106 (2022)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n\n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n  \n              \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            Multidimensional Hydrogen Tunneling in Supported Molecular Switches: The Role   of Surface Interactions\n            \n            \n                \n                \n                    Yair Litman, Mariana Rossi \n                \n                \n                \n                    \n                        Phys. Rev.  Lett., 125, 216001 (2020)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n\n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n  \n              \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            Temperature dependence of the vibrational spectrum of porphycene: a qualitative  failure of classical-nuclei molecular dynamics\n            \n            \n                \n                \n                    Yair Litman, Jörg Behler, Mariana Rossi \n                \n                \n                \n                    \n                        Faraday Discuss., 221, 526-546 (2020)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n\n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n  \n              \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            i-PI 2.0: A universal force engine for advanced molecular simulations\n            \n            \n                \n                \n                    Venkat Kapil,  Mariana Rossi,  Ondrej Marsalek,  Riccardo Petraglia,  Yair Litman,  Thomas Spura,  Bingqing Cheng,  Alice Cuzzocrea,  Robert H. Meißner,  David M. Wilkins,  Benjamin A. Helfrecht, Przemysław Juda, Sébastien P. Bienvenue, Wei Fang, Jan Kessler, Igor Poltavsky, Steven Vandenbrande, Jelle Wieme, Clemence Corminboeuf, Thomas D. Kühne, David E. Manolopoulos, Thomas E. Markland, Jeremy O. Richardson, Alexandre Tkatchenko, Gareth A. Tribello, Veronique Van Speybroeck, Michele Ceriotti \n                \n                \n                \n                    \n                        Comput. Phys. Commun., 236, 214-223 (2019)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n\n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n  \n              \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            Elucidating the Nuclear Quantum Dynamics of Intramolecular Double Hydrogen Transfer in Porphycene\n            \n            \n                \n                \n                    Yair Litman, Jeremy O. Richardson, Takashi Kumagai, Mariana Rossi \n                \n                \n                \n                    \n                        J. Am. Chem. Soc., 141, 2526-2534 (2019)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n\n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n  \n              \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            Decisive role of nuclear quantum effects on surface mediated water dissociation at finite temperature\n            \n            \n                \n                \n                    Yair Litman, Davide Donadio, Michele Ceriotti, Mariana Rossi \n                \n                \n                \n                    \n                        J. Chem. Phys., 148, 102320 (2018)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n\n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n  \n              \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            \n            \n            \n                \n                \n                    Takashi Kumagai, Janina N. Ladenthin, Yair Litman, Mariana Rossi, Leonhard Grill, Sylwester Gawinkowski, Jacek Waluk, Mats Persson \n                \n                \n                \n                    \n                        J. Chem. Phys., 148, 102330 (2018)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n\n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n  \n              \n\n        \n    \n    \n    \n    \n    \n        \n            \n\n                \n                    \n                    \n                    \n\n                \n      \n\n            \n        \n        \n            \n            Positional Isotope Exchange in HX·(H2O)n (X = F, I) Clusters at Low Temperatures\n            \n            \n                \n                \n                    Yair  Litman, Pablo E. Videla, Javier Rodriguez, Daniel Laria \n                \n                \n                \n                    \n                        J. Phys. Chem. A., 120, 7213-7224 (2016)\n                        \n                        \n                        \n                        \n                        \n                        \n                \n                    \n                    \n                    \n            \n \n        \n        \n\n            \n            \n\n                    \n                    journal URL \n                    \n                \n                    \n  \n              \n\n        \n    \n    \n    \n    \n\n\nNo matching items"
  }
]